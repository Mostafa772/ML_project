{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "import itertools\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from loss_functions import *\n",
    "from src.activation_functions import *\n",
    "from src.batch_normalization import *\n",
    "from src.data_preprocessing import *\n",
    "from src.cascade_correlation import CascadeCorrelation\n",
    "from src.k_fold_cross_validation import *\n",
    "from src.layer import *\n",
    "from src.early_stopping import EarlyStopping\n",
    "from src.neural_network import *\n",
    "from src.optimizers import *\n",
    "from src.random_search import *\n",
    "from src.train_and_evaluate import Train\n",
    "from src.utils import *\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### Data pre-processing for MONK Datasets  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONK_NUM=1\n",
    "X_train, y_train = load_data(MONK_NUM=MONK_NUM)\n",
    "X_test, y_test = load_data(MONK_NUM=MONK_NUM, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(X_train)\n",
    "y_train, y_val = train_test_split(y_train)\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_val = np.asarray(X_val)\n",
    "y_val = np.asarray(y_val)\n",
    "X_test = np.asarray(X_test)\n",
    "y_test = np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the resulting datasets\n",
    "for _ in [X_train, X_val, y_train, y_val]:\n",
    "    print(f\"the shape: \", _.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'learning_rate': np.logspace(-3, -1, num=30).tolist(),\n",
    "    'l1': np.logspace(-5, -1, num=20).tolist(),\n",
    "    'l2': np.logspace(-5, -1, num=20).tolist(),\n",
    "    'dropout_rate': np.logspace(-5, -1, num=20).tolist(),\n",
    "    'batch_size': [8, 16, 32],\n",
    "    'n_epochs': [50, 100],\n",
    "    'hidden_size': [3, 4, 5, 6],\n",
    "    'hidden_activation': [Activation_Tanh, Activation_Leaky_ReLU, Activation_Sigmoid, Activation_ReLU],\n",
    "    'batch_norm': [False],\n",
    "    'weight_decay': [0, 5e-2, 1e-2, 1e-3, 1e-5],\n",
    "    'patience': [10, 30, 50],\n",
    "    'n_h_layers': [1],\n",
    "    'weights_init': ['he'],\n",
    "    'output_activation': [Activation_Sigmoid()],\n",
    "    'sched_decay': [1]\n",
    "}\n",
    "\n",
    "print(f\"Number of possible combinations: {count_permutations(param_distributions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams, best_performance = random_search(X_train=X_train, y_train=y_train, param_distributions=param_distributions, n_iters=500, csv_path=f\"monk{MONK_NUM}_top5res.csv\")  # adjust n_iters as needed\n",
    "\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_hyperparams['CC']:\n",
    "    model = CascadeCorrelation(input_size = 17, output_size= 1, activation=Activation_Leaky_ReLU, output_activation = Activation_Sigmoid)\n",
    "else:\n",
    "    model = NN(\n",
    "        l1=best_hyperparams['l1'],\n",
    "        l2=best_hyperparams['l2'],\n",
    "        input_size=17,\n",
    "        hidden_size=best_hyperparams['hidden_size'],\n",
    "        output_size=1,\n",
    "        hidden_activation=best_hyperparams['hidden_activation'],\n",
    "        dropout_rate=best_hyperparams['dropout_rate'],\n",
    "        use_batch_norm=best_hyperparams['batch_norm'],\n",
    "        n_h_layers=best_hyperparams['n_h_layers'],\n",
    "        output_activation=best_hyperparams['output_activation'],\n",
    "        weights_init=best_hyperparams['weights_init']\n",
    "    )\n",
    "\n",
    "train = Train(best_hyperparams, model)\n",
    "train.train_and_evaluate(X_train, y_train, X_val, y_val)\n",
    "train.test(X_test, y_test)\n",
    "train.plot(score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.train_losses, train.val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Accuracy: {train.test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleNN:\n",
    "    def __init__(self, n_models=5):\n",
    "        self.models = []\n",
    "        self.n_models = n_models\n",
    "        self.loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "        \n",
    "\n",
    "    def create_and_train_models(self, hyperparams):\n",
    "        # Create and train multiple models with the same hyperparameters\n",
    "        for i in range(self.n_models):\n",
    "            model = NN(\n",
    "                l1=hyperparams['l1'],\n",
    "                l2=hyperparams['l2'],\n",
    "                input_size=17,\n",
    "                hidden_size=hyperparams['hidden_size'],\n",
    "                output_size=1,\n",
    "                hidden_activation=hyperparams['hidden_activation'],\n",
    "                dropout_rate=hyperparams['dropout_rate'],\n",
    "                use_batch_norm=hyperparams['batch_norm'],\n",
    "                n_h_layers=hyperparams['n_h_layers']\n",
    "            )\n",
    "            print(f\"Training model {i+1}/{self.n_models}\")\n",
    "            # Train model using existing train_and_evaluate function\n",
    "            self.train = Train(hyperparams, model)\n",
    "            model, val_accuracy = self.train.train_and_evaluate(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_val=X_val,\n",
    "                y_val=y_val,\n",
    "            )\n",
    "            self.models.append(model)\n",
    "            print(f\"Model {i+1} validation accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using majority voting\"\"\"\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            model.forward(X, training=False)\n",
    "            self.loss_activation.forward(\n",
    "                model.output, np.zeros((X.shape[0], 2)))  # Dummy y values\n",
    "            pred = np.argmax(self.loss_activation.output, axis=1)\n",
    "            predictions.append(pred)\n",
    "\n",
    "        # Majority voting\n",
    "        predictions = np.array(predictions)\n",
    "        final_predictions = np.apply_along_axis(\n",
    "            lambda x: np.bincount(x).argmax(),\n",
    "            axis=0,\n",
    "            arr=predictions\n",
    "        )\n",
    "        return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleNN(n_models=5)\n",
    "\n",
    "ensemble.create_and_train_models(best_hyperparams)\n",
    "\n",
    "_ , test_accuracy = ensemble.train.test(X_test, y_test)\n",
    "\n",
    "print(f\"Ensemble Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "ensemble.train.plot(score=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
