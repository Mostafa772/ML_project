{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-2.3.0-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.0-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\school stuff\\master's - computer science - ai\\sem 1\\projects\\machine learning\\ml_project\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.4-cp313-cp313-win_amd64.whl.metadata (108 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\school stuff\\master's - computer science - ai\\sem 1\\projects\\machine learning\\ml_project\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp313-cp313-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\school stuff\\master's - computer science - ai\\sem 1\\projects\\machine learning\\ml_project\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading numpy-2.3.0-cp313-cp313-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 6.3/12.7 MB 31.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 34.8 MB/s eta 0:00:00\n",
      "Downloading pandas-2.3.0-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 10.0/11.0 MB 47.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 41.8 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.10.3-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.1/8.1 MB 41.0 MB/s eta 0:00:00\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading scikit_learn-1.7.0-cp313-cp313-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   -------------------------------------- - 10.2/10.7 MB 48.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 43.1 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.2-cp313-cp313-win_amd64.whl (223 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.4-cp313-cp313-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 40.7 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading kiwisolver-1.4.8-cp313-cp313-win_amd64.whl (71 kB)\n",
      "Downloading pillow-11.2.1-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 37.1 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scipy-1.15.3-cp313-cp313-win_amd64.whl (41.0 MB)\n",
      "   ---------------------------------------- 0.0/41.0 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 8.7/41.0 MB 42.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 17.8/41.0 MB 42.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 26.7/41.0 MB 42.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 35.1/41.0 MB 42.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.9/41.0 MB 41.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.0/41.0 MB 37.8 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, pyparsing, pillow, numpy, kiwisolver, joblib, fonttools, cycler, scipy, pandas, contourpy, scikit-learn, matplotlib, seaborn\n",
      "\n",
      "   ----------------------------------------  0/16 [pytz]\n",
      "   ----------------------------------------  0/16 [pytz]\n",
      "   ----------------------------------------  0/16 [pytz]\n",
      "   ----------------------------------------  0/16 [pytz]\n",
      "   -- -------------------------------------  1/16 [tzdata]\n",
      "   -- -------------------------------------  1/16 [tzdata]\n",
      "   -- -------------------------------------  1/16 [tzdata]\n",
      "   -- -------------------------------------  1/16 [tzdata]\n",
      "   -- -------------------------------------  1/16 [tzdata]\n",
      "   ------- --------------------------------  3/16 [pyparsing]\n",
      "   ------- --------------------------------  3/16 [pyparsing]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ---------- -----------------------------  4/16 [pillow]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ------------ ---------------------------  5/16 [numpy]\n",
      "   ----------------- ----------------------  7/16 [joblib]\n",
      "   ----------------- ----------------------  7/16 [joblib]\n",
      "   ----------------- ----------------------  7/16 [joblib]\n",
      "   ----------------- ----------------------  7/16 [joblib]\n",
      "   ----------------- ----------------------  7/16 [joblib]\n",
      "   ----------------- ----------------------  7/16 [joblib]\n",
      "   ----------------- ----------------------  7/16 [joblib]\n",
      "   ----------------- ----------------------  7/16 [joblib]\n",
      "   ----------------- ----------------------  7/16 [joblib]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   -------------------- -------------------  8/16 [fonttools]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   ------------------------- -------------- 10/16 [scipy]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   --------------------------- ------------ 11/16 [pandas]\n",
      "   ------------------------------ --------- 12/16 [contourpy]\n",
      "   ------------------------------ --------- 12/16 [contourpy]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   -------------------------------- ------- 13/16 [scikit-learn]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ----------------------------------- ---- 14/16 [matplotlib]\n",
      "   ------------------------------------- -- 15/16 [seaborn]\n",
      "   ------------------------------------- -- 15/16 [seaborn]\n",
      "   ------------------------------------- -- 15/16 [seaborn]\n",
      "   ------------------------------------- -- 15/16 [seaborn]\n",
      "   ------------------------------------- -- 15/16 [seaborn]\n",
      "   ------------------------------------- -- 15/16 [seaborn]\n",
      "   ---------------------------------------- 16/16 [seaborn]\n",
      "\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 joblib-1.5.1 kiwisolver-1.4.8 matplotlib-3.10.3 numpy-2.3.0 pandas-2.3.0 pillow-11.2.1 pyparsing-3.2.3 pytz-2025.2 scikit-learn-1.7.0 scipy-1.15.3 seaborn-0.13.2 threadpoolctl-3.6.0 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib seaborn scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import ast\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from itertools import product\n",
    "import csv\n",
    "from src.data_preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.optimizers import *\n",
    "from src.random_search import *\n",
    "from src.train_and_evaluate import Train\n",
    "from src.utils import *\n",
    "# from src.model_regularization import *\n",
    "from src.layer import *\n",
    "from src.batch_normalization import *\n",
    "from loss_functions import *\n",
    "from src.neural_network import *\n",
    "from src.train_and_evaluate import *\n",
    "from src.random_search import *\n",
    "from src.k_fold_cross_validation import *\n",
    "from src.early_stopping import *\n",
    "# from src.ensemble_learning import *\n",
    "# from src.data_split import *\n",
    "# from src.model import *\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### Data pre-processing for MONK Datasets  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot encoded data:  (169, 17)\n",
      "one hot encoded data:  (432, 17)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_data(MONK_NUM=2)\n",
    "X_test, y_test = load_data(MONK_NUM=2, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42)\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_val = np.asarray(X_val)\n",
    "y_val = np.asarray(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 135\n",
      "Validation set size: 34\n",
      "Training set overlap with validation set: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))\n",
    "print(\"Training set overlap with validation set:\",\n",
    "      np.intersect1d(X_train, X_val).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape:  (135, 17)\n",
      "the shape:  (34, 17)\n",
      "the shape:  (135,)\n",
      "the shape:  (34,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the resulting datasets\n",
    "for _ in [X_train, X_val, y_train, y_val]:\n",
    "    print(f\"the shape: \", _.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'hidden_size': [[3], [4], [5], [6]],\n",
    "    'hidden_activation': [[Activation_Tanh], [Activation_Leaky_ReLU], [Activation_Sigmoid], [Activation_ReLU]],\n",
    "    'batch_norm': [[True], [False]],\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "    'l1': [0.0, 1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'l2': [0.0, 1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'dropout_rate': [0.0, 0.1, 0.3],\n",
    "    'batch_size': [1000],\n",
    "    'n_epochs': [150, 200],\n",
    "    'weight_decay': [0, 5e-2, 1e-2, 1e-3, 1e-5],\n",
    "    'patience': [0, 20, 30, 50],\n",
    "    'CC': [False]\n",
    "    # # Define combinations of hidden layer sizes and corresponding activations\n",
    "    # 'hidden_configs': [\n",
    "    #     {'hidden_size': [10], 'hidden_activation': [Activation_Tanh], 'batch_norm' : [True]},\n",
    "    #     {'hidden_size': [10, 10], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, True]},\n",
    "    #     {'hidden_size': [10, 10], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, True]},\n",
    "    #     {'hidden_size': [20, 20], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, False]},\n",
    "    #     {'hidden_size': [20, 20], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, True]},\n",
    "    #     {'hidden_size': [20, 20], 'hidden_activation': [Activation_ELU, Activation_ELU], 'batch_norm' : [True, False]},\n",
    "    #     {'hidden_size': [10, 10], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, False]},\n",
    "    #     {'hidden_size': [32, 16], 'hidden_activation': [Activation_Leaky_ReLU, Activation_Sigmoid], 'batch_norm' : [False, True]},  \n",
    "    #     {'hidden_size': [30, 30], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, False]},\n",
    "    #     {'hidden_size': [64, 32, 16], 'hidden_activation': [Activation_ReLU, Activation_ReLU, Activation_Tanh], 'batch_norm' : [True, False, True]},\n",
    "    #     {'hidden_size': [30, 30], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm': [True, False]}\n",
    "    # ]   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "Create a seperate best_results csv file for each MONK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 1e-05, 'l2': 0.001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2609, Acc: 52.78% | Val Loss: 0.2783, Acc: 62.96%\n",
      "Epoch 10: Train Loss: 0.2607, Acc: 52.78% | Val Loss: 0.2779, Acc: 62.96%\n",
      "Epoch 20: Train Loss: 0.2605, Acc: 52.78% | Val Loss: 0.2774, Acc: 62.96%\n",
      "Epoch 30: Train Loss: 0.2602, Acc: 54.63% | Val Loss: 0.2770, Acc: 62.96%\n",
      "Epoch 40: Train Loss: 0.2600, Acc: 54.63% | Val Loss: 0.2766, Acc: 62.96%\n",
      "Epoch 50: Train Loss: 0.2598, Acc: 53.70% | Val Loss: 0.2762, Acc: 62.96%\n",
      "Epoch 60: Train Loss: 0.2596, Acc: 54.63% | Val Loss: 0.2757, Acc: 62.96%\n",
      "Epoch 70: Train Loss: 0.2593, Acc: 54.63% | Val Loss: 0.2753, Acc: 62.96%\n",
      "Epoch 80: Train Loss: 0.2591, Acc: 54.63% | Val Loss: 0.2749, Acc: 62.96%\n",
      "Epoch 90: Train Loss: 0.2589, Acc: 54.63% | Val Loss: 0.2745, Acc: 62.96%\n",
      "Epoch 100: Train Loss: 0.2587, Acc: 54.63% | Val Loss: 0.2740, Acc: 62.96%\n",
      "Epoch 110: Train Loss: 0.2584, Acc: 54.63% | Val Loss: 0.2736, Acc: 62.96%\n",
      "Epoch 120: Train Loss: 0.2582, Acc: 54.63% | Val Loss: 0.2732, Acc: 62.96%\n",
      "Epoch 130: Train Loss: 0.2580, Acc: 54.63% | Val Loss: 0.2728, Acc: 62.96%\n",
      "Epoch 140: Train Loss: 0.2578, Acc: 54.63% | Val Loss: 0.2724, Acc: 62.96%\n",
      "Epoch 150: Train Loss: 0.2576, Acc: 54.63% | Val Loss: 0.2720, Acc: 66.67%\n",
      "Epoch 160: Train Loss: 0.2574, Acc: 54.63% | Val Loss: 0.2716, Acc: 66.67%\n",
      "Epoch 170: Train Loss: 0.2571, Acc: 54.63% | Val Loss: 0.2712, Acc: 62.96%\n",
      "Epoch 180: Train Loss: 0.2569, Acc: 54.63% | Val Loss: 0.2708, Acc: 62.96%\n",
      "Epoch 190: Train Loss: 0.2567, Acc: 54.63% | Val Loss: 0.2704, Acc: 66.67%\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 1/5 | Validation Accuracy: 0.6667\n",
      "[3] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 1e-05, 'l2': 0.001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3179, Acc: 60.19% | Val Loss: 0.3990, Acc: 51.85%\n",
      "Epoch 10: Train Loss: 0.3174, Acc: 60.19% | Val Loss: 0.3987, Acc: 51.85%\n",
      "Epoch 20: Train Loss: 0.3169, Acc: 60.19% | Val Loss: 0.3984, Acc: 51.85%\n",
      "Epoch 30: Train Loss: 0.3164, Acc: 60.19% | Val Loss: 0.3981, Acc: 51.85%\n",
      "Epoch 40: Train Loss: 0.3159, Acc: 60.19% | Val Loss: 0.3978, Acc: 51.85%\n",
      "Epoch 50: Train Loss: 0.3154, Acc: 60.19% | Val Loss: 0.3975, Acc: 51.85%\n",
      "Epoch 60: Train Loss: 0.3149, Acc: 60.19% | Val Loss: 0.3971, Acc: 51.85%\n",
      "Epoch 70: Train Loss: 0.3144, Acc: 60.19% | Val Loss: 0.3968, Acc: 51.85%\n",
      "Epoch 80: Train Loss: 0.3139, Acc: 60.19% | Val Loss: 0.3965, Acc: 51.85%\n",
      "Epoch 90: Train Loss: 0.3134, Acc: 60.19% | Val Loss: 0.3962, Acc: 51.85%\n",
      "Epoch 100: Train Loss: 0.3130, Acc: 60.19% | Val Loss: 0.3959, Acc: 51.85%\n",
      "Epoch 110: Train Loss: 0.3125, Acc: 60.19% | Val Loss: 0.3956, Acc: 51.85%\n",
      "Epoch 120: Train Loss: 0.3120, Acc: 60.19% | Val Loss: 0.3953, Acc: 51.85%\n",
      "Epoch 130: Train Loss: 0.3115, Acc: 60.19% | Val Loss: 0.3950, Acc: 51.85%\n",
      "Epoch 140: Train Loss: 0.3110, Acc: 60.19% | Val Loss: 0.3947, Acc: 51.85%\n",
      "Epoch 150: Train Loss: 0.3105, Acc: 60.19% | Val Loss: 0.3944, Acc: 51.85%\n",
      "Epoch 160: Train Loss: 0.3100, Acc: 60.19% | Val Loss: 0.3940, Acc: 51.85%\n",
      "Epoch 170: Train Loss: 0.3096, Acc: 60.19% | Val Loss: 0.3937, Acc: 51.85%\n",
      "Epoch 180: Train Loss: 0.3091, Acc: 60.19% | Val Loss: 0.3934, Acc: 51.85%\n",
      "Epoch 190: Train Loss: 0.3086, Acc: 60.19% | Val Loss: 0.3931, Acc: 51.85%\n",
      "Final Validation Accuracy: 0.5185\n",
      " Fold 2/5 | Validation Accuracy: 0.5185\n",
      "[3] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 1e-05, 'l2': 0.001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3635, Acc: 54.63% | Val Loss: 0.3918, Acc: 48.15%\n",
      "Epoch 10: Train Loss: 0.3631, Acc: 54.63% | Val Loss: 0.3912, Acc: 48.15%\n",
      "Epoch 20: Train Loss: 0.3627, Acc: 54.63% | Val Loss: 0.3906, Acc: 48.15%\n",
      "Epoch 30: Train Loss: 0.3623, Acc: 54.63% | Val Loss: 0.3899, Acc: 48.15%\n",
      "Epoch 40: Train Loss: 0.3619, Acc: 54.63% | Val Loss: 0.3893, Acc: 48.15%\n",
      "Epoch 50: Train Loss: 0.3615, Acc: 54.63% | Val Loss: 0.3887, Acc: 48.15%\n",
      "Epoch 60: Train Loss: 0.3611, Acc: 54.63% | Val Loss: 0.3881, Acc: 48.15%\n",
      "Epoch 70: Train Loss: 0.3607, Acc: 54.63% | Val Loss: 0.3875, Acc: 48.15%\n",
      "Epoch 80: Train Loss: 0.3603, Acc: 54.63% | Val Loss: 0.3868, Acc: 48.15%\n",
      "Epoch 90: Train Loss: 0.3599, Acc: 54.63% | Val Loss: 0.3862, Acc: 48.15%\n",
      "Epoch 100: Train Loss: 0.3595, Acc: 54.63% | Val Loss: 0.3856, Acc: 48.15%\n",
      "Epoch 110: Train Loss: 0.3591, Acc: 54.63% | Val Loss: 0.3850, Acc: 48.15%\n",
      "Epoch 120: Train Loss: 0.3587, Acc: 54.63% | Val Loss: 0.3844, Acc: 48.15%\n",
      "Epoch 130: Train Loss: 0.3583, Acc: 54.63% | Val Loss: 0.3838, Acc: 48.15%\n",
      "Epoch 140: Train Loss: 0.3580, Acc: 55.56% | Val Loss: 0.3832, Acc: 48.15%\n",
      "Epoch 150: Train Loss: 0.3576, Acc: 55.56% | Val Loss: 0.3826, Acc: 48.15%\n",
      "Epoch 160: Train Loss: 0.3572, Acc: 55.56% | Val Loss: 0.3820, Acc: 48.15%\n",
      "Epoch 170: Train Loss: 0.3568, Acc: 55.56% | Val Loss: 0.3814, Acc: 48.15%\n",
      "Epoch 180: Train Loss: 0.3564, Acc: 54.63% | Val Loss: 0.3808, Acc: 48.15%\n",
      "Epoch 190: Train Loss: 0.3560, Acc: 54.63% | Val Loss: 0.3803, Acc: 48.15%\n",
      "Final Validation Accuracy: 0.4815\n",
      " Fold 3/5 | Validation Accuracy: 0.4815\n",
      "[3] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 1e-05, 'l2': 0.001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3435, Acc: 44.44% | Val Loss: 0.2870, Acc: 37.04%\n",
      "Epoch 10: Train Loss: 0.3429, Acc: 44.44% | Val Loss: 0.2868, Acc: 37.04%\n",
      "Epoch 20: Train Loss: 0.3424, Acc: 45.37% | Val Loss: 0.2865, Acc: 37.04%\n",
      "Epoch 30: Train Loss: 0.3419, Acc: 46.30% | Val Loss: 0.2862, Acc: 37.04%\n",
      "Epoch 40: Train Loss: 0.3413, Acc: 45.37% | Val Loss: 0.2860, Acc: 37.04%\n",
      "Epoch 50: Train Loss: 0.3408, Acc: 44.44% | Val Loss: 0.2857, Acc: 37.04%\n",
      "Epoch 60: Train Loss: 0.3403, Acc: 44.44% | Val Loss: 0.2855, Acc: 37.04%\n",
      "Epoch 70: Train Loss: 0.3398, Acc: 44.44% | Val Loss: 0.2852, Acc: 37.04%\n",
      "Epoch 80: Train Loss: 0.3393, Acc: 44.44% | Val Loss: 0.2850, Acc: 37.04%\n",
      "Epoch 90: Train Loss: 0.3388, Acc: 44.44% | Val Loss: 0.2848, Acc: 37.04%\n",
      "Epoch 100: Train Loss: 0.3383, Acc: 44.44% | Val Loss: 0.2845, Acc: 37.04%\n",
      "Epoch 110: Train Loss: 0.3378, Acc: 44.44% | Val Loss: 0.2843, Acc: 37.04%\n",
      "Epoch 120: Train Loss: 0.3373, Acc: 45.37% | Val Loss: 0.2840, Acc: 37.04%\n",
      "Epoch 130: Train Loss: 0.3368, Acc: 45.37% | Val Loss: 0.2838, Acc: 37.04%\n",
      "Epoch 140: Train Loss: 0.3363, Acc: 46.30% | Val Loss: 0.2836, Acc: 37.04%\n",
      "Epoch 150: Train Loss: 0.3358, Acc: 46.30% | Val Loss: 0.2833, Acc: 37.04%\n",
      "Epoch 160: Train Loss: 0.3353, Acc: 46.30% | Val Loss: 0.2831, Acc: 37.04%\n",
      "Epoch 170: Train Loss: 0.3348, Acc: 46.30% | Val Loss: 0.2829, Acc: 37.04%\n",
      "Epoch 180: Train Loss: 0.3343, Acc: 46.30% | Val Loss: 0.2827, Acc: 37.04%\n",
      "Epoch 190: Train Loss: 0.3338, Acc: 46.30% | Val Loss: 0.2824, Acc: 37.04%\n",
      "Final Validation Accuracy: 0.3704\n",
      " Fold 4/5 | Validation Accuracy: 0.3704\n",
      "[3] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 1e-05, 'l2': 0.001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2639, Acc: 56.48% | Val Loss: 0.2875, Acc: 48.15%\n",
      "Epoch 10: Train Loss: 0.2637, Acc: 56.48% | Val Loss: 0.2873, Acc: 48.15%\n",
      "Epoch 20: Train Loss: 0.2634, Acc: 56.48% | Val Loss: 0.2871, Acc: 48.15%\n",
      "Epoch 30: Train Loss: 0.2632, Acc: 56.48% | Val Loss: 0.2870, Acc: 48.15%\n",
      "Epoch 40: Train Loss: 0.2629, Acc: 56.48% | Val Loss: 0.2868, Acc: 48.15%\n",
      "Epoch 50: Train Loss: 0.2627, Acc: 56.48% | Val Loss: 0.2866, Acc: 48.15%\n",
      "Epoch 60: Train Loss: 0.2624, Acc: 56.48% | Val Loss: 0.2864, Acc: 48.15%\n",
      "Epoch 70: Train Loss: 0.2622, Acc: 56.48% | Val Loss: 0.2862, Acc: 48.15%\n",
      "Epoch 80: Train Loss: 0.2619, Acc: 56.48% | Val Loss: 0.2860, Acc: 48.15%\n",
      "Epoch 90: Train Loss: 0.2617, Acc: 56.48% | Val Loss: 0.2859, Acc: 48.15%\n",
      "Epoch 100: Train Loss: 0.2615, Acc: 56.48% | Val Loss: 0.2857, Acc: 48.15%\n",
      "Epoch 110: Train Loss: 0.2612, Acc: 56.48% | Val Loss: 0.2855, Acc: 48.15%\n",
      "Epoch 120: Train Loss: 0.2610, Acc: 56.48% | Val Loss: 0.2853, Acc: 48.15%\n",
      "Epoch 130: Train Loss: 0.2607, Acc: 56.48% | Val Loss: 0.2852, Acc: 48.15%\n",
      "Epoch 140: Train Loss: 0.2605, Acc: 56.48% | Val Loss: 0.2850, Acc: 48.15%\n",
      "Epoch 150: Train Loss: 0.2603, Acc: 56.48% | Val Loss: 0.2848, Acc: 48.15%\n",
      "Epoch 160: Train Loss: 0.2600, Acc: 57.41% | Val Loss: 0.2847, Acc: 48.15%\n",
      "Epoch 170: Train Loss: 0.2598, Acc: 57.41% | Val Loss: 0.2845, Acc: 48.15%\n",
      "Epoch 180: Train Loss: 0.2596, Acc: 57.41% | Val Loss: 0.2843, Acc: 48.15%\n",
      "Epoch 190: Train Loss: 0.2593, Acc: 57.41% | Val Loss: 0.2842, Acc: 48.15%\n",
      "Final Validation Accuracy: 0.4815\n",
      " Fold 5/5 | Validation Accuracy: 0.4815\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5037\n",
      "[3] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.0, 'l2': 0.001, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2469, Acc: 54.63% | Val Loss: 0.2628, Acc: 51.85%\n",
      "Epoch 10: Train Loss: 0.2493, Acc: 57.41% | Val Loss: 0.2537, Acc: 51.85%\n",
      "Epoch 20: Train Loss: 0.2464, Acc: 58.33% | Val Loss: 0.2488, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.2433, Acc: 61.11% | Val Loss: 0.2463, Acc: 70.37%\n",
      "Epoch 40: Train Loss: 0.2479, Acc: 53.70% | Val Loss: 0.2447, Acc: 77.78%\n",
      "Epoch 50: Train Loss: 0.2479, Acc: 53.70% | Val Loss: 0.2436, Acc: 74.07%\n",
      "Epoch 60: Train Loss: 0.2453, Acc: 57.41% | Val Loss: 0.2427, Acc: 77.78%\n",
      "Epoch 70: Train Loss: 0.2464, Acc: 54.63% | Val Loss: 0.2420, Acc: 77.78%\n",
      "Epoch 80: Train Loss: 0.2458, Acc: 54.63% | Val Loss: 0.2413, Acc: 77.78%\n",
      "Epoch 90: Train Loss: 0.2428, Acc: 59.26% | Val Loss: 0.2407, Acc: 77.78%\n",
      "Epoch 100: Train Loss: 0.2434, Acc: 56.48% | Val Loss: 0.2402, Acc: 81.48%\n",
      "Epoch 110: Train Loss: 0.2431, Acc: 62.04% | Val Loss: 0.2397, Acc: 81.48%\n",
      "Epoch 120: Train Loss: 0.2462, Acc: 54.63% | Val Loss: 0.2393, Acc: 81.48%\n",
      "Epoch 130: Train Loss: 0.2381, Acc: 63.89% | Val Loss: 0.2389, Acc: 81.48%\n",
      "Epoch 140: Train Loss: 0.2430, Acc: 57.41% | Val Loss: 0.2385, Acc: 85.19%\n",
      "Final Validation Accuracy: 0.8519\n",
      " Fold 1/5 | Validation Accuracy: 0.8519\n",
      "[3] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.0, 'l2': 0.001, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2359, Acc: 64.81% | Val Loss: 0.2288, Acc: 62.96%\n",
      "Epoch 10: Train Loss: 0.2249, Acc: 64.81% | Val Loss: 0.2293, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.2348, Acc: 62.04% | Val Loss: 0.2295, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.2294, Acc: 67.59% | Val Loss: 0.2293, Acc: 66.67%\n",
      "Early stopping at epoch 30\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 2/5 | Validation Accuracy: 0.6667\n",
      "[3] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.0, 'l2': 0.001, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3415, Acc: 41.67% | Val Loss: 0.2290, Acc: 66.67%\n",
      "Epoch 10: Train Loss: 0.3179, Acc: 39.81% | Val Loss: 0.2757, Acc: 62.96%\n",
      "Epoch 20: Train Loss: 0.3248, Acc: 43.52% | Val Loss: 0.2984, Acc: 55.56%\n",
      "Epoch 30: Train Loss: 0.3447, Acc: 37.04% | Val Loss: 0.3060, Acc: 44.44%\n",
      "Early stopping at epoch 30\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4444\n",
      " Fold 3/5 | Validation Accuracy: 0.4444\n",
      "[3] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.0, 'l2': 0.001, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2925, Acc: 37.96% | Val Loss: 0.3047, Acc: 40.74%\n",
      "Epoch 10: Train Loss: 0.2903, Acc: 35.19% | Val Loss: 0.2911, Acc: 40.74%\n",
      "Epoch 20: Train Loss: 0.3140, Acc: 28.70% | Val Loss: 0.2870, Acc: 40.74%\n",
      "Epoch 30: Train Loss: 0.2982, Acc: 36.11% | Val Loss: 0.2851, Acc: 44.44%\n",
      "Epoch 40: Train Loss: 0.2859, Acc: 38.89% | Val Loss: 0.2840, Acc: 44.44%\n",
      "Epoch 50: Train Loss: 0.2988, Acc: 35.19% | Val Loss: 0.2833, Acc: 44.44%\n",
      "Epoch 60: Train Loss: 0.2910, Acc: 37.96% | Val Loss: 0.2827, Acc: 44.44%\n",
      "Epoch 70: Train Loss: 0.2976, Acc: 37.96% | Val Loss: 0.2822, Acc: 44.44%\n",
      "Epoch 80: Train Loss: 0.2958, Acc: 37.04% | Val Loss: 0.2818, Acc: 44.44%\n",
      "Epoch 90: Train Loss: 0.2925, Acc: 37.04% | Val Loss: 0.2814, Acc: 44.44%\n",
      "Epoch 100: Train Loss: 0.2930, Acc: 36.11% | Val Loss: 0.2810, Acc: 44.44%\n",
      "Epoch 110: Train Loss: 0.2832, Acc: 38.89% | Val Loss: 0.2807, Acc: 44.44%\n",
      "Epoch 120: Train Loss: 0.2921, Acc: 37.96% | Val Loss: 0.2804, Acc: 44.44%\n",
      "Epoch 130: Train Loss: 0.2836, Acc: 37.96% | Val Loss: 0.2802, Acc: 44.44%\n",
      "Epoch 140: Train Loss: 0.2835, Acc: 41.67% | Val Loss: 0.2799, Acc: 44.44%\n",
      "Final Validation Accuracy: 0.4444\n",
      " Fold 4/5 | Validation Accuracy: 0.4444\n",
      "[3] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.0, 'l2': 0.001, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2814, Acc: 31.48% | Val Loss: 0.2676, Acc: 37.04%\n",
      "Epoch 10: Train Loss: 0.2771, Acc: 30.56% | Val Loss: 0.2687, Acc: 37.04%\n",
      "Epoch 20: Train Loss: 0.2724, Acc: 33.33% | Val Loss: 0.2682, Acc: 37.04%\n",
      "Epoch 30: Train Loss: 0.2711, Acc: 34.26% | Val Loss: 0.2675, Acc: 37.04%\n",
      "Epoch 40: Train Loss: 0.2710, Acc: 36.11% | Val Loss: 0.2667, Acc: 37.04%\n",
      "Epoch 50: Train Loss: 0.2717, Acc: 35.19% | Val Loss: 0.2660, Acc: 37.04%\n",
      "Epoch 60: Train Loss: 0.2691, Acc: 38.89% | Val Loss: 0.2654, Acc: 37.04%\n",
      "Epoch 70: Train Loss: 0.2660, Acc: 41.67% | Val Loss: 0.2649, Acc: 37.04%\n",
      "Epoch 80: Train Loss: 0.2714, Acc: 35.19% | Val Loss: 0.2645, Acc: 37.04%\n",
      "Epoch 90: Train Loss: 0.2684, Acc: 34.26% | Val Loss: 0.2640, Acc: 37.04%\n",
      "Epoch 100: Train Loss: 0.2714, Acc: 31.48% | Val Loss: 0.2637, Acc: 37.04%\n",
      "Epoch 110: Train Loss: 0.2691, Acc: 35.19% | Val Loss: 0.2633, Acc: 37.04%\n",
      "Epoch 120: Train Loss: 0.2719, Acc: 29.63% | Val Loss: 0.2630, Acc: 37.04%\n",
      "Epoch 130: Train Loss: 0.2657, Acc: 36.11% | Val Loss: 0.2627, Acc: 37.04%\n",
      "Epoch 140: Train Loss: 0.2636, Acc: 42.59% | Val Loss: 0.2624, Acc: 37.04%\n",
      "Final Validation Accuracy: 0.3704\n",
      " Fold 5/5 | Validation Accuracy: 0.3704\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5556\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2500, Acc: 60.19% | Val Loss: 0.2745, Acc: 62.96%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 1/5 | Validation Accuracy: 0.6296\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2934, Acc: 54.63% | Val Loss: 0.4326, Acc: 40.74%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4074\n",
      " Fold 2/5 | Validation Accuracy: 0.4074\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3196, Acc: 37.96% | Val Loss: 0.2905, Acc: 48.15%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4815\n",
      " Fold 3/5 | Validation Accuracy: 0.4815\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3337, Acc: 37.96% | Val Loss: 0.2838, Acc: 48.15%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4815\n",
      " Fold 4/5 | Validation Accuracy: 0.4815\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3665, Acc: 41.67% | Val Loss: 0.3031, Acc: 44.44%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4444\n",
      " Fold 5/5 | Validation Accuracy: 0.4444\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.4889\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 1e-05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2514, Acc: 57.41% | Val Loss: 0.2713, Acc: 66.67%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 1/5 | Validation Accuracy: 0.6667\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 1e-05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2425, Acc: 55.56% | Val Loss: 0.2438, Acc: 51.85%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5185\n",
      " Fold 2/5 | Validation Accuracy: 0.5185\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 1e-05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2642, Acc: 53.70% | Val Loss: 0.2579, Acc: 55.56%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 3/5 | Validation Accuracy: 0.5556\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 1e-05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2565, Acc: 55.56% | Val Loss: 0.3675, Acc: 55.56%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 4/5 | Validation Accuracy: 0.5556\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 1e-05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2640, Acc: 42.59% | Val Loss: 0.2560, Acc: 40.74%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4074\n",
      " Fold 5/5 | Validation Accuracy: 0.4074\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5407\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.01, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3355, Acc: 57.41% | Val Loss: 0.2971, Acc: 51.85%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5185\n",
      " Fold 1/5 | Validation Accuracy: 0.5185\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.01, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3305, Acc: 51.85% | Val Loss: 0.3495, Acc: 59.26%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 2/5 | Validation Accuracy: 0.5926\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.01, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2512, Acc: 56.48% | Val Loss: 0.2317, Acc: 70.37%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 3/5 | Validation Accuracy: 0.7037\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.01, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3335, Acc: 37.96% | Val Loss: 0.2639, Acc: 70.37%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 4/5 | Validation Accuracy: 0.7037\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.01, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3638, Acc: 54.63% | Val Loss: 0.3542, Acc: 66.67%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 5/5 | Validation Accuracy: 0.6667\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.6370\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3037, Acc: 50.00% | Val Loss: 0.2471, Acc: 70.37%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 1/5 | Validation Accuracy: 0.7037\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4597, Acc: 45.37% | Val Loss: 0.3867, Acc: 55.56%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 2/5 | Validation Accuracy: 0.5556\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3718, Acc: 54.63% | Val Loss: 0.3217, Acc: 48.15%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4815\n",
      " Fold 3/5 | Validation Accuracy: 0.4815\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2596, Acc: 69.44% | Val Loss: 0.4066, Acc: 55.56%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 4/5 | Validation Accuracy: 0.5556\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4181, Acc: 50.00% | Val Loss: 0.4771, Acc: 44.44%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4444\n",
      " Fold 5/5 | Validation Accuracy: 0.4444\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5481\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 1e-05, 'l2': 0.0001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3538, Acc: 49.07% | Val Loss: 0.4662, Acc: 22.22%\n",
      "Epoch 10: Train Loss: 0.3009, Acc: 62.04% | Val Loss: 0.3911, Acc: 44.44%\n",
      "Epoch 20: Train Loss: 0.3008, Acc: 62.04% | Val Loss: 0.3478, Acc: 51.85%\n",
      "Epoch 30: Train Loss: 0.3091, Acc: 52.78% | Val Loss: 0.3193, Acc: 55.56%\n",
      "Epoch 40: Train Loss: 0.2861, Acc: 55.56% | Val Loss: 0.3005, Acc: 55.56%\n",
      "Epoch 50: Train Loss: 0.2780, Acc: 56.48% | Val Loss: 0.2837, Acc: 55.56%\n",
      "Epoch 60: Train Loss: 0.2598, Acc: 60.19% | Val Loss: 0.2687, Acc: 51.85%\n",
      "Epoch 70: Train Loss: 0.2244, Acc: 69.44% | Val Loss: 0.2569, Acc: 55.56%\n",
      "Epoch 80: Train Loss: 0.2313, Acc: 66.67% | Val Loss: 0.2493, Acc: 51.85%\n",
      "Epoch 90: Train Loss: 0.2113, Acc: 70.37% | Val Loss: 0.2397, Acc: 51.85%\n",
      "Epoch 100: Train Loss: 0.2289, Acc: 65.74% | Val Loss: 0.2307, Acc: 59.26%\n",
      "Epoch 110: Train Loss: 0.2311, Acc: 66.67% | Val Loss: 0.2263, Acc: 59.26%\n",
      "Epoch 120: Train Loss: 0.2028, Acc: 69.44% | Val Loss: 0.2245, Acc: 59.26%\n",
      "Epoch 130: Train Loss: 0.2124, Acc: 71.30% | Val Loss: 0.2231, Acc: 59.26%\n",
      "Epoch 140: Train Loss: 0.2227, Acc: 68.52% | Val Loss: 0.2186, Acc: 59.26%\n",
      "Epoch 150: Train Loss: 0.1868, Acc: 74.07% | Val Loss: 0.2191, Acc: 62.96%\n",
      "Epoch 160: Train Loss: 0.1957, Acc: 70.37% | Val Loss: 0.2184, Acc: 62.96%\n",
      "Epoch 170: Train Loss: 0.1835, Acc: 75.00% | Val Loss: 0.2163, Acc: 62.96%\n",
      "Epoch 180: Train Loss: 0.1893, Acc: 75.93% | Val Loss: 0.2168, Acc: 62.96%\n",
      "Epoch 190: Train Loss: 0.2031, Acc: 73.15% | Val Loss: 0.2169, Acc: 59.26%\n",
      "Early stopping at epoch 190\n",
      "Restoring model weights from epoch 170\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 1/5 | Validation Accuracy: 0.5926\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 1e-05, 'l2': 0.0001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2714, Acc: 62.96% | Val Loss: 0.3061, Acc: 66.67%\n",
      "Epoch 10: Train Loss: 0.2539, Acc: 67.59% | Val Loss: 0.2637, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.2300, Acc: 67.59% | Val Loss: 0.2549, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.2301, Acc: 67.59% | Val Loss: 0.2514, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.2241, Acc: 68.52% | Val Loss: 0.2486, Acc: 66.67%\n",
      "Epoch 50: Train Loss: 0.2185, Acc: 64.81% | Val Loss: 0.2459, Acc: 66.67%\n",
      "Epoch 60: Train Loss: 0.2043, Acc: 69.44% | Val Loss: 0.2441, Acc: 66.67%\n",
      "Epoch 70: Train Loss: 0.2077, Acc: 66.67% | Val Loss: 0.2441, Acc: 70.37%\n",
      "Epoch 80: Train Loss: 0.1634, Acc: 75.00% | Val Loss: 0.2442, Acc: 70.37%\n",
      "Early stopping at epoch 83\n",
      "Restoring model weights from epoch 63\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 2/5 | Validation Accuracy: 0.7037\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 1e-05, 'l2': 0.0001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2917, Acc: 50.00% | Val Loss: 0.2858, Acc: 37.04%\n",
      "Epoch 10: Train Loss: 0.2883, Acc: 63.89% | Val Loss: 0.2404, Acc: 74.07%\n",
      "Epoch 20: Train Loss: 0.2550, Acc: 65.74% | Val Loss: 0.2366, Acc: 70.37%\n",
      "Epoch 30: Train Loss: 0.2479, Acc: 65.74% | Val Loss: 0.2369, Acc: 70.37%\n",
      "Epoch 40: Train Loss: 0.2518, Acc: 65.74% | Val Loss: 0.2366, Acc: 70.37%\n",
      "Epoch 50: Train Loss: 0.2753, Acc: 65.74% | Val Loss: 0.2368, Acc: 70.37%\n",
      "Epoch 60: Train Loss: 0.2362, Acc: 66.67% | Val Loss: 0.2375, Acc: 70.37%\n",
      "Early stopping at epoch 61\n",
      "Restoring model weights from epoch 41\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 3/5 | Validation Accuracy: 0.7037\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 1e-05, 'l2': 0.0001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2656, Acc: 59.26% | Val Loss: 0.2996, Acc: 55.56%\n",
      "Epoch 10: Train Loss: 0.2567, Acc: 55.56% | Val Loss: 0.2643, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.2446, Acc: 62.96% | Val Loss: 0.2616, Acc: 55.56%\n",
      "Epoch 30: Train Loss: 0.2381, Acc: 60.19% | Val Loss: 0.2629, Acc: 55.56%\n",
      "Early stopping at epoch 38\n",
      "Restoring model weights from epoch 18\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 4/5 | Validation Accuracy: 0.5556\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 1e-05, 'l2': 0.0001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3424, Acc: 52.78% | Val Loss: 0.4749, Acc: 37.04%\n",
      "Epoch 10: Train Loss: 0.3595, Acc: 44.44% | Val Loss: 0.3539, Acc: 44.44%\n",
      "Epoch 20: Train Loss: 0.2962, Acc: 61.11% | Val Loss: 0.3088, Acc: 37.04%\n",
      "Epoch 30: Train Loss: 0.2691, Acc: 62.96% | Val Loss: 0.2913, Acc: 48.15%\n",
      "Epoch 40: Train Loss: 0.2438, Acc: 67.59% | Val Loss: 0.2812, Acc: 55.56%\n",
      "Epoch 50: Train Loss: 0.2404, Acc: 64.81% | Val Loss: 0.2741, Acc: 59.26%\n",
      "Epoch 60: Train Loss: 0.2463, Acc: 63.89% | Val Loss: 0.2694, Acc: 62.96%\n",
      "Epoch 70: Train Loss: 0.2411, Acc: 61.11% | Val Loss: 0.2664, Acc: 66.67%\n",
      "Epoch 80: Train Loss: 0.2033, Acc: 69.44% | Val Loss: 0.2640, Acc: 62.96%\n",
      "Epoch 90: Train Loss: 0.2180, Acc: 66.67% | Val Loss: 0.2623, Acc: 66.67%\n",
      "Epoch 100: Train Loss: 0.2087, Acc: 68.52% | Val Loss: 0.2617, Acc: 62.96%\n",
      "Epoch 110: Train Loss: 0.2095, Acc: 62.96% | Val Loss: 0.2607, Acc: 62.96%\n",
      "Epoch 120: Train Loss: 0.2016, Acc: 66.67% | Val Loss: 0.2615, Acc: 66.67%\n",
      "Epoch 130: Train Loss: 0.1974, Acc: 69.44% | Val Loss: 0.2624, Acc: 66.67%\n",
      "Early stopping at epoch 131\n",
      "Restoring model weights from epoch 111\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 5/5 | Validation Accuracy: 0.6667\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.6444\n",
      "[5] [<class 'src.activation_functions.Activation_Tanh'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 1e-05, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2634, Acc: 56.48% | Val Loss: 0.2583, Acc: 70.37%\n",
      "Epoch 10: Train Loss: 0.2807, Acc: 54.63% | Val Loss: 0.2578, Acc: 70.37%\n",
      "Epoch 20: Train Loss: 0.2649, Acc: 58.33% | Val Loss: 0.2574, Acc: 70.37%\n",
      "Epoch 30: Train Loss: 0.2696, Acc: 51.85% | Val Loss: 0.2569, Acc: 70.37%\n",
      "Epoch 40: Train Loss: 0.2900, Acc: 50.93% | Val Loss: 0.2565, Acc: 70.37%\n",
      "Epoch 50: Train Loss: 0.2853, Acc: 50.93% | Val Loss: 0.2560, Acc: 70.37%\n",
      "Epoch 60: Train Loss: 0.2833, Acc: 48.15% | Val Loss: 0.2555, Acc: 70.37%\n",
      "Epoch 70: Train Loss: 0.2754, Acc: 54.63% | Val Loss: 0.2551, Acc: 70.37%\n",
      "Epoch 80: Train Loss: 0.2787, Acc: 58.33% | Val Loss: 0.2546, Acc: 70.37%\n",
      "Epoch 90: Train Loss: 0.2686, Acc: 57.41% | Val Loss: 0.2542, Acc: 70.37%\n",
      "Epoch 100: Train Loss: 0.2775, Acc: 57.41% | Val Loss: 0.2537, Acc: 70.37%\n",
      "Epoch 110: Train Loss: 0.2770, Acc: 52.78% | Val Loss: 0.2533, Acc: 70.37%\n",
      "Epoch 120: Train Loss: 0.2492, Acc: 61.11% | Val Loss: 0.2528, Acc: 70.37%\n",
      "Epoch 130: Train Loss: 0.2749, Acc: 50.93% | Val Loss: 0.2524, Acc: 70.37%\n",
      "Epoch 140: Train Loss: 0.2711, Acc: 54.63% | Val Loss: 0.2520, Acc: 70.37%\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 1/5 | Validation Accuracy: 0.7037\n",
      "[5] [<class 'src.activation_functions.Activation_Tanh'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 1e-05, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3559, Acc: 52.78% | Val Loss: 0.3715, Acc: 59.26%\n",
      "Epoch 10: Train Loss: 0.3675, Acc: 52.78% | Val Loss: 0.3710, Acc: 59.26%\n",
      "Epoch 20: Train Loss: 0.3651, Acc: 51.85% | Val Loss: 0.3706, Acc: 59.26%\n",
      "Epoch 30: Train Loss: 0.3643, Acc: 54.63% | Val Loss: 0.3702, Acc: 59.26%\n",
      "Epoch 40: Train Loss: 0.3368, Acc: 60.19% | Val Loss: 0.3698, Acc: 59.26%\n",
      "Epoch 50: Train Loss: 0.3512, Acc: 54.63% | Val Loss: 0.3694, Acc: 59.26%\n",
      "Epoch 60: Train Loss: 0.3629, Acc: 52.78% | Val Loss: 0.3690, Acc: 59.26%\n",
      "Epoch 70: Train Loss: 0.3796, Acc: 51.85% | Val Loss: 0.3686, Acc: 59.26%\n",
      "Epoch 80: Train Loss: 0.3774, Acc: 51.85% | Val Loss: 0.3682, Acc: 59.26%\n",
      "Epoch 90: Train Loss: 0.3315, Acc: 54.63% | Val Loss: 0.3678, Acc: 59.26%\n",
      "Epoch 100: Train Loss: 0.3649, Acc: 54.63% | Val Loss: 0.3673, Acc: 59.26%\n",
      "Epoch 110: Train Loss: 0.4068, Acc: 49.07% | Val Loss: 0.3669, Acc: 59.26%\n",
      "Epoch 120: Train Loss: 0.3491, Acc: 55.56% | Val Loss: 0.3665, Acc: 59.26%\n",
      "Epoch 130: Train Loss: 0.3911, Acc: 52.78% | Val Loss: 0.3660, Acc: 59.26%\n",
      "Epoch 140: Train Loss: 0.3294, Acc: 55.56% | Val Loss: 0.3656, Acc: 59.26%\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 2/5 | Validation Accuracy: 0.5926\n",
      "[5] [<class 'src.activation_functions.Activation_Tanh'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 1e-05, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4059, Acc: 45.37% | Val Loss: 0.3973, Acc: 44.44%\n",
      "Epoch 10: Train Loss: 0.3622, Acc: 47.22% | Val Loss: 0.3966, Acc: 44.44%\n",
      "Epoch 20: Train Loss: 0.3518, Acc: 52.78% | Val Loss: 0.3960, Acc: 44.44%\n",
      "Epoch 30: Train Loss: 0.4141, Acc: 42.59% | Val Loss: 0.3953, Acc: 44.44%\n",
      "Epoch 40: Train Loss: 0.3619, Acc: 53.70% | Val Loss: 0.3947, Acc: 44.44%\n",
      "Epoch 50: Train Loss: 0.3523, Acc: 50.00% | Val Loss: 0.3941, Acc: 48.15%\n",
      "Epoch 60: Train Loss: 0.3609, Acc: 50.00% | Val Loss: 0.3935, Acc: 48.15%\n",
      "Epoch 70: Train Loss: 0.3405, Acc: 52.78% | Val Loss: 0.3928, Acc: 48.15%\n",
      "Epoch 80: Train Loss: 0.3754, Acc: 46.30% | Val Loss: 0.3921, Acc: 48.15%\n",
      "Epoch 90: Train Loss: 0.3639, Acc: 50.00% | Val Loss: 0.3914, Acc: 48.15%\n",
      "Epoch 100: Train Loss: 0.3887, Acc: 44.44% | Val Loss: 0.3908, Acc: 51.85%\n",
      "Epoch 110: Train Loss: 0.4310, Acc: 40.74% | Val Loss: 0.3901, Acc: 51.85%\n",
      "Epoch 120: Train Loss: 0.3816, Acc: 40.74% | Val Loss: 0.3894, Acc: 51.85%\n",
      "Epoch 130: Train Loss: 0.3678, Acc: 43.52% | Val Loss: 0.3888, Acc: 51.85%\n",
      "Epoch 140: Train Loss: 0.3988, Acc: 45.37% | Val Loss: 0.3881, Acc: 51.85%\n",
      "Final Validation Accuracy: 0.5185\n",
      " Fold 3/5 | Validation Accuracy: 0.5185\n",
      "[5] [<class 'src.activation_functions.Activation_Tanh'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 1e-05, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3164, Acc: 46.30% | Val Loss: 0.2954, Acc: 55.56%\n",
      "Epoch 10: Train Loss: 0.2810, Acc: 60.19% | Val Loss: 0.2951, Acc: 55.56%\n",
      "Epoch 20: Train Loss: 0.2865, Acc: 50.93% | Val Loss: 0.2948, Acc: 55.56%\n",
      "Epoch 30: Train Loss: 0.2815, Acc: 56.48% | Val Loss: 0.2946, Acc: 55.56%\n",
      "Epoch 40: Train Loss: 0.2776, Acc: 56.48% | Val Loss: 0.2943, Acc: 55.56%\n",
      "Epoch 50: Train Loss: 0.2989, Acc: 52.78% | Val Loss: 0.2941, Acc: 55.56%\n",
      "Epoch 60: Train Loss: 0.2947, Acc: 50.93% | Val Loss: 0.2939, Acc: 55.56%\n",
      "Epoch 70: Train Loss: 0.2750, Acc: 58.33% | Val Loss: 0.2937, Acc: 55.56%\n",
      "Epoch 80: Train Loss: 0.2831, Acc: 54.63% | Val Loss: 0.2935, Acc: 55.56%\n",
      "Epoch 90: Train Loss: 0.3172, Acc: 49.07% | Val Loss: 0.2933, Acc: 55.56%\n",
      "Epoch 100: Train Loss: 0.2800, Acc: 55.56% | Val Loss: 0.2930, Acc: 55.56%\n",
      "Epoch 110: Train Loss: 0.2826, Acc: 60.19% | Val Loss: 0.2928, Acc: 55.56%\n",
      "Epoch 120: Train Loss: 0.3064, Acc: 49.07% | Val Loss: 0.2926, Acc: 55.56%\n",
      "Epoch 130: Train Loss: 0.3162, Acc: 50.00% | Val Loss: 0.2924, Acc: 55.56%\n",
      "Epoch 140: Train Loss: 0.3115, Acc: 49.07% | Val Loss: 0.2922, Acc: 55.56%\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 4/5 | Validation Accuracy: 0.5556\n",
      "[5] [<class 'src.activation_functions.Activation_Tanh'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 1e-05, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3215, Acc: 52.78% | Val Loss: 0.3046, Acc: 51.85%\n",
      "Epoch 10: Train Loss: 0.3309, Acc: 51.85% | Val Loss: 0.3043, Acc: 51.85%\n",
      "Epoch 20: Train Loss: 0.2850, Acc: 57.41% | Val Loss: 0.3040, Acc: 51.85%\n",
      "Epoch 30: Train Loss: 0.3522, Acc: 48.15% | Val Loss: 0.3037, Acc: 51.85%\n",
      "Epoch 40: Train Loss: 0.3607, Acc: 50.93% | Val Loss: 0.3034, Acc: 51.85%\n",
      "Epoch 50: Train Loss: 0.3386, Acc: 51.85% | Val Loss: 0.3031, Acc: 51.85%\n",
      "Epoch 60: Train Loss: 0.3340, Acc: 54.63% | Val Loss: 0.3029, Acc: 51.85%\n",
      "Epoch 70: Train Loss: 0.3394, Acc: 51.85% | Val Loss: 0.3026, Acc: 51.85%\n",
      "Epoch 80: Train Loss: 0.3274, Acc: 54.63% | Val Loss: 0.3024, Acc: 51.85%\n",
      "Epoch 90: Train Loss: 0.3477, Acc: 50.93% | Val Loss: 0.3021, Acc: 51.85%\n",
      "Epoch 100: Train Loss: 0.3590, Acc: 45.37% | Val Loss: 0.3019, Acc: 51.85%\n",
      "Epoch 110: Train Loss: 0.3375, Acc: 50.93% | Val Loss: 0.3016, Acc: 51.85%\n",
      "Epoch 120: Train Loss: 0.3304, Acc: 51.85% | Val Loss: 0.3014, Acc: 51.85%\n",
      "Epoch 130: Train Loss: 0.3370, Acc: 52.78% | Val Loss: 0.3012, Acc: 51.85%\n",
      "Epoch 140: Train Loss: 0.3284, Acc: 52.78% | Val Loss: 0.3010, Acc: 51.85%\n",
      "Final Validation Accuracy: 0.5185\n",
      " Fold 5/5 | Validation Accuracy: 0.5185\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5778\n",
      "[4] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3436, Acc: 51.85% | Val Loss: 0.3971, Acc: 44.44%\n",
      "Epoch 10: Train Loss: 0.3463, Acc: 51.85% | Val Loss: 0.3909, Acc: 44.44%\n",
      "Epoch 20: Train Loss: 0.3367, Acc: 52.78% | Val Loss: 0.3849, Acc: 48.15%\n",
      "Epoch 30: Train Loss: 0.3523, Acc: 51.85% | Val Loss: 0.3797, Acc: 48.15%\n",
      "Epoch 40: Train Loss: 0.3383, Acc: 47.22% | Val Loss: 0.3747, Acc: 48.15%\n",
      "Epoch 50: Train Loss: 0.3331, Acc: 51.85% | Val Loss: 0.3698, Acc: 48.15%\n",
      "Epoch 60: Train Loss: 0.3214, Acc: 53.70% | Val Loss: 0.3650, Acc: 48.15%\n",
      "Epoch 70: Train Loss: 0.3103, Acc: 51.85% | Val Loss: 0.3601, Acc: 48.15%\n",
      "Epoch 80: Train Loss: 0.3130, Acc: 53.70% | Val Loss: 0.3556, Acc: 48.15%\n",
      "Epoch 90: Train Loss: 0.3045, Acc: 54.63% | Val Loss: 0.3517, Acc: 48.15%\n",
      "Epoch 100: Train Loss: 0.3255, Acc: 50.93% | Val Loss: 0.3478, Acc: 48.15%\n",
      "Epoch 110: Train Loss: 0.2986, Acc: 53.70% | Val Loss: 0.3439, Acc: 48.15%\n",
      "Epoch 120: Train Loss: 0.3084, Acc: 51.85% | Val Loss: 0.3403, Acc: 48.15%\n",
      "Epoch 130: Train Loss: 0.3166, Acc: 51.85% | Val Loss: 0.3366, Acc: 48.15%\n",
      "Epoch 140: Train Loss: 0.3182, Acc: 49.07% | Val Loss: 0.3327, Acc: 48.15%\n",
      "Epoch 150: Train Loss: 0.3007, Acc: 52.78% | Val Loss: 0.3293, Acc: 48.15%\n",
      "Epoch 160: Train Loss: 0.3083, Acc: 51.85% | Val Loss: 0.3262, Acc: 48.15%\n",
      "Epoch 170: Train Loss: 0.2943, Acc: 55.56% | Val Loss: 0.3229, Acc: 48.15%\n",
      "Epoch 180: Train Loss: 0.2859, Acc: 54.63% | Val Loss: 0.3195, Acc: 48.15%\n",
      "Epoch 190: Train Loss: 0.2886, Acc: 56.48% | Val Loss: 0.3164, Acc: 48.15%\n",
      "Final Validation Accuracy: 0.4815\n",
      " Fold 1/5 | Validation Accuracy: 0.4815\n",
      "[4] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3461, Acc: 43.52% | Val Loss: 0.2917, Acc: 44.44%\n",
      "Epoch 10: Train Loss: 0.3235, Acc: 49.07% | Val Loss: 0.2878, Acc: 44.44%\n",
      "Epoch 20: Train Loss: 0.3295, Acc: 44.44% | Val Loss: 0.2845, Acc: 44.44%\n",
      "Epoch 30: Train Loss: 0.3092, Acc: 50.93% | Val Loss: 0.2814, Acc: 51.85%\n",
      "Epoch 40: Train Loss: 0.3249, Acc: 48.15% | Val Loss: 0.2788, Acc: 51.85%\n",
      "Epoch 50: Train Loss: 0.2988, Acc: 49.07% | Val Loss: 0.2765, Acc: 51.85%\n",
      "Epoch 60: Train Loss: 0.3150, Acc: 45.37% | Val Loss: 0.2744, Acc: 51.85%\n",
      "Epoch 70: Train Loss: 0.2975, Acc: 50.93% | Val Loss: 0.2723, Acc: 55.56%\n",
      "Epoch 80: Train Loss: 0.3074, Acc: 49.07% | Val Loss: 0.2705, Acc: 55.56%\n",
      "Epoch 90: Train Loss: 0.3076, Acc: 50.93% | Val Loss: 0.2689, Acc: 55.56%\n",
      "Epoch 100: Train Loss: 0.2918, Acc: 53.70% | Val Loss: 0.2675, Acc: 59.26%\n",
      "Epoch 110: Train Loss: 0.2879, Acc: 49.07% | Val Loss: 0.2662, Acc: 59.26%\n",
      "Epoch 120: Train Loss: 0.2856, Acc: 52.78% | Val Loss: 0.2650, Acc: 55.56%\n",
      "Epoch 130: Train Loss: 0.2925, Acc: 50.93% | Val Loss: 0.2638, Acc: 55.56%\n",
      "Epoch 140: Train Loss: 0.2788, Acc: 56.48% | Val Loss: 0.2627, Acc: 55.56%\n",
      "Epoch 150: Train Loss: 0.2780, Acc: 52.78% | Val Loss: 0.2616, Acc: 55.56%\n",
      "Epoch 160: Train Loss: 0.2748, Acc: 55.56% | Val Loss: 0.2605, Acc: 55.56%\n",
      "Epoch 170: Train Loss: 0.2869, Acc: 52.78% | Val Loss: 0.2595, Acc: 55.56%\n",
      "Epoch 180: Train Loss: 0.2784, Acc: 52.78% | Val Loss: 0.2585, Acc: 55.56%\n",
      "Epoch 190: Train Loss: 0.2706, Acc: 53.70% | Val Loss: 0.2576, Acc: 55.56%\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 2/5 | Validation Accuracy: 0.5556\n",
      "[4] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2970, Acc: 56.48% | Val Loss: 0.2896, Acc: 55.56%\n",
      "Epoch 10: Train Loss: 0.2853, Acc: 57.41% | Val Loss: 0.2882, Acc: 55.56%\n",
      "Epoch 20: Train Loss: 0.2955, Acc: 54.63% | Val Loss: 0.2871, Acc: 55.56%\n",
      "Epoch 30: Train Loss: 0.2772, Acc: 54.63% | Val Loss: 0.2862, Acc: 55.56%\n",
      "Epoch 40: Train Loss: 0.2667, Acc: 60.19% | Val Loss: 0.2854, Acc: 55.56%\n",
      "Epoch 50: Train Loss: 0.3005, Acc: 50.93% | Val Loss: 0.2846, Acc: 55.56%\n",
      "Epoch 60: Train Loss: 0.2852, Acc: 55.56% | Val Loss: 0.2838, Acc: 55.56%\n",
      "Epoch 70: Train Loss: 0.2802, Acc: 56.48% | Val Loss: 0.2830, Acc: 55.56%\n",
      "Epoch 80: Train Loss: 0.2706, Acc: 59.26% | Val Loss: 0.2821, Acc: 55.56%\n",
      "Epoch 90: Train Loss: 0.2707, Acc: 60.19% | Val Loss: 0.2814, Acc: 55.56%\n",
      "Epoch 100: Train Loss: 0.2645, Acc: 60.19% | Val Loss: 0.2808, Acc: 55.56%\n",
      "Epoch 110: Train Loss: 0.2690, Acc: 55.56% | Val Loss: 0.2802, Acc: 55.56%\n",
      "Epoch 120: Train Loss: 0.2790, Acc: 54.63% | Val Loss: 0.2793, Acc: 55.56%\n",
      "Epoch 130: Train Loss: 0.2457, Acc: 62.96% | Val Loss: 0.2784, Acc: 55.56%\n",
      "Epoch 140: Train Loss: 0.2428, Acc: 62.04% | Val Loss: 0.2777, Acc: 55.56%\n",
      "Epoch 150: Train Loss: 0.2563, Acc: 61.11% | Val Loss: 0.2770, Acc: 55.56%\n",
      "Epoch 160: Train Loss: 0.2605, Acc: 60.19% | Val Loss: 0.2762, Acc: 55.56%\n",
      "Epoch 170: Train Loss: 0.2526, Acc: 59.26% | Val Loss: 0.2755, Acc: 55.56%\n",
      "Epoch 180: Train Loss: 0.2578, Acc: 60.19% | Val Loss: 0.2748, Acc: 55.56%\n",
      "Epoch 190: Train Loss: 0.2542, Acc: 59.26% | Val Loss: 0.2741, Acc: 55.56%\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 3/5 | Validation Accuracy: 0.5556\n",
      "[4] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3332, Acc: 50.93% | Val Loss: 0.3271, Acc: 55.56%\n",
      "Epoch 10: Train Loss: 0.3560, Acc: 50.00% | Val Loss: 0.3218, Acc: 55.56%\n",
      "Epoch 20: Train Loss: 0.3218, Acc: 55.56% | Val Loss: 0.3168, Acc: 62.96%\n",
      "Epoch 30: Train Loss: 0.3124, Acc: 55.56% | Val Loss: 0.3124, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.3031, Acc: 54.63% | Val Loss: 0.3086, Acc: 62.96%\n",
      "Epoch 50: Train Loss: 0.3111, Acc: 51.85% | Val Loss: 0.3054, Acc: 62.96%\n",
      "Epoch 60: Train Loss: 0.2961, Acc: 53.70% | Val Loss: 0.3028, Acc: 66.67%\n",
      "Epoch 70: Train Loss: 0.2788, Acc: 56.48% | Val Loss: 0.3009, Acc: 62.96%\n",
      "Epoch 80: Train Loss: 0.2978, Acc: 54.63% | Val Loss: 0.2997, Acc: 66.67%\n",
      "Epoch 90: Train Loss: 0.2474, Acc: 58.33% | Val Loss: 0.2990, Acc: 66.67%\n",
      "Epoch 100: Train Loss: 0.2969, Acc: 54.63% | Val Loss: 0.2988, Acc: 70.37%\n",
      "Epoch 110: Train Loss: 0.2793, Acc: 59.26% | Val Loss: 0.2990, Acc: 66.67%\n",
      "Early stopping at epoch 119\n",
      "Restoring model weights from epoch 99\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 4/5 | Validation Accuracy: 0.6667\n",
      "[4] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3924, Acc: 47.22% | Val Loss: 0.3883, Acc: 48.15%\n",
      "Epoch 10: Train Loss: 0.3888, Acc: 48.15% | Val Loss: 0.3851, Acc: 48.15%\n",
      "Epoch 20: Train Loss: 0.4093, Acc: 49.07% | Val Loss: 0.3818, Acc: 48.15%\n",
      "Epoch 30: Train Loss: 0.4046, Acc: 48.15% | Val Loss: 0.3787, Acc: 51.85%\n",
      "Epoch 40: Train Loss: 0.3757, Acc: 49.07% | Val Loss: 0.3756, Acc: 51.85%\n",
      "Epoch 50: Train Loss: 0.3749, Acc: 50.93% | Val Loss: 0.3725, Acc: 51.85%\n",
      "Epoch 60: Train Loss: 0.3861, Acc: 49.07% | Val Loss: 0.3693, Acc: 51.85%\n",
      "Epoch 70: Train Loss: 0.3570, Acc: 53.70% | Val Loss: 0.3661, Acc: 51.85%\n",
      "Epoch 80: Train Loss: 0.3621, Acc: 53.70% | Val Loss: 0.3630, Acc: 51.85%\n",
      "Epoch 90: Train Loss: 0.3575, Acc: 53.70% | Val Loss: 0.3599, Acc: 51.85%\n",
      "Epoch 100: Train Loss: 0.3702, Acc: 50.93% | Val Loss: 0.3568, Acc: 51.85%\n",
      "Epoch 110: Train Loss: 0.3642, Acc: 50.00% | Val Loss: 0.3538, Acc: 48.15%\n",
      "Epoch 120: Train Loss: 0.3290, Acc: 55.56% | Val Loss: 0.3508, Acc: 48.15%\n",
      "Epoch 130: Train Loss: 0.3626, Acc: 50.00% | Val Loss: 0.3477, Acc: 48.15%\n",
      "Epoch 140: Train Loss: 0.3372, Acc: 52.78% | Val Loss: 0.3445, Acc: 48.15%\n",
      "Epoch 150: Train Loss: 0.3496, Acc: 50.93% | Val Loss: 0.3414, Acc: 48.15%\n",
      "Epoch 160: Train Loss: 0.3386, Acc: 51.85% | Val Loss: 0.3384, Acc: 48.15%\n",
      "Epoch 170: Train Loss: 0.3252, Acc: 57.41% | Val Loss: 0.3354, Acc: 48.15%\n",
      "Epoch 180: Train Loss: 0.3235, Acc: 52.78% | Val Loss: 0.3324, Acc: 48.15%\n",
      "Epoch 190: Train Loss: 0.3342, Acc: 53.70% | Val Loss: 0.3296, Acc: 51.85%\n",
      "Final Validation Accuracy: 0.5185\n",
      " Fold 5/5 | Validation Accuracy: 0.5185\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5556\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3847, Acc: 36.11% | Val Loss: 0.3691, Acc: 33.33%\n",
      "Epoch 10: Train Loss: 0.2473, Acc: 63.89% | Val Loss: 0.1772, Acc: 77.78%\n",
      "Epoch 20: Train Loss: 0.2291, Acc: 63.89% | Val Loss: 0.1974, Acc: 77.78%\n",
      "Early stopping at epoch 29\n",
      "Restoring model weights from epoch 9\n",
      "Final Validation Accuracy: 0.7778\n",
      " Fold 1/5 | Validation Accuracy: 0.7778\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4088, Acc: 35.19% | Val Loss: 0.3472, Acc: 37.04%\n",
      "Epoch 10: Train Loss: 0.2467, Acc: 66.67% | Val Loss: 0.2471, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.2226, Acc: 66.67% | Val Loss: 0.2310, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.2156, Acc: 67.59% | Val Loss: 0.2342, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.2082, Acc: 68.52% | Val Loss: 0.2303, Acc: 59.26%\n",
      "Epoch 50: Train Loss: 0.2027, Acc: 68.52% | Val Loss: 0.2312, Acc: 66.67%\n",
      "Epoch 60: Train Loss: 0.1939, Acc: 69.44% | Val Loss: 0.2342, Acc: 62.96%\n",
      "Early stopping at epoch 62\n",
      "Restoring model weights from epoch 42\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 2/5 | Validation Accuracy: 0.6296\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2647, Acc: 50.00% | Val Loss: 0.2226, Acc: 74.07%\n",
      "Epoch 10: Train Loss: 0.2028, Acc: 68.52% | Val Loss: 0.2340, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.1860, Acc: 74.07% | Val Loss: 0.2280, Acc: 66.67%\n",
      "Early stopping at epoch 21\n",
      "Restoring model weights from epoch 1\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 3/5 | Validation Accuracy: 0.6667\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3410, Acc: 42.59% | Val Loss: 0.2651, Acc: 51.85%\n",
      "Epoch 10: Train Loss: 0.2343, Acc: 69.44% | Val Loss: 0.3228, Acc: 55.56%\n",
      "Epoch 20: Train Loss: 0.2093, Acc: 66.67% | Val Loss: 0.2567, Acc: 44.44%\n",
      "Epoch 30: Train Loss: 0.1977, Acc: 70.37% | Val Loss: 0.2746, Acc: 59.26%\n",
      "Early stopping at epoch 39\n",
      "Restoring model weights from epoch 19\n",
      "Final Validation Accuracy: 0.5185\n",
      " Fold 4/5 | Validation Accuracy: 0.5185\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2443, Acc: 70.37% | Val Loss: 0.2942, Acc: 62.96%\n",
      "Epoch 10: Train Loss: 0.2077, Acc: 67.59% | Val Loss: 0.2554, Acc: 62.96%\n",
      "Epoch 20: Train Loss: 0.1904, Acc: 72.22% | Val Loss: 0.2653, Acc: 62.96%\n",
      "Epoch 30: Train Loss: 0.1681, Acc: 74.07% | Val Loss: 0.2751, Acc: 66.67%\n",
      "Early stopping at epoch 33\n",
      "Restoring model weights from epoch 13\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 5/5 | Validation Accuracy: 0.6667\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.6519\n",
      "[6] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3219, Acc: 62.96% | Val Loss: 0.2076, Acc: 77.78%\n",
      "Epoch 10: Train Loss: 0.3175, Acc: 62.04% | Val Loss: 0.2069, Acc: 77.78%\n",
      "Epoch 20: Train Loss: 0.3459, Acc: 61.11% | Val Loss: 0.2063, Acc: 77.78%\n",
      "Epoch 30: Train Loss: 0.3203, Acc: 62.04% | Val Loss: 0.2058, Acc: 77.78%\n",
      "Epoch 40: Train Loss: 0.3098, Acc: 62.96% | Val Loss: 0.2051, Acc: 77.78%\n",
      "Epoch 50: Train Loss: 0.3100, Acc: 63.89% | Val Loss: 0.2046, Acc: 77.78%\n",
      "Epoch 60: Train Loss: 0.3512, Acc: 61.11% | Val Loss: 0.2040, Acc: 77.78%\n",
      "Epoch 70: Train Loss: 0.3293, Acc: 60.19% | Val Loss: 0.2036, Acc: 77.78%\n",
      "Epoch 80: Train Loss: 0.2993, Acc: 62.04% | Val Loss: 0.2032, Acc: 77.78%\n",
      "Epoch 90: Train Loss: 0.3220, Acc: 61.11% | Val Loss: 0.2027, Acc: 77.78%\n",
      "Epoch 100: Train Loss: 0.3025, Acc: 65.74% | Val Loss: 0.2021, Acc: 77.78%\n",
      "Epoch 110: Train Loss: 0.3050, Acc: 64.81% | Val Loss: 0.2016, Acc: 77.78%\n",
      "Epoch 120: Train Loss: 0.3303, Acc: 57.41% | Val Loss: 0.2011, Acc: 74.07%\n",
      "Epoch 130: Train Loss: 0.3278, Acc: 59.26% | Val Loss: 0.2009, Acc: 74.07%\n",
      "Epoch 140: Train Loss: 0.2863, Acc: 64.81% | Val Loss: 0.2007, Acc: 74.07%\n",
      "Epoch 150: Train Loss: 0.3179, Acc: 62.96% | Val Loss: 0.2005, Acc: 74.07%\n",
      "Epoch 160: Train Loss: 0.3011, Acc: 61.11% | Val Loss: 0.2004, Acc: 74.07%\n",
      "Epoch 170: Train Loss: 0.3378, Acc: 57.41% | Val Loss: 0.2004, Acc: 74.07%\n",
      "Epoch 180: Train Loss: 0.3033, Acc: 62.04% | Val Loss: 0.2005, Acc: 70.37%\n",
      "Epoch 190: Train Loss: 0.2856, Acc: 65.74% | Val Loss: 0.2006, Acc: 70.37%\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 1/5 | Validation Accuracy: 0.7037\n",
      "[6] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3389, Acc: 42.59% | Val Loss: 0.2626, Acc: 44.44%\n",
      "Epoch 10: Train Loss: 0.3084, Acc: 54.63% | Val Loss: 0.2563, Acc: 40.74%\n",
      "Epoch 20: Train Loss: 0.2577, Acc: 60.19% | Val Loss: 0.2508, Acc: 44.44%\n",
      "Epoch 30: Train Loss: 0.2755, Acc: 52.78% | Val Loss: 0.2462, Acc: 48.15%\n",
      "Epoch 40: Train Loss: 0.3434, Acc: 41.67% | Val Loss: 0.2422, Acc: 48.15%\n",
      "Epoch 50: Train Loss: 0.2687, Acc: 53.70% | Val Loss: 0.2389, Acc: 51.85%\n",
      "Epoch 60: Train Loss: 0.2665, Acc: 55.56% | Val Loss: 0.2362, Acc: 59.26%\n",
      "Epoch 70: Train Loss: 0.2685, Acc: 57.41% | Val Loss: 0.2341, Acc: 62.96%\n",
      "Epoch 80: Train Loss: 0.2365, Acc: 61.11% | Val Loss: 0.2323, Acc: 66.67%\n",
      "Epoch 90: Train Loss: 0.2448, Acc: 65.74% | Val Loss: 0.2310, Acc: 66.67%\n",
      "Epoch 100: Train Loss: 0.2554, Acc: 56.48% | Val Loss: 0.2301, Acc: 66.67%\n",
      "Epoch 110: Train Loss: 0.2732, Acc: 56.48% | Val Loss: 0.2295, Acc: 66.67%\n",
      "Epoch 120: Train Loss: 0.2670, Acc: 55.56% | Val Loss: 0.2290, Acc: 66.67%\n",
      "Epoch 130: Train Loss: 0.2655, Acc: 57.41% | Val Loss: 0.2287, Acc: 66.67%\n",
      "Epoch 140: Train Loss: 0.2505, Acc: 64.81% | Val Loss: 0.2285, Acc: 66.67%\n",
      "Epoch 150: Train Loss: 0.2530, Acc: 58.33% | Val Loss: 0.2283, Acc: 66.67%\n",
      "Epoch 160: Train Loss: 0.2639, Acc: 59.26% | Val Loss: 0.2282, Acc: 66.67%\n",
      "Epoch 170: Train Loss: 0.2489, Acc: 62.96% | Val Loss: 0.2281, Acc: 66.67%\n",
      "Epoch 180: Train Loss: 0.2253, Acc: 66.67% | Val Loss: 0.2280, Acc: 66.67%\n",
      "Epoch 190: Train Loss: 0.2643, Acc: 58.33% | Val Loss: 0.2280, Acc: 66.67%\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 2/5 | Validation Accuracy: 0.6667\n",
      "[6] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2887, Acc: 54.63% | Val Loss: 0.2506, Acc: 66.67%\n",
      "Epoch 10: Train Loss: 0.3091, Acc: 52.78% | Val Loss: 0.2466, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.3058, Acc: 50.93% | Val Loss: 0.2427, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.2998, Acc: 50.00% | Val Loss: 0.2396, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.2900, Acc: 55.56% | Val Loss: 0.2370, Acc: 66.67%\n",
      "Epoch 50: Train Loss: 0.3062, Acc: 55.56% | Val Loss: 0.2347, Acc: 66.67%\n",
      "Epoch 60: Train Loss: 0.2624, Acc: 57.41% | Val Loss: 0.2329, Acc: 70.37%\n",
      "Epoch 70: Train Loss: 0.2975, Acc: 54.63% | Val Loss: 0.2310, Acc: 74.07%\n",
      "Epoch 80: Train Loss: 0.2729, Acc: 59.26% | Val Loss: 0.2294, Acc: 74.07%\n",
      "Epoch 90: Train Loss: 0.2569, Acc: 61.11% | Val Loss: 0.2281, Acc: 74.07%\n",
      "Epoch 100: Train Loss: 0.2604, Acc: 60.19% | Val Loss: 0.2270, Acc: 74.07%\n",
      "Epoch 110: Train Loss: 0.2687, Acc: 59.26% | Val Loss: 0.2261, Acc: 74.07%\n",
      "Epoch 120: Train Loss: 0.2681, Acc: 57.41% | Val Loss: 0.2254, Acc: 74.07%\n",
      "Epoch 130: Train Loss: 0.2869, Acc: 51.85% | Val Loss: 0.2248, Acc: 74.07%\n",
      "Epoch 140: Train Loss: 0.2804, Acc: 60.19% | Val Loss: 0.2243, Acc: 74.07%\n",
      "Epoch 150: Train Loss: 0.2687, Acc: 59.26% | Val Loss: 0.2237, Acc: 74.07%\n",
      "Epoch 160: Train Loss: 0.2539, Acc: 64.81% | Val Loss: 0.2232, Acc: 74.07%\n",
      "Epoch 170: Train Loss: 0.2896, Acc: 59.26% | Val Loss: 0.2227, Acc: 74.07%\n",
      "Epoch 180: Train Loss: 0.2840, Acc: 59.26% | Val Loss: 0.2223, Acc: 74.07%\n",
      "Epoch 190: Train Loss: 0.2944, Acc: 57.41% | Val Loss: 0.2219, Acc: 74.07%\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 3/5 | Validation Accuracy: 0.7037\n",
      "[6] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2760, Acc: 52.78% | Val Loss: 0.2583, Acc: 40.74%\n",
      "Epoch 10: Train Loss: 0.2669, Acc: 59.26% | Val Loss: 0.2575, Acc: 40.74%\n",
      "Epoch 20: Train Loss: 0.2470, Acc: 58.33% | Val Loss: 0.2572, Acc: 37.04%\n",
      "Epoch 30: Train Loss: 0.2600, Acc: 54.63% | Val Loss: 0.2572, Acc: 40.74%\n",
      "Epoch 40: Train Loss: 0.2582, Acc: 55.56% | Val Loss: 0.2576, Acc: 44.44%\n",
      "Epoch 50: Train Loss: 0.2663, Acc: 52.78% | Val Loss: 0.2580, Acc: 48.15%\n",
      "Epoch 60: Train Loss: 0.2877, Acc: 46.30% | Val Loss: 0.2586, Acc: 48.15%\n",
      "Epoch 70: Train Loss: 0.2435, Acc: 61.11% | Val Loss: 0.2593, Acc: 48.15%\n",
      "Early stopping at epoch 73\n",
      "Restoring model weights from epoch 23\n",
      "Final Validation Accuracy: 0.4815\n",
      " Fold 4/5 | Validation Accuracy: 0.4815\n",
      "[6] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3046, Acc: 54.63% | Val Loss: 0.2745, Acc: 59.26%\n",
      "Epoch 10: Train Loss: 0.3211, Acc: 51.85% | Val Loss: 0.2760, Acc: 59.26%\n",
      "Epoch 20: Train Loss: 0.2962, Acc: 59.26% | Val Loss: 0.2775, Acc: 59.26%\n",
      "Epoch 30: Train Loss: 0.3027, Acc: 55.56% | Val Loss: 0.2787, Acc: 59.26%\n",
      "Epoch 40: Train Loss: 0.2656, Acc: 62.96% | Val Loss: 0.2797, Acc: 59.26%\n",
      "Epoch 50: Train Loss: 0.3046, Acc: 56.48% | Val Loss: 0.2808, Acc: 59.26%\n",
      "Early stopping at epoch 50\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 5/5 | Validation Accuracy: 0.5926\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.6296\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.01, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.01, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3355, Acc: 57.41% | Val Loss: 0.3217, Acc: 44.44%\n",
      "Epoch 10: Train Loss: 0.3163, Acc: 58.33% | Val Loss: 0.3214, Acc: 48.15%\n",
      "Epoch 20: Train Loss: 0.3148, Acc: 57.41% | Val Loss: 0.3212, Acc: 48.15%\n",
      "Epoch 30: Train Loss: 0.3119, Acc: 58.33% | Val Loss: 0.3210, Acc: 48.15%\n",
      "Epoch 40: Train Loss: 0.3272, Acc: 57.41% | Val Loss: 0.3208, Acc: 48.15%\n",
      "Epoch 50: Train Loss: 0.3200, Acc: 58.33% | Val Loss: 0.3206, Acc: 48.15%\n",
      "Epoch 60: Train Loss: 0.3356, Acc: 56.48% | Val Loss: 0.3205, Acc: 48.15%\n",
      "Epoch 70: Train Loss: 0.3477, Acc: 58.33% | Val Loss: 0.3203, Acc: 48.15%\n",
      "Epoch 80: Train Loss: 0.2906, Acc: 62.96% | Val Loss: 0.3202, Acc: 48.15%\n",
      "Epoch 90: Train Loss: 0.3361, Acc: 55.56% | Val Loss: 0.3200, Acc: 48.15%\n",
      "Epoch 100: Train Loss: 0.3138, Acc: 58.33% | Val Loss: 0.3199, Acc: 48.15%\n",
      "Epoch 110: Train Loss: 0.3145, Acc: 61.11% | Val Loss: 0.3198, Acc: 48.15%\n",
      "Epoch 120: Train Loss: 0.3264, Acc: 55.56% | Val Loss: 0.3197, Acc: 48.15%\n",
      "Epoch 130: Train Loss: 0.3062, Acc: 62.96% | Val Loss: 0.3195, Acc: 48.15%\n",
      "Epoch 140: Train Loss: 0.3354, Acc: 54.63% | Val Loss: 0.3194, Acc: 48.15%\n",
      "Final Validation Accuracy: 0.4815\n",
      " Fold 1/5 | Validation Accuracy: 0.4815\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.01, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.01, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3516, Acc: 53.70% | Val Loss: 0.3660, Acc: 44.44%\n",
      "Epoch 10: Train Loss: 0.3702, Acc: 50.00% | Val Loss: 0.3657, Acc: 44.44%\n",
      "Epoch 20: Train Loss: 0.3627, Acc: 52.78% | Val Loss: 0.3654, Acc: 44.44%\n",
      "Epoch 30: Train Loss: 0.3540, Acc: 55.56% | Val Loss: 0.3651, Acc: 44.44%\n",
      "Epoch 40: Train Loss: 0.3665, Acc: 50.00% | Val Loss: 0.3648, Acc: 48.15%\n",
      "Epoch 50: Train Loss: 0.3564, Acc: 53.70% | Val Loss: 0.3646, Acc: 48.15%\n",
      "Epoch 60: Train Loss: 0.3636, Acc: 52.78% | Val Loss: 0.3644, Acc: 48.15%\n",
      "Epoch 70: Train Loss: 0.3551, Acc: 53.70% | Val Loss: 0.3642, Acc: 48.15%\n",
      "Epoch 80: Train Loss: 0.3658, Acc: 51.85% | Val Loss: 0.3640, Acc: 48.15%\n",
      "Epoch 90: Train Loss: 0.3507, Acc: 54.63% | Val Loss: 0.3638, Acc: 48.15%\n",
      "Epoch 100: Train Loss: 0.3641, Acc: 55.56% | Val Loss: 0.3636, Acc: 48.15%\n",
      "Epoch 110: Train Loss: 0.3472, Acc: 53.70% | Val Loss: 0.3634, Acc: 48.15%\n",
      "Epoch 120: Train Loss: 0.3480, Acc: 57.41% | Val Loss: 0.3633, Acc: 48.15%\n",
      "Epoch 130: Train Loss: 0.3602, Acc: 54.63% | Val Loss: 0.3631, Acc: 48.15%\n",
      "Epoch 140: Train Loss: 0.3533, Acc: 53.70% | Val Loss: 0.3630, Acc: 48.15%\n",
      "Final Validation Accuracy: 0.4815\n",
      " Fold 2/5 | Validation Accuracy: 0.4815\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.01, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.01, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3304, Acc: 52.78% | Val Loss: 0.3957, Acc: 55.56%\n",
      "Epoch 10: Train Loss: 0.3339, Acc: 52.78% | Val Loss: 0.3952, Acc: 55.56%\n",
      "Epoch 20: Train Loss: 0.3524, Acc: 52.78% | Val Loss: 0.3948, Acc: 55.56%\n",
      "Epoch 30: Train Loss: 0.3358, Acc: 52.78% | Val Loss: 0.3944, Acc: 55.56%\n",
      "Epoch 40: Train Loss: 0.3163, Acc: 57.41% | Val Loss: 0.3940, Acc: 55.56%\n",
      "Epoch 50: Train Loss: 0.3278, Acc: 52.78% | Val Loss: 0.3936, Acc: 55.56%\n",
      "Epoch 60: Train Loss: 0.3365, Acc: 50.93% | Val Loss: 0.3933, Acc: 55.56%\n",
      "Epoch 70: Train Loss: 0.3011, Acc: 57.41% | Val Loss: 0.3930, Acc: 55.56%\n",
      "Epoch 80: Train Loss: 0.3289, Acc: 54.63% | Val Loss: 0.3927, Acc: 55.56%\n",
      "Epoch 90: Train Loss: 0.3359, Acc: 53.70% | Val Loss: 0.3924, Acc: 55.56%\n",
      "Epoch 100: Train Loss: 0.3085, Acc: 55.56% | Val Loss: 0.3921, Acc: 55.56%\n",
      "Epoch 110: Train Loss: 0.3146, Acc: 55.56% | Val Loss: 0.3919, Acc: 55.56%\n",
      "Epoch 120: Train Loss: 0.3282, Acc: 54.63% | Val Loss: 0.3916, Acc: 55.56%\n",
      "Epoch 130: Train Loss: 0.3357, Acc: 53.70% | Val Loss: 0.3914, Acc: 55.56%\n",
      "Epoch 140: Train Loss: 0.3567, Acc: 52.78% | Val Loss: 0.3912, Acc: 55.56%\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 3/5 | Validation Accuracy: 0.5556\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.01, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.01, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3412, Acc: 51.85% | Val Loss: 0.3649, Acc: 66.67%\n",
      "Epoch 10: Train Loss: 0.3358, Acc: 53.70% | Val Loss: 0.3646, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.3275, Acc: 53.70% | Val Loss: 0.3644, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.3361, Acc: 54.63% | Val Loss: 0.3641, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.3242, Acc: 55.56% | Val Loss: 0.3639, Acc: 66.67%\n",
      "Epoch 50: Train Loss: 0.3272, Acc: 51.85% | Val Loss: 0.3637, Acc: 66.67%\n",
      "Epoch 60: Train Loss: 0.3271, Acc: 54.63% | Val Loss: 0.3635, Acc: 66.67%\n",
      "Epoch 70: Train Loss: 0.3110, Acc: 57.41% | Val Loss: 0.3633, Acc: 66.67%\n",
      "Epoch 80: Train Loss: 0.3217, Acc: 56.48% | Val Loss: 0.3631, Acc: 66.67%\n",
      "Epoch 90: Train Loss: 0.3329, Acc: 53.70% | Val Loss: 0.3630, Acc: 66.67%\n",
      "Epoch 100: Train Loss: 0.3293, Acc: 55.56% | Val Loss: 0.3628, Acc: 66.67%\n",
      "Epoch 110: Train Loss: 0.3193, Acc: 55.56% | Val Loss: 0.3627, Acc: 62.96%\n",
      "Epoch 120: Train Loss: 0.3177, Acc: 54.63% | Val Loss: 0.3625, Acc: 62.96%\n",
      "Epoch 130: Train Loss: 0.3257, Acc: 54.63% | Val Loss: 0.3624, Acc: 62.96%\n",
      "Epoch 140: Train Loss: 0.3272, Acc: 50.00% | Val Loss: 0.3623, Acc: 62.96%\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 4/5 | Validation Accuracy: 0.6296\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.01, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.01, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3313, Acc: 46.30% | Val Loss: 0.3127, Acc: 51.85%\n",
      "Epoch 10: Train Loss: 0.3006, Acc: 50.93% | Val Loss: 0.3124, Acc: 51.85%\n",
      "Epoch 20: Train Loss: 0.3217, Acc: 45.37% | Val Loss: 0.3121, Acc: 51.85%\n",
      "Epoch 30: Train Loss: 0.2888, Acc: 50.00% | Val Loss: 0.3118, Acc: 51.85%\n",
      "Epoch 40: Train Loss: 0.3253, Acc: 46.30% | Val Loss: 0.3115, Acc: 51.85%\n",
      "Epoch 50: Train Loss: 0.3309, Acc: 42.59% | Val Loss: 0.3112, Acc: 51.85%\n",
      "Epoch 60: Train Loss: 0.3260, Acc: 43.52% | Val Loss: 0.3110, Acc: 51.85%\n",
      "Epoch 70: Train Loss: 0.3268, Acc: 44.44% | Val Loss: 0.3108, Acc: 51.85%\n",
      "Epoch 80: Train Loss: 0.3342, Acc: 48.15% | Val Loss: 0.3105, Acc: 51.85%\n",
      "Epoch 90: Train Loss: 0.3227, Acc: 48.15% | Val Loss: 0.3104, Acc: 51.85%\n",
      "Epoch 100: Train Loss: 0.3234, Acc: 47.22% | Val Loss: 0.3102, Acc: 51.85%\n",
      "Epoch 110: Train Loss: 0.3156, Acc: 48.15% | Val Loss: 0.3100, Acc: 51.85%\n",
      "Epoch 120: Train Loss: 0.3081, Acc: 45.37% | Val Loss: 0.3098, Acc: 51.85%\n",
      "Epoch 130: Train Loss: 0.3246, Acc: 45.37% | Val Loss: 0.3097, Acc: 51.85%\n",
      "Epoch 140: Train Loss: 0.3144, Acc: 48.15% | Val Loss: 0.3095, Acc: 51.85%\n",
      "Final Validation Accuracy: 0.5185\n",
      " Fold 5/5 | Validation Accuracy: 0.5185\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5333\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3910, Acc: 34.26% | Val Loss: 0.3691, Acc: 33.33%\n",
      "Epoch 10: Train Loss: 0.2429, Acc: 63.89% | Val Loss: 0.1795, Acc: 77.78%\n",
      "Epoch 20: Train Loss: 0.2302, Acc: 63.89% | Val Loss: 0.2021, Acc: 77.78%\n",
      "Early stopping at epoch 27\n",
      "Restoring model weights from epoch 7\n",
      "Final Validation Accuracy: 0.7778\n",
      " Fold 1/5 | Validation Accuracy: 0.7778\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2622, Acc: 55.56% | Val Loss: 0.2352, Acc: 66.67%\n",
      "Epoch 10: Train Loss: 0.2210, Acc: 63.89% | Val Loss: 0.2338, Acc: 62.96%\n",
      "Epoch 20: Train Loss: 0.1962, Acc: 71.30% | Val Loss: 0.2339, Acc: 62.96%\n",
      "Epoch 30: Train Loss: 0.1747, Acc: 74.07% | Val Loss: 0.2417, Acc: 59.26%\n",
      "Early stopping at epoch 34\n",
      "Restoring model weights from epoch 14\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 2/5 | Validation Accuracy: 0.5926\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4345, Acc: 34.26% | Val Loss: 0.3684, Acc: 29.63%\n",
      "Epoch 10: Train Loss: 0.2265, Acc: 65.74% | Val Loss: 0.2088, Acc: 70.37%\n",
      "Epoch 20: Train Loss: 0.2273, Acc: 65.74% | Val Loss: 0.2089, Acc: 70.37%\n",
      "Early stopping at epoch 29\n",
      "Restoring model weights from epoch 9\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 3/5 | Validation Accuracy: 0.7037\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2288, Acc: 66.67% | Val Loss: 0.2656, Acc: 55.56%\n",
      "Epoch 10: Train Loss: 0.1834, Acc: 72.22% | Val Loss: 0.2729, Acc: 62.96%\n",
      "Epoch 20: Train Loss: 0.1690, Acc: 74.07% | Val Loss: 0.2760, Acc: 59.26%\n",
      "Early stopping at epoch 21\n",
      "Restoring model weights from epoch 1\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 4/5 | Validation Accuracy: 0.5926\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2212, Acc: 67.59% | Val Loss: 0.2539, Acc: 62.96%\n",
      "Epoch 10: Train Loss: 0.2025, Acc: 66.67% | Val Loss: 0.2572, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.1747, Acc: 71.30% | Val Loss: 0.2747, Acc: 66.67%\n",
      "Early stopping at epoch 21\n",
      "Restoring model weights from epoch 1\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 5/5 | Validation Accuracy: 0.6667\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.6667\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.001, 'l2': 0.0001, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3107, Acc: 62.96% | Val Loss: 0.2463, Acc: 70.37%\n",
      "Epoch 10: Train Loss: 0.3100, Acc: 60.19% | Val Loss: 0.2317, Acc: 62.96%\n",
      "Epoch 20: Train Loss: 0.3186, Acc: 60.19% | Val Loss: 0.2270, Acc: 62.96%\n",
      "Epoch 30: Train Loss: 0.2980, Acc: 62.04% | Val Loss: 0.2237, Acc: 59.26%\n",
      "Epoch 40: Train Loss: 0.3019, Acc: 64.81% | Val Loss: 0.2227, Acc: 59.26%\n",
      "Epoch 50: Train Loss: 0.2943, Acc: 62.04% | Val Loss: 0.2227, Acc: 59.26%\n",
      "Epoch 60: Train Loss: 0.3105, Acc: 61.11% | Val Loss: 0.2230, Acc: 59.26%\n",
      "Epoch 70: Train Loss: 0.3170, Acc: 57.41% | Val Loss: 0.2235, Acc: 59.26%\n",
      "Epoch 80: Train Loss: 0.2812, Acc: 65.74% | Val Loss: 0.2240, Acc: 59.26%\n",
      "Epoch 90: Train Loss: 0.2920, Acc: 62.04% | Val Loss: 0.2244, Acc: 59.26%\n",
      "Early stopping at epoch 93\n",
      "Restoring model weights from epoch 43\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 1/5 | Validation Accuracy: 0.5926\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.001, 'l2': 0.0001, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3818, Acc: 53.70% | Val Loss: 0.3733, Acc: 48.15%\n",
      "Epoch 10: Train Loss: 0.4081, Acc: 45.37% | Val Loss: 0.3697, Acc: 51.85%\n",
      "Epoch 20: Train Loss: 0.3539, Acc: 53.70% | Val Loss: 0.3629, Acc: 51.85%\n",
      "Epoch 30: Train Loss: 0.3850, Acc: 50.93% | Val Loss: 0.3590, Acc: 51.85%\n",
      "Epoch 40: Train Loss: 0.3892, Acc: 49.07% | Val Loss: 0.3566, Acc: 48.15%\n",
      "Epoch 50: Train Loss: 0.3659, Acc: 51.85% | Val Loss: 0.3547, Acc: 51.85%\n",
      "Epoch 60: Train Loss: 0.3440, Acc: 53.70% | Val Loss: 0.3526, Acc: 51.85%\n",
      "Epoch 70: Train Loss: 0.3497, Acc: 52.78% | Val Loss: 0.3506, Acc: 51.85%\n",
      "Epoch 80: Train Loss: 0.3734, Acc: 53.70% | Val Loss: 0.3487, Acc: 51.85%\n",
      "Epoch 90: Train Loss: 0.3291, Acc: 60.19% | Val Loss: 0.3467, Acc: 51.85%\n",
      "Epoch 100: Train Loss: 0.3607, Acc: 55.56% | Val Loss: 0.3445, Acc: 51.85%\n",
      "Epoch 110: Train Loss: 0.3469, Acc: 58.33% | Val Loss: 0.3423, Acc: 51.85%\n",
      "Epoch 120: Train Loss: 0.3263, Acc: 60.19% | Val Loss: 0.3403, Acc: 51.85%\n",
      "Epoch 130: Train Loss: 0.3377, Acc: 58.33% | Val Loss: 0.3382, Acc: 51.85%\n",
      "Epoch 140: Train Loss: 0.3205, Acc: 62.04% | Val Loss: 0.3359, Acc: 51.85%\n",
      "Epoch 150: Train Loss: 0.3368, Acc: 58.33% | Val Loss: 0.3336, Acc: 51.85%\n",
      "Epoch 160: Train Loss: 0.3076, Acc: 61.11% | Val Loss: 0.3314, Acc: 51.85%\n",
      "Epoch 170: Train Loss: 0.3201, Acc: 58.33% | Val Loss: 0.3291, Acc: 51.85%\n",
      "Epoch 180: Train Loss: 0.2931, Acc: 63.89% | Val Loss: 0.3268, Acc: 51.85%\n",
      "Epoch 190: Train Loss: 0.3130, Acc: 60.19% | Val Loss: 0.3247, Acc: 51.85%\n",
      "Final Validation Accuracy: 0.5185\n",
      " Fold 2/5 | Validation Accuracy: 0.5185\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.001, 'l2': 0.0001, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2527, Acc: 62.96% | Val Loss: 0.3477, Acc: 33.33%\n",
      "Epoch 10: Train Loss: 0.2488, Acc: 62.96% | Val Loss: 0.2596, Acc: 48.15%\n",
      "Epoch 20: Train Loss: 0.2524, Acc: 58.33% | Val Loss: 0.2417, Acc: 59.26%\n",
      "Epoch 30: Train Loss: 0.2386, Acc: 63.89% | Val Loss: 0.2356, Acc: 59.26%\n",
      "Epoch 40: Train Loss: 0.2620, Acc: 56.48% | Val Loss: 0.2327, Acc: 59.26%\n",
      "Epoch 50: Train Loss: 0.2382, Acc: 61.11% | Val Loss: 0.2311, Acc: 55.56%\n",
      "Epoch 60: Train Loss: 0.2372, Acc: 58.33% | Val Loss: 0.2299, Acc: 55.56%\n",
      "Epoch 70: Train Loss: 0.2480, Acc: 58.33% | Val Loss: 0.2290, Acc: 59.26%\n",
      "Epoch 80: Train Loss: 0.2433, Acc: 62.04% | Val Loss: 0.2281, Acc: 59.26%\n",
      "Epoch 90: Train Loss: 0.2269, Acc: 64.81% | Val Loss: 0.2274, Acc: 59.26%\n",
      "Epoch 100: Train Loss: 0.2201, Acc: 65.74% | Val Loss: 0.2267, Acc: 59.26%\n",
      "Epoch 110: Train Loss: 0.2221, Acc: 62.96% | Val Loss: 0.2262, Acc: 62.96%\n",
      "Epoch 120: Train Loss: 0.2446, Acc: 60.19% | Val Loss: 0.2257, Acc: 62.96%\n",
      "Epoch 130: Train Loss: 0.2187, Acc: 67.59% | Val Loss: 0.2252, Acc: 62.96%\n",
      "Epoch 140: Train Loss: 0.2375, Acc: 62.96% | Val Loss: 0.2248, Acc: 62.96%\n",
      "Epoch 150: Train Loss: 0.2247, Acc: 64.81% | Val Loss: 0.2244, Acc: 62.96%\n",
      "Epoch 160: Train Loss: 0.2215, Acc: 69.44% | Val Loss: 0.2241, Acc: 62.96%\n",
      "Epoch 170: Train Loss: 0.2277, Acc: 62.96% | Val Loss: 0.2237, Acc: 62.96%\n",
      "Epoch 180: Train Loss: 0.2204, Acc: 65.74% | Val Loss: 0.2234, Acc: 62.96%\n",
      "Epoch 190: Train Loss: 0.2278, Acc: 64.81% | Val Loss: 0.2231, Acc: 62.96%\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 3/5 | Validation Accuracy: 0.6296\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.001, 'l2': 0.0001, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4598, Acc: 33.33% | Val Loss: 0.3803, Acc: 44.44%\n",
      "Epoch 10: Train Loss: 0.4844, Acc: 32.41% | Val Loss: 0.3949, Acc: 44.44%\n",
      "Epoch 20: Train Loss: 0.4627, Acc: 37.96% | Val Loss: 0.4020, Acc: 40.74%\n",
      "Epoch 30: Train Loss: 0.4490, Acc: 38.89% | Val Loss: 0.4030, Acc: 44.44%\n",
      "Epoch 40: Train Loss: 0.4385, Acc: 37.04% | Val Loss: 0.4020, Acc: 44.44%\n",
      "Epoch 50: Train Loss: 0.4467, Acc: 38.89% | Val Loss: 0.4001, Acc: 44.44%\n",
      "Early stopping at epoch 50\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4444\n",
      " Fold 4/5 | Validation Accuracy: 0.4444\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.1] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.001, 'l2': 0.0001, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 1e-05, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4235, Acc: 36.11% | Val Loss: 0.4443, Acc: 40.74%\n",
      "Epoch 10: Train Loss: 0.3919, Acc: 41.67% | Val Loss: 0.3842, Acc: 48.15%\n",
      "Epoch 20: Train Loss: 0.4004, Acc: 39.81% | Val Loss: 0.3696, Acc: 44.44%\n",
      "Epoch 30: Train Loss: 0.4084, Acc: 39.81% | Val Loss: 0.3631, Acc: 48.15%\n",
      "Epoch 40: Train Loss: 0.3873, Acc: 41.67% | Val Loss: 0.3589, Acc: 48.15%\n",
      "Epoch 50: Train Loss: 0.4054, Acc: 38.89% | Val Loss: 0.3553, Acc: 48.15%\n",
      "Epoch 60: Train Loss: 0.3971, Acc: 41.67% | Val Loss: 0.3521, Acc: 48.15%\n",
      "Epoch 70: Train Loss: 0.3816, Acc: 41.67% | Val Loss: 0.3490, Acc: 51.85%\n",
      "Epoch 80: Train Loss: 0.3636, Acc: 44.44% | Val Loss: 0.3460, Acc: 55.56%\n",
      "Epoch 90: Train Loss: 0.3755, Acc: 43.52% | Val Loss: 0.3432, Acc: 55.56%\n",
      "Epoch 100: Train Loss: 0.3835, Acc: 46.30% | Val Loss: 0.3405, Acc: 55.56%\n",
      "Epoch 110: Train Loss: 0.3832, Acc: 44.44% | Val Loss: 0.3377, Acc: 55.56%\n",
      "Epoch 120: Train Loss: 0.3456, Acc: 49.07% | Val Loss: 0.3350, Acc: 55.56%\n",
      "Epoch 130: Train Loss: 0.3375, Acc: 50.00% | Val Loss: 0.3325, Acc: 55.56%\n",
      "Epoch 140: Train Loss: 0.3705, Acc: 46.30% | Val Loss: 0.3302, Acc: 55.56%\n",
      "Epoch 150: Train Loss: 0.3396, Acc: 51.85% | Val Loss: 0.3278, Acc: 55.56%\n",
      "Epoch 160: Train Loss: 0.3400, Acc: 51.85% | Val Loss: 0.3254, Acc: 59.26%\n",
      "Epoch 170: Train Loss: 0.3284, Acc: 56.48% | Val Loss: 0.3231, Acc: 59.26%\n",
      "Epoch 180: Train Loss: 0.3253, Acc: 53.70% | Val Loss: 0.3208, Acc: 62.96%\n",
      "Epoch 190: Train Loss: 0.3322, Acc: 53.70% | Val Loss: 0.3185, Acc: 62.96%\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 5/5 | Validation Accuracy: 0.6296\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5630\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 1e-05, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2542, Acc: 54.63% | Val Loss: 0.2749, Acc: 62.96%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 1/5 | Validation Accuracy: 0.6296\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 1e-05, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2949, Acc: 57.41% | Val Loss: 0.4327, Acc: 37.04%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.3704\n",
      " Fold 2/5 | Validation Accuracy: 0.3704\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 1e-05, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3374, Acc: 40.74% | Val Loss: 0.2886, Acc: 48.15%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4815\n",
      " Fold 3/5 | Validation Accuracy: 0.4815\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 1e-05, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3172, Acc: 45.37% | Val Loss: 0.2825, Acc: 48.15%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4815\n",
      " Fold 4/5 | Validation Accuracy: 0.4815\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 1e-05, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3393, Acc: 47.22% | Val Loss: 0.3010, Acc: 51.85%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5185\n",
      " Fold 5/5 | Validation Accuracy: 0.5185\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.4963\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.01, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2969, Acc: 61.11% | Val Loss: 0.2399, Acc: 70.37%\n",
      "Epoch 10: Train Loss: 0.1898, Acc: 69.44% | Val Loss: 0.1987, Acc: 77.78%\n",
      "Epoch 20: Train Loss: 0.1558, Acc: 81.48% | Val Loss: 0.2054, Acc: 81.48%\n",
      "Epoch 30: Train Loss: 0.1140, Acc: 88.89% | Val Loss: 0.2141, Acc: 66.67%\n",
      "Early stopping at epoch 32\n",
      "Restoring model weights from epoch 12\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 1/5 | Validation Accuracy: 0.6667\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.01, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3282, Acc: 44.44% | Val Loss: 0.3979, Acc: 44.44%\n",
      "Epoch 10: Train Loss: 0.1866, Acc: 73.15% | Val Loss: 0.2305, Acc: 70.37%\n",
      "Epoch 20: Train Loss: 0.1661, Acc: 78.70% | Val Loss: 0.2392, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.1460, Acc: 80.56% | Val Loss: 0.2524, Acc: 70.37%\n",
      "Early stopping at epoch 32\n",
      "Restoring model weights from epoch 12\n",
      "Final Validation Accuracy: 0.7407\n",
      " Fold 2/5 | Validation Accuracy: 0.7407\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.01, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4411, Acc: 42.59% | Val Loss: 0.7026, Acc: 29.63%\n",
      "Epoch 10: Train Loss: 0.2119, Acc: 72.22% | Val Loss: 0.2539, Acc: 59.26%\n",
      "Epoch 20: Train Loss: 0.2190, Acc: 66.67% | Val Loss: 0.2388, Acc: 70.37%\n",
      "Epoch 30: Train Loss: 0.1620, Acc: 79.63% | Val Loss: 0.2350, Acc: 70.37%\n",
      "Early stopping at epoch 33\n",
      "Restoring model weights from epoch 13\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 3/5 | Validation Accuracy: 0.7037\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.01, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2940, Acc: 49.07% | Val Loss: 0.3446, Acc: 55.56%\n",
      "Epoch 10: Train Loss: 0.1960, Acc: 72.22% | Val Loss: 0.3050, Acc: 59.26%\n",
      "Epoch 20: Train Loss: 0.1653, Acc: 75.93% | Val Loss: 0.2920, Acc: 62.96%\n",
      "Epoch 30: Train Loss: 0.1417, Acc: 76.85% | Val Loss: 0.3095, Acc: 62.96%\n",
      "Early stopping at epoch 37\n",
      "Restoring model weights from epoch 17\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 4/5 | Validation Accuracy: 0.7037\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.01, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3883, Acc: 47.22% | Val Loss: 0.3672, Acc: 33.33%\n",
      "Epoch 10: Train Loss: 0.1809, Acc: 70.37% | Val Loss: 0.2596, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.1614, Acc: 75.00% | Val Loss: 0.2576, Acc: 77.78%\n",
      "Epoch 30: Train Loss: 0.1323, Acc: 80.56% | Val Loss: 0.2616, Acc: 74.07%\n",
      "Early stopping at epoch 37\n",
      "Restoring model weights from epoch 17\n",
      "Final Validation Accuracy: 0.7778\n",
      " Fold 5/5 | Validation Accuracy: 0.7778\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.7185\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 1e-05, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3919, Acc: 44.44% | Val Loss: 0.4783, Acc: 25.93%\n",
      "Epoch 10: Train Loss: 0.3548, Acc: 57.41% | Val Loss: 0.4778, Acc: 25.93%\n",
      "Epoch 20: Train Loss: 0.3565, Acc: 51.85% | Val Loss: 0.4772, Acc: 25.93%\n",
      "Epoch 30: Train Loss: 0.3805, Acc: 50.00% | Val Loss: 0.4766, Acc: 25.93%\n",
      "Epoch 40: Train Loss: 0.3669, Acc: 50.00% | Val Loss: 0.4760, Acc: 25.93%\n",
      "Epoch 50: Train Loss: 0.3791, Acc: 49.07% | Val Loss: 0.4755, Acc: 25.93%\n",
      "Epoch 60: Train Loss: 0.3769, Acc: 44.44% | Val Loss: 0.4749, Acc: 25.93%\n",
      "Epoch 70: Train Loss: 0.3503, Acc: 53.70% | Val Loss: 0.4743, Acc: 25.93%\n",
      "Epoch 80: Train Loss: 0.3771, Acc: 54.63% | Val Loss: 0.4737, Acc: 25.93%\n",
      "Epoch 90: Train Loss: 0.3718, Acc: 48.15% | Val Loss: 0.4731, Acc: 25.93%\n",
      "Epoch 100: Train Loss: 0.3547, Acc: 50.00% | Val Loss: 0.4725, Acc: 25.93%\n",
      "Epoch 110: Train Loss: 0.3708, Acc: 47.22% | Val Loss: 0.4719, Acc: 25.93%\n",
      "Epoch 120: Train Loss: 0.3666, Acc: 49.07% | Val Loss: 0.4713, Acc: 25.93%\n",
      "Epoch 130: Train Loss: 0.3561, Acc: 44.44% | Val Loss: 0.4708, Acc: 25.93%\n",
      "Epoch 140: Train Loss: 0.3565, Acc: 44.44% | Val Loss: 0.4702, Acc: 25.93%\n",
      "Epoch 150: Train Loss: 0.3684, Acc: 50.93% | Val Loss: 0.4696, Acc: 25.93%\n",
      "Epoch 160: Train Loss: 0.3629, Acc: 47.22% | Val Loss: 0.4690, Acc: 25.93%\n",
      "Epoch 170: Train Loss: 0.3534, Acc: 46.30% | Val Loss: 0.4685, Acc: 25.93%\n",
      "Epoch 180: Train Loss: 0.3418, Acc: 52.78% | Val Loss: 0.4679, Acc: 25.93%\n",
      "Epoch 190: Train Loss: 0.3709, Acc: 48.15% | Val Loss: 0.4673, Acc: 25.93%\n",
      "Final Validation Accuracy: 0.2593\n",
      " Fold 1/5 | Validation Accuracy: 0.2593\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 1e-05, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2673, Acc: 66.67% | Val Loss: 0.2610, Acc: 66.67%\n",
      "Epoch 10: Train Loss: 0.2596, Acc: 66.67% | Val Loss: 0.2609, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.2417, Acc: 66.67% | Val Loss: 0.2608, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.2636, Acc: 66.67% | Val Loss: 0.2608, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.2717, Acc: 66.67% | Val Loss: 0.2607, Acc: 66.67%\n",
      "Epoch 50: Train Loss: 0.2502, Acc: 66.67% | Val Loss: 0.2606, Acc: 66.67%\n",
      "Epoch 60: Train Loss: 0.2489, Acc: 66.67% | Val Loss: 0.2605, Acc: 66.67%\n",
      "Epoch 70: Train Loss: 0.2540, Acc: 66.67% | Val Loss: 0.2604, Acc: 66.67%\n",
      "Epoch 80: Train Loss: 0.2606, Acc: 66.67% | Val Loss: 0.2604, Acc: 66.67%\n",
      "Epoch 90: Train Loss: 0.2673, Acc: 66.67% | Val Loss: 0.2603, Acc: 66.67%\n",
      "Epoch 100: Train Loss: 0.2604, Acc: 66.67% | Val Loss: 0.2602, Acc: 66.67%\n",
      "Epoch 110: Train Loss: 0.2435, Acc: 66.67% | Val Loss: 0.2601, Acc: 66.67%\n",
      "Epoch 120: Train Loss: 0.2387, Acc: 66.67% | Val Loss: 0.2601, Acc: 66.67%\n",
      "Epoch 130: Train Loss: 0.2519, Acc: 66.67% | Val Loss: 0.2600, Acc: 66.67%\n",
      "Epoch 140: Train Loss: 0.2540, Acc: 66.67% | Val Loss: 0.2599, Acc: 66.67%\n",
      "Epoch 150: Train Loss: 0.2592, Acc: 66.67% | Val Loss: 0.2599, Acc: 66.67%\n",
      "Epoch 160: Train Loss: 0.2526, Acc: 66.67% | Val Loss: 0.2598, Acc: 66.67%\n",
      "Epoch 170: Train Loss: 0.2615, Acc: 66.67% | Val Loss: 0.2597, Acc: 66.67%\n",
      "Epoch 180: Train Loss: 0.2469, Acc: 66.67% | Val Loss: 0.2596, Acc: 66.67%\n",
      "Epoch 190: Train Loss: 0.2583, Acc: 66.67% | Val Loss: 0.2596, Acc: 66.67%\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 2/5 | Validation Accuracy: 0.6667\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 1e-05, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3760, Acc: 48.15% | Val Loss: 0.4765, Acc: 25.93%\n",
      "Epoch 10: Train Loss: 0.3956, Acc: 49.07% | Val Loss: 0.4757, Acc: 25.93%\n",
      "Epoch 20: Train Loss: 0.3932, Acc: 52.78% | Val Loss: 0.4749, Acc: 25.93%\n",
      "Epoch 30: Train Loss: 0.3919, Acc: 53.70% | Val Loss: 0.4741, Acc: 25.93%\n",
      "Epoch 40: Train Loss: 0.3827, Acc: 51.85% | Val Loss: 0.4732, Acc: 25.93%\n",
      "Epoch 50: Train Loss: 0.4011, Acc: 51.85% | Val Loss: 0.4724, Acc: 25.93%\n",
      "Epoch 60: Train Loss: 0.3791, Acc: 50.00% | Val Loss: 0.4716, Acc: 25.93%\n",
      "Epoch 70: Train Loss: 0.3731, Acc: 55.56% | Val Loss: 0.4708, Acc: 25.93%\n",
      "Epoch 80: Train Loss: 0.3913, Acc: 54.63% | Val Loss: 0.4699, Acc: 25.93%\n",
      "Epoch 90: Train Loss: 0.3658, Acc: 57.41% | Val Loss: 0.4691, Acc: 25.93%\n",
      "Epoch 100: Train Loss: 0.3611, Acc: 54.63% | Val Loss: 0.4683, Acc: 25.93%\n",
      "Epoch 110: Train Loss: 0.4253, Acc: 47.22% | Val Loss: 0.4675, Acc: 25.93%\n",
      "Epoch 120: Train Loss: 0.3932, Acc: 52.78% | Val Loss: 0.4666, Acc: 25.93%\n",
      "Epoch 130: Train Loss: 0.3655, Acc: 56.48% | Val Loss: 0.4658, Acc: 25.93%\n",
      "Epoch 140: Train Loss: 0.4080, Acc: 45.37% | Val Loss: 0.4649, Acc: 29.63%\n",
      "Epoch 150: Train Loss: 0.3839, Acc: 56.48% | Val Loss: 0.4640, Acc: 29.63%\n",
      "Epoch 160: Train Loss: 0.3864, Acc: 55.56% | Val Loss: 0.4632, Acc: 29.63%\n",
      "Epoch 170: Train Loss: 0.3823, Acc: 52.78% | Val Loss: 0.4623, Acc: 29.63%\n",
      "Epoch 180: Train Loss: 0.4006, Acc: 47.22% | Val Loss: 0.4614, Acc: 29.63%\n",
      "Epoch 190: Train Loss: 0.3967, Acc: 50.00% | Val Loss: 0.4606, Acc: 29.63%\n",
      "Final Validation Accuracy: 0.2963\n",
      " Fold 3/5 | Validation Accuracy: 0.2963\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 1e-05, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3472, Acc: 55.56% | Val Loss: 0.3413, Acc: 62.96%\n",
      "Epoch 10: Train Loss: 0.3674, Acc: 50.93% | Val Loss: 0.3408, Acc: 62.96%\n",
      "Epoch 20: Train Loss: 0.3331, Acc: 58.33% | Val Loss: 0.3402, Acc: 62.96%\n",
      "Epoch 30: Train Loss: 0.3500, Acc: 50.00% | Val Loss: 0.3397, Acc: 62.96%\n",
      "Epoch 40: Train Loss: 0.3466, Acc: 52.78% | Val Loss: 0.3392, Acc: 62.96%\n",
      "Epoch 50: Train Loss: 0.3574, Acc: 51.85% | Val Loss: 0.3387, Acc: 62.96%\n",
      "Epoch 60: Train Loss: 0.3415, Acc: 55.56% | Val Loss: 0.3382, Acc: 62.96%\n",
      "Epoch 70: Train Loss: 0.3626, Acc: 48.15% | Val Loss: 0.3377, Acc: 62.96%\n",
      "Epoch 80: Train Loss: 0.3131, Acc: 60.19% | Val Loss: 0.3372, Acc: 62.96%\n",
      "Epoch 90: Train Loss: 0.3148, Acc: 61.11% | Val Loss: 0.3367, Acc: 62.96%\n",
      "Epoch 100: Train Loss: 0.3466, Acc: 55.56% | Val Loss: 0.3362, Acc: 62.96%\n",
      "Epoch 110: Train Loss: 0.3618, Acc: 53.70% | Val Loss: 0.3358, Acc: 62.96%\n",
      "Epoch 120: Train Loss: 0.3626, Acc: 50.93% | Val Loss: 0.3353, Acc: 62.96%\n",
      "Epoch 130: Train Loss: 0.3088, Acc: 58.33% | Val Loss: 0.3349, Acc: 62.96%\n",
      "Epoch 140: Train Loss: 0.3066, Acc: 59.26% | Val Loss: 0.3344, Acc: 62.96%\n",
      "Epoch 150: Train Loss: 0.3281, Acc: 55.56% | Val Loss: 0.3340, Acc: 62.96%\n",
      "Epoch 160: Train Loss: 0.3221, Acc: 62.96% | Val Loss: 0.3336, Acc: 62.96%\n",
      "Epoch 170: Train Loss: 0.3200, Acc: 61.11% | Val Loss: 0.3331, Acc: 62.96%\n",
      "Epoch 180: Train Loss: 0.3614, Acc: 54.63% | Val Loss: 0.3327, Acc: 62.96%\n",
      "Epoch 190: Train Loss: 0.3550, Acc: 54.63% | Val Loss: 0.3322, Acc: 62.96%\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 4/5 | Validation Accuracy: 0.6296\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 1e-05, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2403, Acc: 68.52% | Val Loss: 0.2910, Acc: 62.96%\n",
      "Epoch 10: Train Loss: 0.2638, Acc: 63.89% | Val Loss: 0.2908, Acc: 62.96%\n",
      "Epoch 20: Train Loss: 0.2740, Acc: 64.81% | Val Loss: 0.2907, Acc: 62.96%\n",
      "Epoch 30: Train Loss: 0.2474, Acc: 65.74% | Val Loss: 0.2905, Acc: 62.96%\n",
      "Epoch 40: Train Loss: 0.2597, Acc: 64.81% | Val Loss: 0.2904, Acc: 62.96%\n",
      "Epoch 50: Train Loss: 0.2633, Acc: 64.81% | Val Loss: 0.2902, Acc: 62.96%\n",
      "Epoch 60: Train Loss: 0.2678, Acc: 64.81% | Val Loss: 0.2901, Acc: 62.96%\n",
      "Epoch 70: Train Loss: 0.2752, Acc: 62.96% | Val Loss: 0.2899, Acc: 62.96%\n",
      "Epoch 80: Train Loss: 0.2712, Acc: 66.67% | Val Loss: 0.2898, Acc: 62.96%\n",
      "Epoch 90: Train Loss: 0.2712, Acc: 63.89% | Val Loss: 0.2896, Acc: 62.96%\n",
      "Epoch 100: Train Loss: 0.2445, Acc: 67.59% | Val Loss: 0.2895, Acc: 62.96%\n",
      "Epoch 110: Train Loss: 0.2472, Acc: 66.67% | Val Loss: 0.2893, Acc: 62.96%\n",
      "Epoch 120: Train Loss: 0.2478, Acc: 63.89% | Val Loss: 0.2892, Acc: 62.96%\n",
      "Epoch 130: Train Loss: 0.2489, Acc: 65.74% | Val Loss: 0.2890, Acc: 62.96%\n",
      "Epoch 140: Train Loss: 0.2594, Acc: 66.67% | Val Loss: 0.2889, Acc: 62.96%\n",
      "Epoch 150: Train Loss: 0.2366, Acc: 67.59% | Val Loss: 0.2887, Acc: 62.96%\n",
      "Epoch 160: Train Loss: 0.2550, Acc: 63.89% | Val Loss: 0.2886, Acc: 62.96%\n",
      "Epoch 170: Train Loss: 0.2465, Acc: 64.81% | Val Loss: 0.2884, Acc: 62.96%\n",
      "Epoch 180: Train Loss: 0.2473, Acc: 64.81% | Val Loss: 0.2883, Acc: 62.96%\n",
      "Epoch 190: Train Loss: 0.2457, Acc: 67.59% | Val Loss: 0.2881, Acc: 62.96%\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 5/5 | Validation Accuracy: 0.6296\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.4963\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.0001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3213, Acc: 61.11% | Val Loss: 0.2506, Acc: 70.37%\n",
      "Epoch 10: Train Loss: 0.3394, Acc: 61.11% | Val Loss: 0.2448, Acc: 70.37%\n",
      "Epoch 20: Train Loss: 0.3284, Acc: 62.96% | Val Loss: 0.2440, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.2849, Acc: 63.89% | Val Loss: 0.2393, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.2538, Acc: 63.89% | Val Loss: 0.2391, Acc: 66.67%\n",
      "Epoch 50: Train Loss: 0.2404, Acc: 69.44% | Val Loss: 0.2351, Acc: 66.67%\n",
      "Epoch 60: Train Loss: 0.2599, Acc: 67.59% | Val Loss: 0.2348, Acc: 66.67%\n",
      "Epoch 70: Train Loss: 0.2376, Acc: 66.67% | Val Loss: 0.2314, Acc: 66.67%\n",
      "Epoch 80: Train Loss: 0.1826, Acc: 79.63% | Val Loss: 0.2243, Acc: 74.07%\n",
      "Epoch 90: Train Loss: 0.1800, Acc: 77.78% | Val Loss: 0.2212, Acc: 70.37%\n",
      "Epoch 100: Train Loss: 0.1755, Acc: 79.63% | Val Loss: 0.2192, Acc: 70.37%\n",
      "Epoch 110: Train Loss: 0.1872, Acc: 76.85% | Val Loss: 0.2230, Acc: 70.37%\n",
      "Early stopping at epoch 118\n",
      "Restoring model weights from epoch 98\n",
      "Final Validation Accuracy: 0.7407\n",
      " Fold 1/5 | Validation Accuracy: 0.7407\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.0001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3303, Acc: 45.37% | Val Loss: 0.3745, Acc: 48.15%\n",
      "Epoch 10: Train Loss: 0.3081, Acc: 61.11% | Val Loss: 0.3375, Acc: 59.26%\n",
      "Epoch 20: Train Loss: 0.2864, Acc: 58.33% | Val Loss: 0.3145, Acc: 55.56%\n",
      "Epoch 30: Train Loss: 0.2362, Acc: 62.96% | Val Loss: 0.2965, Acc: 51.85%\n",
      "Epoch 40: Train Loss: 0.2324, Acc: 63.89% | Val Loss: 0.2793, Acc: 59.26%\n",
      "Epoch 50: Train Loss: 0.2149, Acc: 66.67% | Val Loss: 0.2656, Acc: 59.26%\n",
      "Epoch 60: Train Loss: 0.2070, Acc: 67.59% | Val Loss: 0.2590, Acc: 62.96%\n",
      "Epoch 70: Train Loss: 0.2191, Acc: 67.59% | Val Loss: 0.2564, Acc: 66.67%\n",
      "Epoch 80: Train Loss: 0.1913, Acc: 67.59% | Val Loss: 0.2553, Acc: 66.67%\n",
      "Epoch 90: Train Loss: 0.2049, Acc: 66.67% | Val Loss: 0.2542, Acc: 66.67%\n",
      "Epoch 100: Train Loss: 0.2021, Acc: 67.59% | Val Loss: 0.2536, Acc: 66.67%\n",
      "Epoch 110: Train Loss: 0.1920, Acc: 69.44% | Val Loss: 0.2533, Acc: 66.67%\n",
      "Epoch 120: Train Loss: 0.1778, Acc: 69.44% | Val Loss: 0.2549, Acc: 62.96%\n",
      "Early stopping at epoch 124\n",
      "Restoring model weights from epoch 104\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 2/5 | Validation Accuracy: 0.6296\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.0001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3500, Acc: 55.56% | Val Loss: 0.3677, Acc: 51.85%\n",
      "Epoch 10: Train Loss: 0.2681, Acc: 68.52% | Val Loss: 0.3413, Acc: 55.56%\n",
      "Epoch 20: Train Loss: 0.2745, Acc: 68.52% | Val Loss: 0.3149, Acc: 55.56%\n",
      "Epoch 30: Train Loss: 0.2743, Acc: 65.74% | Val Loss: 0.3107, Acc: 59.26%\n",
      "Epoch 40: Train Loss: 0.2767, Acc: 63.89% | Val Loss: 0.3140, Acc: 59.26%\n",
      "Early stopping at epoch 46\n",
      "Restoring model weights from epoch 26\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 3/5 | Validation Accuracy: 0.6296\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.0001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3226, Acc: 50.93% | Val Loss: 0.3389, Acc: 55.56%\n",
      "Epoch 10: Train Loss: 0.2647, Acc: 64.81% | Val Loss: 0.3263, Acc: 59.26%\n",
      "Epoch 20: Train Loss: 0.2421, Acc: 68.52% | Val Loss: 0.3250, Acc: 55.56%\n",
      "Epoch 30: Train Loss: 0.2116, Acc: 72.22% | Val Loss: 0.3186, Acc: 59.26%\n",
      "Epoch 40: Train Loss: 0.2354, Acc: 73.15% | Val Loss: 0.3079, Acc: 59.26%\n",
      "Epoch 50: Train Loss: 0.2017, Acc: 74.07% | Val Loss: 0.2986, Acc: 59.26%\n",
      "Epoch 60: Train Loss: 0.2080, Acc: 72.22% | Val Loss: 0.2923, Acc: 59.26%\n",
      "Epoch 70: Train Loss: 0.2013, Acc: 74.07% | Val Loss: 0.2863, Acc: 59.26%\n",
      "Epoch 80: Train Loss: 0.2002, Acc: 74.07% | Val Loss: 0.2815, Acc: 55.56%\n",
      "Epoch 90: Train Loss: 0.1838, Acc: 75.93% | Val Loss: 0.2784, Acc: 55.56%\n",
      "Epoch 100: Train Loss: 0.1917, Acc: 73.15% | Val Loss: 0.2773, Acc: 55.56%\n",
      "Epoch 110: Train Loss: 0.1781, Acc: 75.00% | Val Loss: 0.2760, Acc: 59.26%\n",
      "Epoch 120: Train Loss: 0.1745, Acc: 75.00% | Val Loss: 0.2769, Acc: 59.26%\n",
      "Early stopping at epoch 129\n",
      "Restoring model weights from epoch 109\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 4/5 | Validation Accuracy: 0.5926\n",
      "[6] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.0001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3533, Acc: 50.93% | Val Loss: 0.3553, Acc: 66.67%\n",
      "Epoch 10: Train Loss: 0.2863, Acc: 62.96% | Val Loss: 0.3068, Acc: 59.26%\n",
      "Epoch 20: Train Loss: 0.2799, Acc: 58.33% | Val Loss: 0.2887, Acc: 55.56%\n",
      "Epoch 30: Train Loss: 0.2444, Acc: 64.81% | Val Loss: 0.2791, Acc: 59.26%\n",
      "Epoch 40: Train Loss: 0.2174, Acc: 62.96% | Val Loss: 0.2740, Acc: 55.56%\n",
      "Epoch 50: Train Loss: 0.2115, Acc: 67.59% | Val Loss: 0.2686, Acc: 55.56%\n",
      "Epoch 60: Train Loss: 0.2024, Acc: 70.37% | Val Loss: 0.2659, Acc: 55.56%\n",
      "Epoch 70: Train Loss: 0.1962, Acc: 69.44% | Val Loss: 0.2658, Acc: 62.96%\n",
      "Epoch 80: Train Loss: 0.2101, Acc: 66.67% | Val Loss: 0.2666, Acc: 62.96%\n",
      "Early stopping at epoch 85\n",
      "Restoring model weights from epoch 65\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 5/5 | Validation Accuracy: 0.6296\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.6444\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.01, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3929, Acc: 45.37% | Val Loss: 0.4754, Acc: 22.22%\n",
      "Epoch 10: Train Loss: 0.3766, Acc: 52.78% | Val Loss: 0.4401, Acc: 51.85%\n",
      "Epoch 20: Train Loss: 0.3373, Acc: 55.56% | Val Loss: 0.4048, Acc: 51.85%\n",
      "Epoch 30: Train Loss: 0.3560, Acc: 50.00% | Val Loss: 0.3741, Acc: 51.85%\n",
      "Epoch 40: Train Loss: 0.3089, Acc: 52.78% | Val Loss: 0.3327, Acc: 51.85%\n",
      "Epoch 50: Train Loss: 0.2911, Acc: 54.63% | Val Loss: 0.2823, Acc: 55.56%\n",
      "Epoch 60: Train Loss: 0.2493, Acc: 59.26% | Val Loss: 0.2450, Acc: 62.96%\n",
      "Epoch 70: Train Loss: 0.2392, Acc: 59.26% | Val Loss: 0.2212, Acc: 62.96%\n",
      "Epoch 80: Train Loss: 0.2266, Acc: 62.04% | Val Loss: 0.2085, Acc: 62.96%\n",
      "Epoch 90: Train Loss: 0.2277, Acc: 62.04% | Val Loss: 0.2032, Acc: 66.67%\n",
      "Epoch 100: Train Loss: 0.2330, Acc: 62.96% | Val Loss: 0.2021, Acc: 66.67%\n",
      "Epoch 110: Train Loss: 0.2274, Acc: 62.96% | Val Loss: 0.1968, Acc: 74.07%\n",
      "Epoch 120: Train Loss: 0.2291, Acc: 63.89% | Val Loss: 0.1940, Acc: 77.78%\n",
      "Epoch 130: Train Loss: 0.2299, Acc: 64.81% | Val Loss: 0.1944, Acc: 77.78%\n",
      "Epoch 140: Train Loss: 0.2281, Acc: 63.89% | Val Loss: 0.1917, Acc: 77.78%\n",
      "Final Validation Accuracy: 0.7778\n",
      " Fold 1/5 | Validation Accuracy: 0.7778\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.01, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2731, Acc: 66.67% | Val Loss: 0.2683, Acc: 66.67%\n",
      "Epoch 10: Train Loss: 0.2785, Acc: 66.67% | Val Loss: 0.2655, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.2678, Acc: 66.67% | Val Loss: 0.2618, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.2621, Acc: 66.67% | Val Loss: 0.2554, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.2583, Acc: 66.67% | Val Loss: 0.2488, Acc: 66.67%\n",
      "Epoch 50: Train Loss: 0.2443, Acc: 66.67% | Val Loss: 0.2439, Acc: 66.67%\n",
      "Epoch 60: Train Loss: 0.2290, Acc: 66.67% | Val Loss: 0.2398, Acc: 66.67%\n",
      "Epoch 70: Train Loss: 0.2176, Acc: 68.52% | Val Loss: 0.2365, Acc: 66.67%\n",
      "Epoch 80: Train Loss: 0.2303, Acc: 68.52% | Val Loss: 0.2334, Acc: 66.67%\n",
      "Epoch 90: Train Loss: 0.2146, Acc: 68.52% | Val Loss: 0.2309, Acc: 66.67%\n",
      "Epoch 100: Train Loss: 0.2127, Acc: 65.74% | Val Loss: 0.2287, Acc: 66.67%\n",
      "Epoch 110: Train Loss: 0.2177, Acc: 66.67% | Val Loss: 0.2277, Acc: 66.67%\n",
      "Epoch 120: Train Loss: 0.2151, Acc: 66.67% | Val Loss: 0.2257, Acc: 66.67%\n",
      "Epoch 130: Train Loss: 0.2161, Acc: 66.67% | Val Loss: 0.2248, Acc: 66.67%\n",
      "Epoch 140: Train Loss: 0.2187, Acc: 66.67% | Val Loss: 0.2244, Acc: 66.67%\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 2/5 | Validation Accuracy: 0.6667\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.01, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3925, Acc: 41.67% | Val Loss: 0.4556, Acc: 37.04%\n",
      "Epoch 10: Train Loss: 0.2949, Acc: 50.93% | Val Loss: 0.3489, Acc: 37.04%\n",
      "Epoch 20: Train Loss: 0.2649, Acc: 48.15% | Val Loss: 0.2695, Acc: 48.15%\n",
      "Epoch 30: Train Loss: 0.2423, Acc: 59.26% | Val Loss: 0.2344, Acc: 59.26%\n",
      "Epoch 40: Train Loss: 0.2311, Acc: 66.67% | Val Loss: 0.2247, Acc: 51.85%\n",
      "Epoch 50: Train Loss: 0.2390, Acc: 63.89% | Val Loss: 0.2218, Acc: 59.26%\n",
      "Epoch 60: Train Loss: 0.2245, Acc: 65.74% | Val Loss: 0.2223, Acc: 59.26%\n",
      "Epoch 70: Train Loss: 0.2149, Acc: 69.44% | Val Loss: 0.2218, Acc: 59.26%\n",
      "Epoch 80: Train Loss: 0.2105, Acc: 66.67% | Val Loss: 0.2206, Acc: 59.26%\n",
      "Epoch 90: Train Loss: 0.2210, Acc: 66.67% | Val Loss: 0.2198, Acc: 59.26%\n",
      "Epoch 100: Train Loss: 0.2223, Acc: 64.81% | Val Loss: 0.2183, Acc: 62.96%\n",
      "Epoch 110: Train Loss: 0.2250, Acc: 64.81% | Val Loss: 0.2163, Acc: 62.96%\n",
      "Epoch 120: Train Loss: 0.2169, Acc: 63.89% | Val Loss: 0.2173, Acc: 66.67%\n",
      "Epoch 130: Train Loss: 0.2192, Acc: 65.74% | Val Loss: 0.2173, Acc: 70.37%\n",
      "Early stopping at epoch 131\n",
      "Restoring model weights from epoch 111\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 3/5 | Validation Accuracy: 0.7037\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.01, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3670, Acc: 44.44% | Val Loss: 0.3293, Acc: 59.26%\n",
      "Epoch 10: Train Loss: 0.3002, Acc: 54.63% | Val Loss: 0.2893, Acc: 62.96%\n",
      "Epoch 20: Train Loss: 0.2542, Acc: 57.41% | Val Loss: 0.2721, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.2542, Acc: 55.56% | Val Loss: 0.2712, Acc: 55.56%\n",
      "Epoch 40: Train Loss: 0.2469, Acc: 63.89% | Val Loss: 0.2715, Acc: 55.56%\n",
      "Early stopping at epoch 45\n",
      "Restoring model weights from epoch 25\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 4/5 | Validation Accuracy: 0.5556\n",
      "[4] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.01, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2738, Acc: 56.48% | Val Loss: 0.3081, Acc: 51.85%\n",
      "Epoch 10: Train Loss: 0.2509, Acc: 65.74% | Val Loss: 0.2880, Acc: 55.56%\n",
      "Epoch 20: Train Loss: 0.2315, Acc: 62.96% | Val Loss: 0.2731, Acc: 48.15%\n",
      "Epoch 30: Train Loss: 0.2326, Acc: 66.67% | Val Loss: 0.2625, Acc: 51.85%\n",
      "Epoch 40: Train Loss: 0.2321, Acc: 62.04% | Val Loss: 0.2532, Acc: 51.85%\n",
      "Epoch 50: Train Loss: 0.2156, Acc: 64.81% | Val Loss: 0.2459, Acc: 55.56%\n",
      "Epoch 60: Train Loss: 0.2162, Acc: 66.67% | Val Loss: 0.2410, Acc: 59.26%\n",
      "Epoch 70: Train Loss: 0.2164, Acc: 67.59% | Val Loss: 0.2394, Acc: 62.96%\n",
      "Epoch 80: Train Loss: 0.2092, Acc: 67.59% | Val Loss: 0.2388, Acc: 62.96%\n",
      "Epoch 90: Train Loss: 0.2111, Acc: 67.59% | Val Loss: 0.2390, Acc: 62.96%\n",
      "Epoch 100: Train Loss: 0.2120, Acc: 67.59% | Val Loss: 0.2385, Acc: 62.96%\n",
      "Epoch 110: Train Loss: 0.2099, Acc: 67.59% | Val Loss: 0.2381, Acc: 62.96%\n",
      "Epoch 120: Train Loss: 0.2117, Acc: 67.59% | Val Loss: 0.2375, Acc: 62.96%\n",
      "Epoch 130: Train Loss: 0.2098, Acc: 67.59% | Val Loss: 0.2380, Acc: 62.96%\n",
      "Early stopping at epoch 139\n",
      "Restoring model weights from epoch 119\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 5/5 | Validation Accuracy: 0.6296\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.6667\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.001, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2752, Acc: 52.78% | Val Loss: 0.2446, Acc: 70.37%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 1/5 | Validation Accuracy: 0.7037\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.001, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4010, Acc: 44.44% | Val Loss: 0.3786, Acc: 59.26%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 2/5 | Validation Accuracy: 0.5926\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.001, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3165, Acc: 60.19% | Val Loss: 0.3120, Acc: 51.85%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5185\n",
      " Fold 3/5 | Validation Accuracy: 0.5185\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.001, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2689, Acc: 64.81% | Val Loss: 0.3965, Acc: 55.56%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 4/5 | Validation Accuracy: 0.5556\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.001, 'l2': 0.001, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3757, Acc: 50.00% | Val Loss: 0.4740, Acc: 40.74%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4074\n",
      " Fold 5/5 | Validation Accuracy: 0.4074\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5556\n",
      "[6] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2968, Acc: 63.89% | Val Loss: 0.2455, Acc: 70.37%\n",
      "Epoch 10: Train Loss: 0.3185, Acc: 57.41% | Val Loss: 0.2297, Acc: 62.96%\n",
      "Epoch 20: Train Loss: 0.3115, Acc: 57.41% | Val Loss: 0.2243, Acc: 62.96%\n",
      "Epoch 30: Train Loss: 0.3172, Acc: 55.56% | Val Loss: 0.2205, Acc: 62.96%\n",
      "Epoch 40: Train Loss: 0.2981, Acc: 57.41% | Val Loss: 0.2192, Acc: 62.96%\n",
      "Epoch 50: Train Loss: 0.2987, Acc: 62.96% | Val Loss: 0.2188, Acc: 62.96%\n",
      "Epoch 60: Train Loss: 0.3497, Acc: 57.41% | Val Loss: 0.2186, Acc: 62.96%\n",
      "Epoch 70: Train Loss: 0.3259, Acc: 56.48% | Val Loss: 0.2186, Acc: 62.96%\n",
      "Epoch 80: Train Loss: 0.2988, Acc: 59.26% | Val Loss: 0.2186, Acc: 62.96%\n",
      "Early stopping at epoch 87\n",
      "Restoring model weights from epoch 67\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 1/5 | Validation Accuracy: 0.6296\n",
      "[6] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3487, Acc: 43.52% | Val Loss: 0.3192, Acc: 37.04%\n",
      "Epoch 10: Train Loss: 0.3611, Acc: 48.15% | Val Loss: 0.3451, Acc: 29.63%\n",
      "Epoch 20: Train Loss: 0.3681, Acc: 43.52% | Val Loss: 0.3604, Acc: 29.63%\n",
      "Early stopping at epoch 20\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.2963\n",
      " Fold 2/5 | Validation Accuracy: 0.2963\n",
      "[6] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2649, Acc: 62.04% | Val Loss: 0.3477, Acc: 55.56%\n",
      "Epoch 10: Train Loss: 0.3316, Acc: 49.07% | Val Loss: 0.2890, Acc: 55.56%\n",
      "Epoch 20: Train Loss: 0.3011, Acc: 51.85% | Val Loss: 0.2736, Acc: 55.56%\n",
      "Epoch 30: Train Loss: 0.2485, Acc: 63.89% | Val Loss: 0.2686, Acc: 59.26%\n",
      "Epoch 40: Train Loss: 0.3009, Acc: 57.41% | Val Loss: 0.2668, Acc: 59.26%\n",
      "Epoch 50: Train Loss: 0.2883, Acc: 55.56% | Val Loss: 0.2661, Acc: 59.26%\n",
      "Epoch 60: Train Loss: 0.2674, Acc: 60.19% | Val Loss: 0.2657, Acc: 59.26%\n",
      "Epoch 70: Train Loss: 0.2631, Acc: 60.19% | Val Loss: 0.2655, Acc: 59.26%\n",
      "Epoch 80: Train Loss: 0.2621, Acc: 61.11% | Val Loss: 0.2653, Acc: 59.26%\n",
      "Epoch 90: Train Loss: 0.2774, Acc: 62.96% | Val Loss: 0.2652, Acc: 59.26%\n",
      "Epoch 100: Train Loss: 0.2995, Acc: 56.48% | Val Loss: 0.2650, Acc: 59.26%\n",
      "Epoch 110: Train Loss: 0.3343, Acc: 49.07% | Val Loss: 0.2649, Acc: 59.26%\n",
      "Epoch 120: Train Loss: 0.2976, Acc: 59.26% | Val Loss: 0.2648, Acc: 59.26%\n",
      "Epoch 130: Train Loss: 0.2568, Acc: 61.11% | Val Loss: 0.2647, Acc: 59.26%\n",
      "Epoch 140: Train Loss: 0.2816, Acc: 56.48% | Val Loss: 0.2646, Acc: 59.26%\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 3/5 | Validation Accuracy: 0.5926\n",
      "[6] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2911, Acc: 50.00% | Val Loss: 0.3328, Acc: 59.26%\n",
      "Epoch 10: Train Loss: 0.3117, Acc: 49.07% | Val Loss: 0.3080, Acc: 55.56%\n",
      "Epoch 20: Train Loss: 0.2824, Acc: 57.41% | Val Loss: 0.3069, Acc: 59.26%\n",
      "Epoch 30: Train Loss: 0.2736, Acc: 60.19% | Val Loss: 0.3072, Acc: 59.26%\n",
      "Early stopping at epoch 38\n",
      "Restoring model weights from epoch 18\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 4/5 | Validation Accuracy: 0.5926\n",
      "[6] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.001, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4250, Acc: 37.04% | Val Loss: 0.4040, Acc: 40.74%\n",
      "Epoch 10: Train Loss: 0.4314, Acc: 34.26% | Val Loss: 0.4015, Acc: 40.74%\n",
      "Epoch 20: Train Loss: 0.4204, Acc: 37.04% | Val Loss: 0.4095, Acc: 37.04%\n",
      "Early stopping at epoch 24\n",
      "Restoring model weights from epoch 4\n",
      "Final Validation Accuracy: 0.3704\n",
      " Fold 5/5 | Validation Accuracy: 0.3704\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.4963\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3063, Acc: 50.93% | Val Loss: 0.2508, Acc: 70.37%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 1/5 | Validation Accuracy: 0.7037\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4381, Acc: 45.37% | Val Loss: 0.3912, Acc: 59.26%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 2/5 | Validation Accuracy: 0.5926\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3597, Acc: 60.19% | Val Loss: 0.3244, Acc: 44.44%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4444\n",
      " Fold 3/5 | Validation Accuracy: 0.4444\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2576, Acc: 69.44% | Val Loss: 0.4076, Acc: 55.56%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 4/5 | Validation Accuracy: 0.5556\n",
      "[5] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 1e-05, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4005, Acc: 52.78% | Val Loss: 0.4800, Acc: 40.74%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4074\n",
      " Fold 5/5 | Validation Accuracy: 0.4074\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5407\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3355, Acc: 57.41% | Val Loss: 0.3056, Acc: 51.85%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5185\n",
      " Fold 1/5 | Validation Accuracy: 0.5185\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3305, Acc: 51.85% | Val Loss: 0.3060, Acc: 70.37%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 2/5 | Validation Accuracy: 0.7037\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2512, Acc: 56.48% | Val Loss: 0.2301, Acc: 70.37%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 3/5 | Validation Accuracy: 0.7037\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3335, Acc: 37.96% | Val Loss: 0.2736, Acc: 66.67%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 4/5 | Validation Accuracy: 0.6667\n",
      "[6] [<class 'src.activation_functions.Activation_Tanh'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [False], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3638, Acc: 54.63% | Val Loss: 0.3295, Acc: 62.96%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 5/5 | Validation Accuracy: 0.6296\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.6444\n",
      "[3] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2524, Acc: 47.22% | Val Loss: 0.2601, Acc: 59.26%\n",
      "Epoch 10: Train Loss: 0.2412, Acc: 60.19% | Val Loss: 0.2310, Acc: 77.78%\n",
      "Epoch 20: Train Loss: 0.2377, Acc: 63.89% | Val Loss: 0.2108, Acc: 77.78%\n",
      "Epoch 30: Train Loss: 0.2285, Acc: 63.89% | Val Loss: 0.1991, Acc: 77.78%\n",
      "Epoch 40: Train Loss: 0.2336, Acc: 63.89% | Val Loss: 0.1937, Acc: 77.78%\n",
      "Epoch 50: Train Loss: 0.2344, Acc: 63.89% | Val Loss: 0.1922, Acc: 77.78%\n",
      "Epoch 60: Train Loss: 0.2301, Acc: 63.89% | Val Loss: 0.1919, Acc: 77.78%\n",
      "Epoch 70: Train Loss: 0.2331, Acc: 63.89% | Val Loss: 0.1925, Acc: 77.78%\n",
      "Early stopping at epoch 77\n",
      "Restoring model weights from epoch 57\n",
      "Final Validation Accuracy: 0.7778\n",
      " Fold 1/5 | Validation Accuracy: 0.7778\n",
      "[3] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2738, Acc: 35.19% | Val Loss: 0.2744, Acc: 33.33%\n",
      "Epoch 10: Train Loss: 0.2479, Acc: 55.56% | Val Loss: 0.2502, Acc: 40.74%\n",
      "Epoch 20: Train Loss: 0.2333, Acc: 66.67% | Val Loss: 0.2348, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.2255, Acc: 66.67% | Val Loss: 0.2266, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.2223, Acc: 66.67% | Val Loss: 0.2232, Acc: 66.67%\n",
      "Epoch 50: Train Loss: 0.2196, Acc: 66.67% | Val Loss: 0.2224, Acc: 66.67%\n",
      "Epoch 60: Train Loss: 0.2204, Acc: 66.67% | Val Loss: 0.2224, Acc: 66.67%\n",
      "Epoch 70: Train Loss: 0.2237, Acc: 66.67% | Val Loss: 0.2225, Acc: 66.67%\n",
      "Early stopping at epoch 72\n",
      "Restoring model weights from epoch 52\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 2/5 | Validation Accuracy: 0.6667\n",
      "[3] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.5228, Acc: 33.33% | Val Loss: 0.5410, Acc: 29.63%\n",
      "Epoch 10: Train Loss: 0.4988, Acc: 34.26% | Val Loss: 0.5076, Acc: 29.63%\n",
      "Epoch 20: Train Loss: 0.4312, Acc: 35.19% | Val Loss: 0.4662, Acc: 29.63%\n",
      "Epoch 30: Train Loss: 0.3981, Acc: 36.11% | Val Loss: 0.4171, Acc: 33.33%\n",
      "Epoch 40: Train Loss: 0.3527, Acc: 35.19% | Val Loss: 0.3631, Acc: 25.93%\n",
      "Epoch 50: Train Loss: 0.3092, Acc: 37.96% | Val Loss: 0.3108, Acc: 29.63%\n",
      "Epoch 60: Train Loss: 0.2541, Acc: 52.78% | Val Loss: 0.2689, Acc: 40.74%\n",
      "Epoch 70: Train Loss: 0.2458, Acc: 56.48% | Val Loss: 0.2407, Acc: 62.96%\n",
      "Epoch 80: Train Loss: 0.2360, Acc: 61.11% | Val Loss: 0.2257, Acc: 66.67%\n",
      "Epoch 90: Train Loss: 0.2330, Acc: 62.04% | Val Loss: 0.2182, Acc: 70.37%\n",
      "Epoch 100: Train Loss: 0.2383, Acc: 63.89% | Val Loss: 0.2144, Acc: 70.37%\n",
      "Epoch 110: Train Loss: 0.2337, Acc: 62.04% | Val Loss: 0.2124, Acc: 70.37%\n",
      "Epoch 120: Train Loss: 0.2296, Acc: 63.89% | Val Loss: 0.2113, Acc: 70.37%\n",
      "Epoch 130: Train Loss: 0.2243, Acc: 62.96% | Val Loss: 0.2109, Acc: 70.37%\n",
      "Epoch 140: Train Loss: 0.2284, Acc: 63.89% | Val Loss: 0.2107, Acc: 70.37%\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 3/5 | Validation Accuracy: 0.7037\n",
      "[3] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2375, Acc: 54.63% | Val Loss: 0.2527, Acc: 59.26%\n",
      "Epoch 10: Train Loss: 0.2413, Acc: 62.96% | Val Loss: 0.2551, Acc: 55.56%\n",
      "Epoch 20: Train Loss: 0.2324, Acc: 66.67% | Val Loss: 0.2645, Acc: 55.56%\n",
      "Early stopping at epoch 23\n",
      "Restoring model weights from epoch 3\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 4/5 | Validation Accuracy: 0.5556\n",
      "[3] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.0001, 'l2': 0.01, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2450, Acc: 56.48% | Val Loss: 0.2536, Acc: 48.15%\n",
      "Epoch 10: Train Loss: 0.2443, Acc: 57.41% | Val Loss: 0.2449, Acc: 55.56%\n",
      "Epoch 20: Train Loss: 0.2326, Acc: 59.26% | Val Loss: 0.2409, Acc: 59.26%\n",
      "Epoch 30: Train Loss: 0.2249, Acc: 64.81% | Val Loss: 0.2389, Acc: 59.26%\n",
      "Epoch 40: Train Loss: 0.2329, Acc: 67.59% | Val Loss: 0.2376, Acc: 62.96%\n",
      "Epoch 50: Train Loss: 0.2215, Acc: 67.59% | Val Loss: 0.2365, Acc: 62.96%\n",
      "Epoch 60: Train Loss: 0.2151, Acc: 67.59% | Val Loss: 0.2356, Acc: 62.96%\n",
      "Epoch 70: Train Loss: 0.2257, Acc: 67.59% | Val Loss: 0.2351, Acc: 62.96%\n",
      "Epoch 80: Train Loss: 0.2177, Acc: 67.59% | Val Loss: 0.2349, Acc: 62.96%\n",
      "Epoch 90: Train Loss: 0.2209, Acc: 67.59% | Val Loss: 0.2348, Acc: 62.96%\n",
      "Epoch 100: Train Loss: 0.2225, Acc: 67.59% | Val Loss: 0.2346, Acc: 62.96%\n",
      "Epoch 110: Train Loss: 0.2220, Acc: 67.59% | Val Loss: 0.2345, Acc: 62.96%\n",
      "Epoch 120: Train Loss: 0.2198, Acc: 67.59% | Val Loss: 0.2345, Acc: 62.96%\n",
      "Epoch 130: Train Loss: 0.2179, Acc: 67.59% | Val Loss: 0.2347, Acc: 62.96%\n",
      "Early stopping at epoch 133\n",
      "Restoring model weights from epoch 113\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 5/5 | Validation Accuracy: 0.6296\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.6667\n",
      "[5] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [True], 'learning_rate': 0.0001, 'l1': 0.0, 'l2': 0.01, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2778, Acc: 51.85% | Val Loss: 0.2602, Acc: 66.67%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 1/5 | Validation Accuracy: 0.6667\n",
      "[5] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [True], 'learning_rate': 0.0001, 'l1': 0.0, 'l2': 0.01, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3001, Acc: 51.85% | Val Loss: 0.2777, Acc: 59.26%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 2/5 | Validation Accuracy: 0.5926\n",
      "[5] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [True], 'learning_rate': 0.0001, 'l1': 0.0, 'l2': 0.01, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3255, Acc: 46.30% | Val Loss: 0.3247, Acc: 29.63%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.2963\n",
      " Fold 3/5 | Validation Accuracy: 0.2963\n",
      "[5] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [True], 'learning_rate': 0.0001, 'l1': 0.0, 'l2': 0.01, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3135, Acc: 41.67% | Val Loss: 0.2576, Acc: 55.56%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 4/5 | Validation Accuracy: 0.5556\n",
      "[5] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Tanh'>], 'batch_norm': [True], 'learning_rate': 0.0001, 'l1': 0.0, 'l2': 0.01, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2959, Acc: 50.93% | Val Loss: 0.2649, Acc: 59.26%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5926\n",
      " Fold 5/5 | Validation Accuracy: 0.5926\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5407\n",
      "[5] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 0.001, 'l2': 0.0, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2467, Acc: 50.93% | Val Loss: 0.2220, Acc: 77.78%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.7778\n",
      " Fold 1/5 | Validation Accuracy: 0.7778\n",
      "[5] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 0.001, 'l2': 0.0, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4083, Acc: 34.26% | Val Loss: 0.3680, Acc: 33.33%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.3333\n",
      " Fold 2/5 | Validation Accuracy: 0.3333\n",
      "[5] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 0.001, 'l2': 0.0, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2945, Acc: 62.96% | Val Loss: 0.2354, Acc: 70.37%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 3/5 | Validation Accuracy: 0.7037\n",
      "[5] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 0.001, 'l2': 0.0, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2342, Acc: 65.74% | Val Loss: 0.3221, Acc: 55.56%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 4/5 | Validation Accuracy: 0.5556\n",
      "[5] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [5], 'hidden_activation': [<class 'src.activation_functions.Activation_Sigmoid'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 0.001, 'l2': 0.0, 'dropout_rate': 0.3, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.05, 'patience': 0, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4624, Acc: 35.19% | Val Loss: 0.4527, Acc: 40.74%\n",
      "Early stopping at epoch 0\n",
      "Restoring model weights from epoch 0\n",
      "Final Validation Accuracy: 0.4074\n",
      " Fold 5/5 | Validation Accuracy: 0.4074\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5556\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3510, Acc: 46.30% | Val Loss: 0.4354, Acc: 51.85%\n",
      "Epoch 10: Train Loss: 0.2233, Acc: 66.67% | Val Loss: 0.2133, Acc: 74.07%\n",
      "Epoch 20: Train Loss: 0.1840, Acc: 74.07% | Val Loss: 0.2390, Acc: 55.56%\n",
      "Epoch 30: Train Loss: 0.1648, Acc: 81.48% | Val Loss: 0.2246, Acc: 48.15%\n",
      "Epoch 40: Train Loss: 0.1417, Acc: 84.26% | Val Loss: 0.2385, Acc: 44.44%\n",
      "Epoch 50: Train Loss: 0.1251, Acc: 85.19% | Val Loss: 0.2411, Acc: 59.26%\n",
      "Epoch 60: Train Loss: 0.1001, Acc: 88.89% | Val Loss: 0.2309, Acc: 66.67%\n",
      "Early stopping at epoch 63\n",
      "Restoring model weights from epoch 13\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 1/5 | Validation Accuracy: 0.6296\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3412, Acc: 49.07% | Val Loss: 0.4570, Acc: 40.74%\n",
      "Epoch 10: Train Loss: 0.1784, Acc: 73.15% | Val Loss: 0.2639, Acc: 55.56%\n",
      "Epoch 20: Train Loss: 0.1488, Acc: 81.48% | Val Loss: 0.2578, Acc: 59.26%\n",
      "Epoch 30: Train Loss: 0.1170, Acc: 84.26% | Val Loss: 0.2533, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.0820, Acc: 89.81% | Val Loss: 0.2654, Acc: 70.37%\n",
      "Epoch 50: Train Loss: 0.0551, Acc: 91.67% | Val Loss: 0.2700, Acc: 77.78%\n",
      "Epoch 60: Train Loss: 0.0422, Acc: 100.00% | Val Loss: 0.2779, Acc: 88.89%\n",
      "Epoch 70: Train Loss: 0.0328, Acc: 100.00% | Val Loss: 0.2825, Acc: 88.89%\n",
      "Early stopping at epoch 79\n",
      "Restoring model weights from epoch 29\n",
      "Final Validation Accuracy: 0.9630\n",
      " Fold 2/5 | Validation Accuracy: 0.9630\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2649, Acc: 62.04% | Val Loss: 0.2801, Acc: 55.56%\n",
      "Epoch 10: Train Loss: 0.1501, Acc: 84.26% | Val Loss: 0.2451, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.0732, Acc: 99.07% | Val Loss: 0.2605, Acc: 88.89%\n",
      "Epoch 30: Train Loss: 0.0366, Acc: 100.00% | Val Loss: 0.2826, Acc: 96.30%\n",
      "Epoch 40: Train Loss: 0.0219, Acc: 100.00% | Val Loss: 0.3069, Acc: 100.00%\n",
      "Epoch 50: Train Loss: 0.0147, Acc: 100.00% | Val Loss: 0.3179, Acc: 100.00%\n",
      "Early stopping at epoch 58\n",
      "Restoring model weights from epoch 8\n",
      "Final Validation Accuracy: 1.0000\n",
      " Fold 3/5 | Validation Accuracy: 1.0000\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3005, Acc: 49.07% | Val Loss: 0.3372, Acc: 40.74%\n",
      "Epoch 10: Train Loss: 0.2005, Acc: 71.30% | Val Loss: 0.2713, Acc: 55.56%\n",
      "Epoch 20: Train Loss: 0.1790, Acc: 69.44% | Val Loss: 0.2818, Acc: 59.26%\n",
      "Epoch 30: Train Loss: 0.1340, Acc: 83.33% | Val Loss: 0.2956, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.0861, Acc: 91.67% | Val Loss: 0.3022, Acc: 66.67%\n",
      "Epoch 50: Train Loss: 0.0600, Acc: 100.00% | Val Loss: 0.3140, Acc: 88.89%\n",
      "Epoch 60: Train Loss: 0.0426, Acc: 100.00% | Val Loss: 0.3192, Acc: 85.19%\n",
      "Early stopping at epoch 62\n",
      "Restoring model weights from epoch 12\n",
      "Final Validation Accuracy: 0.8519\n",
      " Fold 4/5 | Validation Accuracy: 0.8519\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 50, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2841, Acc: 57.41% | Val Loss: 0.2964, Acc: 59.26%\n",
      "Epoch 10: Train Loss: 0.2006, Acc: 75.93% | Val Loss: 0.2650, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.1711, Acc: 74.07% | Val Loss: 0.2710, Acc: 74.07%\n",
      "Epoch 30: Train Loss: 0.1327, Acc: 82.41% | Val Loss: 0.2912, Acc: 77.78%\n",
      "Epoch 40: Train Loss: 0.0840, Acc: 89.81% | Val Loss: 0.3028, Acc: 74.07%\n",
      "Epoch 50: Train Loss: 0.0507, Acc: 95.37% | Val Loss: 0.3218, Acc: 88.89%\n",
      "Early stopping at epoch 57\n",
      "Restoring model weights from epoch 7\n",
      "Final Validation Accuracy: 0.8889\n",
      " Fold 5/5 | Validation Accuracy: 0.8889\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.8667\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.01, 'l2': 0.001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2580, Acc: 60.19% | Val Loss: 0.2717, Acc: 66.67%\n",
      "Epoch 10: Train Loss: 0.2429, Acc: 62.04% | Val Loss: 0.2357, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.2382, Acc: 59.26% | Val Loss: 0.2183, Acc: 77.78%\n",
      "Epoch 30: Train Loss: 0.2354, Acc: 60.19% | Val Loss: 0.2117, Acc: 77.78%\n",
      "Epoch 40: Train Loss: 0.2337, Acc: 62.96% | Val Loss: 0.2092, Acc: 74.07%\n",
      "Epoch 50: Train Loss: 0.2325, Acc: 63.89% | Val Loss: 0.2074, Acc: 77.78%\n",
      "Epoch 60: Train Loss: 0.2316, Acc: 63.89% | Val Loss: 0.2052, Acc: 77.78%\n",
      "Epoch 70: Train Loss: 0.2311, Acc: 63.89% | Val Loss: 0.2029, Acc: 77.78%\n",
      "Epoch 80: Train Loss: 0.2309, Acc: 63.89% | Val Loss: 0.2011, Acc: 77.78%\n",
      "Epoch 90: Train Loss: 0.2306, Acc: 63.89% | Val Loss: 0.2000, Acc: 77.78%\n",
      "Epoch 100: Train Loss: 0.2305, Acc: 63.89% | Val Loss: 0.1993, Acc: 77.78%\n",
      "Epoch 110: Train Loss: 0.2304, Acc: 63.89% | Val Loss: 0.1988, Acc: 77.78%\n",
      "Epoch 120: Train Loss: 0.2302, Acc: 63.89% | Val Loss: 0.1982, Acc: 77.78%\n",
      "Epoch 130: Train Loss: 0.2300, Acc: 63.89% | Val Loss: 0.1975, Acc: 77.78%\n",
      "Epoch 140: Train Loss: 0.2298, Acc: 63.89% | Val Loss: 0.1970, Acc: 77.78%\n",
      "Epoch 150: Train Loss: 0.2296, Acc: 63.89% | Val Loss: 0.1967, Acc: 77.78%\n",
      "Epoch 160: Train Loss: 0.2294, Acc: 63.89% | Val Loss: 0.1963, Acc: 77.78%\n",
      "Epoch 170: Train Loss: 0.2292, Acc: 63.89% | Val Loss: 0.1962, Acc: 77.78%\n",
      "Epoch 180: Train Loss: 0.2291, Acc: 63.89% | Val Loss: 0.1962, Acc: 77.78%\n",
      "Epoch 190: Train Loss: 0.2290, Acc: 63.89% | Val Loss: 0.1962, Acc: 77.78%\n",
      "Early stopping at epoch 198\n",
      "Restoring model weights from epoch 168\n",
      "Final Validation Accuracy: 0.7778\n",
      " Fold 1/5 | Validation Accuracy: 0.7778\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.01, 'l2': 0.001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2808, Acc: 57.41% | Val Loss: 0.2769, Acc: 70.37%\n",
      "Epoch 10: Train Loss: 0.2699, Acc: 62.04% | Val Loss: 0.2716, Acc: 70.37%\n",
      "Epoch 20: Train Loss: 0.2630, Acc: 66.67% | Val Loss: 0.2665, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.2575, Acc: 66.67% | Val Loss: 0.2609, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.2530, Acc: 66.67% | Val Loss: 0.2565, Acc: 66.67%\n",
      "Epoch 50: Train Loss: 0.2490, Acc: 66.67% | Val Loss: 0.2530, Acc: 66.67%\n",
      "Epoch 60: Train Loss: 0.2449, Acc: 66.67% | Val Loss: 0.2490, Acc: 66.67%\n",
      "Epoch 70: Train Loss: 0.2400, Acc: 66.67% | Val Loss: 0.2453, Acc: 66.67%\n",
      "Epoch 80: Train Loss: 0.2342, Acc: 66.67% | Val Loss: 0.2420, Acc: 66.67%\n",
      "Epoch 90: Train Loss: 0.2293, Acc: 66.67% | Val Loss: 0.2396, Acc: 66.67%\n",
      "Epoch 100: Train Loss: 0.2253, Acc: 66.67% | Val Loss: 0.2375, Acc: 66.67%\n",
      "Epoch 110: Train Loss: 0.2226, Acc: 66.67% | Val Loss: 0.2357, Acc: 66.67%\n",
      "Epoch 120: Train Loss: 0.2209, Acc: 66.67% | Val Loss: 0.2343, Acc: 66.67%\n",
      "Epoch 130: Train Loss: 0.2196, Acc: 66.67% | Val Loss: 0.2333, Acc: 66.67%\n",
      "Epoch 140: Train Loss: 0.2186, Acc: 66.67% | Val Loss: 0.2324, Acc: 66.67%\n",
      "Epoch 150: Train Loss: 0.2176, Acc: 66.67% | Val Loss: 0.2316, Acc: 66.67%\n",
      "Epoch 160: Train Loss: 0.2166, Acc: 66.67% | Val Loss: 0.2309, Acc: 66.67%\n",
      "Epoch 170: Train Loss: 0.2160, Acc: 66.67% | Val Loss: 0.2303, Acc: 66.67%\n",
      "Epoch 180: Train Loss: 0.2155, Acc: 66.67% | Val Loss: 0.2298, Acc: 66.67%\n",
      "Epoch 190: Train Loss: 0.2150, Acc: 66.67% | Val Loss: 0.2294, Acc: 66.67%\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 2/5 | Validation Accuracy: 0.6667\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.01, 'l2': 0.001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.5108, Acc: 35.19% | Val Loss: 0.4812, Acc: 29.63%\n",
      "Epoch 10: Train Loss: 0.4316, Acc: 37.96% | Val Loss: 0.3969, Acc: 48.15%\n",
      "Epoch 20: Train Loss: 0.3474, Acc: 40.74% | Val Loss: 0.3261, Acc: 55.56%\n",
      "Epoch 30: Train Loss: 0.2865, Acc: 42.59% | Val Loss: 0.2704, Acc: 62.96%\n",
      "Epoch 40: Train Loss: 0.2497, Acc: 62.04% | Val Loss: 0.2377, Acc: 70.37%\n",
      "Epoch 50: Train Loss: 0.2345, Acc: 62.96% | Val Loss: 0.2247, Acc: 62.96%\n",
      "Epoch 60: Train Loss: 0.2286, Acc: 66.67% | Val Loss: 0.2188, Acc: 66.67%\n",
      "Epoch 70: Train Loss: 0.2257, Acc: 65.74% | Val Loss: 0.2157, Acc: 70.37%\n",
      "Epoch 80: Train Loss: 0.2243, Acc: 65.74% | Val Loss: 0.2142, Acc: 70.37%\n",
      "Epoch 90: Train Loss: 0.2239, Acc: 65.74% | Val Loss: 0.2131, Acc: 70.37%\n",
      "Epoch 100: Train Loss: 0.2237, Acc: 64.81% | Val Loss: 0.2125, Acc: 70.37%\n",
      "Epoch 110: Train Loss: 0.2235, Acc: 66.67% | Val Loss: 0.2121, Acc: 70.37%\n",
      "Epoch 120: Train Loss: 0.2235, Acc: 65.74% | Val Loss: 0.2118, Acc: 70.37%\n",
      "Epoch 130: Train Loss: 0.2234, Acc: 65.74% | Val Loss: 0.2117, Acc: 70.37%\n",
      "Epoch 140: Train Loss: 0.2233, Acc: 65.74% | Val Loss: 0.2115, Acc: 70.37%\n",
      "Epoch 150: Train Loss: 0.2234, Acc: 65.74% | Val Loss: 0.2113, Acc: 70.37%\n",
      "Epoch 160: Train Loss: 0.2235, Acc: 65.74% | Val Loss: 0.2112, Acc: 70.37%\n",
      "Epoch 170: Train Loss: 0.2237, Acc: 65.74% | Val Loss: 0.2112, Acc: 70.37%\n",
      "Epoch 180: Train Loss: 0.2238, Acc: 65.74% | Val Loss: 0.2112, Acc: 70.37%\n",
      "Epoch 190: Train Loss: 0.2239, Acc: 65.74% | Val Loss: 0.2113, Acc: 70.37%\n",
      "Early stopping at epoch 198\n",
      "Restoring model weights from epoch 168\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 3/5 | Validation Accuracy: 0.7037\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.01, 'l2': 0.001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2687, Acc: 66.67% | Val Loss: 0.3918, Acc: 62.96%\n",
      "Epoch 10: Train Loss: 0.2526, Acc: 71.30% | Val Loss: 0.3762, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.2420, Acc: 71.30% | Val Loss: 0.3621, Acc: 70.37%\n",
      "Epoch 30: Train Loss: 0.2324, Acc: 71.30% | Val Loss: 0.3478, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.2251, Acc: 70.37% | Val Loss: 0.3331, Acc: 66.67%\n",
      "Epoch 50: Train Loss: 0.2207, Acc: 70.37% | Val Loss: 0.3206, Acc: 62.96%\n",
      "Epoch 60: Train Loss: 0.2179, Acc: 68.52% | Val Loss: 0.3108, Acc: 66.67%\n",
      "Epoch 70: Train Loss: 0.2154, Acc: 66.67% | Val Loss: 0.3031, Acc: 66.67%\n",
      "Epoch 80: Train Loss: 0.2143, Acc: 67.59% | Val Loss: 0.2963, Acc: 62.96%\n",
      "Epoch 90: Train Loss: 0.2126, Acc: 67.59% | Val Loss: 0.2910, Acc: 51.85%\n",
      "Epoch 100: Train Loss: 0.2116, Acc: 70.37% | Val Loss: 0.2870, Acc: 55.56%\n",
      "Epoch 110: Train Loss: 0.2109, Acc: 70.37% | Val Loss: 0.2834, Acc: 55.56%\n",
      "Epoch 120: Train Loss: 0.2103, Acc: 69.44% | Val Loss: 0.2800, Acc: 55.56%\n",
      "Epoch 130: Train Loss: 0.2096, Acc: 69.44% | Val Loss: 0.2768, Acc: 55.56%\n",
      "Epoch 140: Train Loss: 0.2093, Acc: 69.44% | Val Loss: 0.2748, Acc: 55.56%\n",
      "Epoch 150: Train Loss: 0.2094, Acc: 69.44% | Val Loss: 0.2736, Acc: 55.56%\n",
      "Epoch 160: Train Loss: 0.2098, Acc: 69.44% | Val Loss: 0.2733, Acc: 55.56%\n",
      "Epoch 170: Train Loss: 0.2101, Acc: 69.44% | Val Loss: 0.2728, Acc: 55.56%\n",
      "Epoch 180: Train Loss: 0.2103, Acc: 69.44% | Val Loss: 0.2718, Acc: 55.56%\n",
      "Epoch 190: Train Loss: 0.2105, Acc: 69.44% | Val Loss: 0.2713, Acc: 55.56%\n",
      "Final Validation Accuracy: 0.5556\n",
      " Fold 4/5 | Validation Accuracy: 0.5556\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.0] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [3], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.01, 'l1': 0.01, 'l2': 0.001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4781, Acc: 39.81% | Val Loss: 0.4509, Acc: 33.33%\n",
      "Epoch 10: Train Loss: 0.4240, Acc: 38.89% | Val Loss: 0.4049, Acc: 37.04%\n",
      "Epoch 20: Train Loss: 0.3722, Acc: 43.52% | Val Loss: 0.3637, Acc: 44.44%\n",
      "Epoch 30: Train Loss: 0.3261, Acc: 50.93% | Val Loss: 0.3264, Acc: 48.15%\n",
      "Epoch 40: Train Loss: 0.2931, Acc: 52.78% | Val Loss: 0.2978, Acc: 40.74%\n",
      "Epoch 50: Train Loss: 0.2729, Acc: 57.41% | Val Loss: 0.2806, Acc: 40.74%\n",
      "Epoch 60: Train Loss: 0.2589, Acc: 63.89% | Val Loss: 0.2690, Acc: 40.74%\n",
      "Epoch 70: Train Loss: 0.2491, Acc: 63.89% | Val Loss: 0.2607, Acc: 51.85%\n",
      "Epoch 80: Train Loss: 0.2422, Acc: 65.74% | Val Loss: 0.2545, Acc: 55.56%\n",
      "Epoch 90: Train Loss: 0.2372, Acc: 65.74% | Val Loss: 0.2498, Acc: 55.56%\n",
      "Epoch 100: Train Loss: 0.2328, Acc: 66.67% | Val Loss: 0.2457, Acc: 55.56%\n",
      "Epoch 110: Train Loss: 0.2298, Acc: 68.52% | Val Loss: 0.2429, Acc: 59.26%\n",
      "Epoch 120: Train Loss: 0.2277, Acc: 67.59% | Val Loss: 0.2409, Acc: 59.26%\n",
      "Epoch 130: Train Loss: 0.2259, Acc: 67.59% | Val Loss: 0.2393, Acc: 62.96%\n",
      "Epoch 140: Train Loss: 0.2246, Acc: 68.52% | Val Loss: 0.2382, Acc: 59.26%\n",
      "Epoch 150: Train Loss: 0.2236, Acc: 68.52% | Val Loss: 0.2373, Acc: 59.26%\n",
      "Epoch 160: Train Loss: 0.2227, Acc: 67.59% | Val Loss: 0.2368, Acc: 62.96%\n",
      "Epoch 170: Train Loss: 0.2219, Acc: 67.59% | Val Loss: 0.2364, Acc: 62.96%\n",
      "Epoch 180: Train Loss: 0.2214, Acc: 67.59% | Val Loss: 0.2360, Acc: 62.96%\n",
      "Epoch 190: Train Loss: 0.2210, Acc: 67.59% | Val Loss: 0.2357, Acc: 62.96%\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 5/5 | Validation Accuracy: 0.6296\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.6667\n",
      "[6] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3534, Acc: 62.96% | Val Loss: 0.2503, Acc: 70.37%\n",
      "Epoch 10: Train Loss: 0.3515, Acc: 61.11% | Val Loss: 0.2503, Acc: 70.37%\n",
      "Epoch 20: Train Loss: 0.3625, Acc: 59.26% | Val Loss: 0.2502, Acc: 70.37%\n",
      "Epoch 30: Train Loss: 0.3504, Acc: 61.11% | Val Loss: 0.2502, Acc: 70.37%\n",
      "Epoch 40: Train Loss: 0.3550, Acc: 59.26% | Val Loss: 0.2502, Acc: 70.37%\n",
      "Epoch 50: Train Loss: 0.3533, Acc: 61.11% | Val Loss: 0.2502, Acc: 70.37%\n",
      "Epoch 60: Train Loss: 0.3556, Acc: 60.19% | Val Loss: 0.2501, Acc: 70.37%\n",
      "Epoch 70: Train Loss: 0.3674, Acc: 59.26% | Val Loss: 0.2501, Acc: 70.37%\n",
      "Epoch 80: Train Loss: 0.3320, Acc: 63.89% | Val Loss: 0.2501, Acc: 70.37%\n",
      "Epoch 90: Train Loss: 0.3538, Acc: 61.11% | Val Loss: 0.2501, Acc: 70.37%\n",
      "Epoch 100: Train Loss: 0.3495, Acc: 61.11% | Val Loss: 0.2501, Acc: 70.37%\n",
      "Epoch 110: Train Loss: 0.3447, Acc: 59.26% | Val Loss: 0.2500, Acc: 70.37%\n",
      "Epoch 120: Train Loss: 0.3684, Acc: 59.26% | Val Loss: 0.2500, Acc: 70.37%\n",
      "Epoch 130: Train Loss: 0.3486, Acc: 60.19% | Val Loss: 0.2500, Acc: 70.37%\n",
      "Epoch 140: Train Loss: 0.3385, Acc: 62.96% | Val Loss: 0.2500, Acc: 70.37%\n",
      "Epoch 150: Train Loss: 0.3600, Acc: 60.19% | Val Loss: 0.2499, Acc: 70.37%\n",
      "Epoch 160: Train Loss: 0.3307, Acc: 63.89% | Val Loss: 0.2499, Acc: 70.37%\n",
      "Epoch 170: Train Loss: 0.3549, Acc: 60.19% | Val Loss: 0.2499, Acc: 70.37%\n",
      "Epoch 180: Train Loss: 0.3583, Acc: 60.19% | Val Loss: 0.2498, Acc: 70.37%\n",
      "Epoch 190: Train Loss: 0.3410, Acc: 62.04% | Val Loss: 0.2498, Acc: 70.37%\n",
      "Final Validation Accuracy: 0.7037\n",
      " Fold 1/5 | Validation Accuracy: 0.7037\n",
      "[6] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4193, Acc: 47.22% | Val Loss: 0.3810, Acc: 40.74%\n",
      "Epoch 10: Train Loss: 0.3581, Acc: 55.56% | Val Loss: 0.3801, Acc: 40.74%\n",
      "Epoch 20: Train Loss: 0.3545, Acc: 52.78% | Val Loss: 0.3791, Acc: 40.74%\n",
      "Epoch 30: Train Loss: 0.3988, Acc: 50.93% | Val Loss: 0.3782, Acc: 40.74%\n",
      "Epoch 40: Train Loss: 0.4089, Acc: 47.22% | Val Loss: 0.3772, Acc: 40.74%\n",
      "Epoch 50: Train Loss: 0.3410, Acc: 55.56% | Val Loss: 0.3762, Acc: 40.74%\n",
      "Epoch 60: Train Loss: 0.3839, Acc: 49.07% | Val Loss: 0.3753, Acc: 40.74%\n",
      "Epoch 70: Train Loss: 0.3559, Acc: 54.63% | Val Loss: 0.3745, Acc: 40.74%\n",
      "Epoch 80: Train Loss: 0.3768, Acc: 50.00% | Val Loss: 0.3735, Acc: 40.74%\n",
      "Epoch 90: Train Loss: 0.3667, Acc: 54.63% | Val Loss: 0.3726, Acc: 40.74%\n",
      "Epoch 100: Train Loss: 0.3577, Acc: 53.70% | Val Loss: 0.3714, Acc: 40.74%\n",
      "Epoch 110: Train Loss: 0.3688, Acc: 50.00% | Val Loss: 0.3703, Acc: 40.74%\n",
      "Epoch 120: Train Loss: 0.3544, Acc: 54.63% | Val Loss: 0.3695, Acc: 40.74%\n",
      "Epoch 130: Train Loss: 0.4164, Acc: 48.15% | Val Loss: 0.3685, Acc: 40.74%\n",
      "Epoch 140: Train Loss: 0.3768, Acc: 53.70% | Val Loss: 0.3675, Acc: 40.74%\n",
      "Epoch 150: Train Loss: 0.3868, Acc: 50.93% | Val Loss: 0.3664, Acc: 40.74%\n",
      "Epoch 160: Train Loss: 0.3877, Acc: 51.85% | Val Loss: 0.3654, Acc: 40.74%\n",
      "Epoch 170: Train Loss: 0.3514, Acc: 56.48% | Val Loss: 0.3644, Acc: 40.74%\n",
      "Epoch 180: Train Loss: 0.3688, Acc: 55.56% | Val Loss: 0.3635, Acc: 40.74%\n",
      "Epoch 190: Train Loss: 0.3913, Acc: 50.00% | Val Loss: 0.3627, Acc: 40.74%\n",
      "Final Validation Accuracy: 0.4074\n",
      " Fold 2/5 | Validation Accuracy: 0.4074\n",
      "[6] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3234, Acc: 57.41% | Val Loss: 0.2853, Acc: 66.67%\n",
      "Epoch 10: Train Loss: 0.3465, Acc: 54.63% | Val Loss: 0.2850, Acc: 66.67%\n",
      "Epoch 20: Train Loss: 0.3406, Acc: 56.48% | Val Loss: 0.2847, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.3373, Acc: 53.70% | Val Loss: 0.2844, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.3191, Acc: 57.41% | Val Loss: 0.2842, Acc: 66.67%\n",
      "Epoch 50: Train Loss: 0.3300, Acc: 60.19% | Val Loss: 0.2839, Acc: 66.67%\n",
      "Epoch 60: Train Loss: 0.3102, Acc: 56.48% | Val Loss: 0.2836, Acc: 66.67%\n",
      "Epoch 70: Train Loss: 0.3138, Acc: 57.41% | Val Loss: 0.2833, Acc: 66.67%\n",
      "Epoch 80: Train Loss: 0.3207, Acc: 57.41% | Val Loss: 0.2830, Acc: 66.67%\n",
      "Epoch 90: Train Loss: 0.3169, Acc: 59.26% | Val Loss: 0.2828, Acc: 66.67%\n",
      "Epoch 100: Train Loss: 0.3193, Acc: 58.33% | Val Loss: 0.2825, Acc: 66.67%\n",
      "Epoch 110: Train Loss: 0.3185, Acc: 55.56% | Val Loss: 0.2823, Acc: 66.67%\n",
      "Epoch 120: Train Loss: 0.3147, Acc: 60.19% | Val Loss: 0.2820, Acc: 66.67%\n",
      "Epoch 130: Train Loss: 0.3301, Acc: 55.56% | Val Loss: 0.2817, Acc: 66.67%\n",
      "Epoch 140: Train Loss: 0.3258, Acc: 55.56% | Val Loss: 0.2815, Acc: 66.67%\n",
      "Epoch 150: Train Loss: 0.3234, Acc: 55.56% | Val Loss: 0.2812, Acc: 66.67%\n",
      "Epoch 160: Train Loss: 0.3197, Acc: 59.26% | Val Loss: 0.2809, Acc: 66.67%\n",
      "Epoch 170: Train Loss: 0.3432, Acc: 54.63% | Val Loss: 0.2806, Acc: 66.67%\n",
      "Epoch 180: Train Loss: 0.3209, Acc: 57.41% | Val Loss: 0.2804, Acc: 66.67%\n",
      "Epoch 190: Train Loss: 0.3389, Acc: 55.56% | Val Loss: 0.2802, Acc: 66.67%\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 3/5 | Validation Accuracy: 0.6667\n",
      "[6] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3663, Acc: 49.07% | Val Loss: 0.3572, Acc: 40.74%\n",
      "Epoch 10: Train Loss: 0.3450, Acc: 50.00% | Val Loss: 0.3564, Acc: 40.74%\n",
      "Epoch 20: Train Loss: 0.3418, Acc: 53.70% | Val Loss: 0.3556, Acc: 40.74%\n",
      "Epoch 30: Train Loss: 0.3627, Acc: 47.22% | Val Loss: 0.3549, Acc: 40.74%\n",
      "Epoch 40: Train Loss: 0.3672, Acc: 50.00% | Val Loss: 0.3541, Acc: 40.74%\n",
      "Epoch 50: Train Loss: 0.3846, Acc: 46.30% | Val Loss: 0.3534, Acc: 40.74%\n",
      "Epoch 60: Train Loss: 0.3577, Acc: 45.37% | Val Loss: 0.3527, Acc: 40.74%\n",
      "Epoch 70: Train Loss: 0.3626, Acc: 51.85% | Val Loss: 0.3520, Acc: 40.74%\n",
      "Epoch 80: Train Loss: 0.3729, Acc: 48.15% | Val Loss: 0.3512, Acc: 40.74%\n",
      "Epoch 90: Train Loss: 0.3558, Acc: 50.93% | Val Loss: 0.3505, Acc: 40.74%\n",
      "Epoch 100: Train Loss: 0.3504, Acc: 50.00% | Val Loss: 0.3498, Acc: 40.74%\n",
      "Epoch 110: Train Loss: 0.3435, Acc: 52.78% | Val Loss: 0.3491, Acc: 40.74%\n",
      "Epoch 120: Train Loss: 0.3227, Acc: 55.56% | Val Loss: 0.3483, Acc: 40.74%\n",
      "Epoch 130: Train Loss: 0.3463, Acc: 49.07% | Val Loss: 0.3476, Acc: 40.74%\n",
      "Epoch 140: Train Loss: 0.3372, Acc: 51.85% | Val Loss: 0.3469, Acc: 40.74%\n",
      "Epoch 150: Train Loss: 0.3491, Acc: 50.00% | Val Loss: 0.3462, Acc: 40.74%\n",
      "Epoch 160: Train Loss: 0.3234, Acc: 51.85% | Val Loss: 0.3455, Acc: 40.74%\n",
      "Epoch 170: Train Loss: 0.3667, Acc: 46.30% | Val Loss: 0.3449, Acc: 40.74%\n",
      "Epoch 180: Train Loss: 0.3658, Acc: 43.52% | Val Loss: 0.3442, Acc: 40.74%\n",
      "Epoch 190: Train Loss: 0.3499, Acc: 50.93% | Val Loss: 0.3435, Acc: 40.74%\n",
      "Final Validation Accuracy: 0.4074\n",
      " Fold 4/5 | Validation Accuracy: 0.4074\n",
      "[6] [<class 'src.activation_functions.Activation_ReLU'>] [0.1] [False]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [6], 'hidden_activation': [<class 'src.activation_functions.Activation_ReLU'>], 'batch_norm': [False], 'learning_rate': 0.0001, 'l1': 0.0001, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0, 'patience': 20, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4994, Acc: 40.74% | Val Loss: 0.4812, Acc: 33.33%\n",
      "Epoch 10: Train Loss: 0.5006, Acc: 40.74% | Val Loss: 0.4799, Acc: 33.33%\n",
      "Epoch 20: Train Loss: 0.5031, Acc: 42.59% | Val Loss: 0.4786, Acc: 33.33%\n",
      "Epoch 30: Train Loss: 0.4760, Acc: 43.52% | Val Loss: 0.4772, Acc: 33.33%\n",
      "Epoch 40: Train Loss: 0.4471, Acc: 49.07% | Val Loss: 0.4759, Acc: 33.33%\n",
      "Epoch 50: Train Loss: 0.4466, Acc: 45.37% | Val Loss: 0.4746, Acc: 33.33%\n",
      "Epoch 60: Train Loss: 0.4601, Acc: 46.30% | Val Loss: 0.4732, Acc: 33.33%\n",
      "Epoch 70: Train Loss: 0.4676, Acc: 45.37% | Val Loss: 0.4719, Acc: 33.33%\n",
      "Epoch 80: Train Loss: 0.4767, Acc: 44.44% | Val Loss: 0.4706, Acc: 33.33%\n",
      "Epoch 90: Train Loss: 0.4317, Acc: 48.15% | Val Loss: 0.4693, Acc: 33.33%\n",
      "Epoch 100: Train Loss: 0.4667, Acc: 45.37% | Val Loss: 0.4680, Acc: 33.33%\n",
      "Epoch 110: Train Loss: 0.4559, Acc: 49.07% | Val Loss: 0.4667, Acc: 33.33%\n",
      "Epoch 120: Train Loss: 0.4626, Acc: 41.67% | Val Loss: 0.4654, Acc: 33.33%\n",
      "Epoch 130: Train Loss: 0.4624, Acc: 46.30% | Val Loss: 0.4641, Acc: 33.33%\n",
      "Epoch 140: Train Loss: 0.4393, Acc: 49.07% | Val Loss: 0.4628, Acc: 33.33%\n",
      "Epoch 150: Train Loss: 0.4548, Acc: 43.52% | Val Loss: 0.4615, Acc: 33.33%\n",
      "Epoch 160: Train Loss: 0.4368, Acc: 49.07% | Val Loss: 0.4602, Acc: 33.33%\n",
      "Epoch 170: Train Loss: 0.4278, Acc: 48.15% | Val Loss: 0.4589, Acc: 33.33%\n",
      "Epoch 180: Train Loss: 0.4482, Acc: 46.30% | Val Loss: 0.4576, Acc: 33.33%\n",
      "Epoch 190: Train Loss: 0.4274, Acc: 44.44% | Val Loss: 0.4563, Acc: 33.33%\n",
      "Final Validation Accuracy: 0.3333\n",
      " Fold 5/5 | Validation Accuracy: 0.3333\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5037\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 1e-05, 'l2': 0.0001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.3510, Acc: 46.30% | Val Loss: 0.4643, Acc: 22.22%\n",
      "Epoch 10: Train Loss: 0.3250, Acc: 56.48% | Val Loss: 0.3839, Acc: 48.15%\n",
      "Epoch 20: Train Loss: 0.3020, Acc: 55.56% | Val Loss: 0.3396, Acc: 51.85%\n",
      "Epoch 30: Train Loss: 0.2811, Acc: 60.19% | Val Loss: 0.3106, Acc: 51.85%\n",
      "Epoch 40: Train Loss: 0.2617, Acc: 60.19% | Val Loss: 0.2888, Acc: 55.56%\n",
      "Epoch 50: Train Loss: 0.2454, Acc: 63.89% | Val Loss: 0.2682, Acc: 55.56%\n",
      "Epoch 60: Train Loss: 0.2308, Acc: 64.81% | Val Loss: 0.2469, Acc: 55.56%\n",
      "Epoch 70: Train Loss: 0.2193, Acc: 66.67% | Val Loss: 0.2297, Acc: 59.26%\n",
      "Epoch 80: Train Loss: 0.2098, Acc: 72.22% | Val Loss: 0.2178, Acc: 66.67%\n",
      "Epoch 90: Train Loss: 0.2004, Acc: 73.15% | Val Loss: 0.2121, Acc: 74.07%\n",
      "Epoch 100: Train Loss: 0.1895, Acc: 74.07% | Val Loss: 0.2099, Acc: 70.37%\n",
      "Epoch 110: Train Loss: 0.1784, Acc: 78.70% | Val Loss: 0.2136, Acc: 66.67%\n",
      "Epoch 120: Train Loss: 0.1702, Acc: 82.41% | Val Loss: 0.2153, Acc: 66.67%\n",
      "Early stopping at epoch 128\n",
      "Restoring model weights from epoch 98\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 1/5 | Validation Accuracy: 0.6667\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 1e-05, 'l2': 0.0001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2600, Acc: 54.63% | Val Loss: 0.2756, Acc: 66.67%\n",
      "Epoch 10: Train Loss: 0.2331, Acc: 66.67% | Val Loss: 0.2557, Acc: 70.37%\n",
      "Epoch 20: Train Loss: 0.2172, Acc: 70.37% | Val Loss: 0.2470, Acc: 70.37%\n",
      "Epoch 30: Train Loss: 0.2059, Acc: 71.30% | Val Loss: 0.2433, Acc: 66.67%\n",
      "Epoch 40: Train Loss: 0.1956, Acc: 73.15% | Val Loss: 0.2419, Acc: 62.96%\n",
      "Epoch 50: Train Loss: 0.1860, Acc: 73.15% | Val Loss: 0.2420, Acc: 62.96%\n",
      "Epoch 60: Train Loss: 0.1761, Acc: 77.78% | Val Loss: 0.2428, Acc: 62.96%\n",
      "Epoch 70: Train Loss: 0.1632, Acc: 78.70% | Val Loss: 0.2431, Acc: 74.07%\n",
      "Early stopping at epoch 71\n",
      "Restoring model weights from epoch 41\n",
      "Final Validation Accuracy: 0.7407\n",
      " Fold 2/5 | Validation Accuracy: 0.7407\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 1e-05, 'l2': 0.0001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2410, Acc: 60.19% | Val Loss: 0.2368, Acc: 70.37%\n",
      "Epoch 10: Train Loss: 0.2278, Acc: 65.74% | Val Loss: 0.2245, Acc: 70.37%\n",
      "Epoch 20: Train Loss: 0.2183, Acc: 65.74% | Val Loss: 0.2220, Acc: 70.37%\n",
      "Epoch 30: Train Loss: 0.2098, Acc: 65.74% | Val Loss: 0.2213, Acc: 70.37%\n",
      "Epoch 40: Train Loss: 0.2016, Acc: 66.67% | Val Loss: 0.2216, Acc: 70.37%\n",
      "Epoch 50: Train Loss: 0.1920, Acc: 67.59% | Val Loss: 0.2216, Acc: 70.37%\n",
      "Early stopping at epoch 59\n",
      "Restoring model weights from epoch 29\n",
      "Final Validation Accuracy: 0.6667\n",
      " Fold 3/5 | Validation Accuracy: 0.6667\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 1e-05, 'l2': 0.0001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.4448, Acc: 39.81% | Val Loss: 0.4435, Acc: 55.56%\n",
      "Epoch 10: Train Loss: 0.4098, Acc: 43.52% | Val Loss: 0.4022, Acc: 48.15%\n",
      "Epoch 20: Train Loss: 0.3745, Acc: 47.22% | Val Loss: 0.3862, Acc: 44.44%\n",
      "Epoch 30: Train Loss: 0.3409, Acc: 55.56% | Val Loss: 0.3714, Acc: 40.74%\n",
      "Epoch 40: Train Loss: 0.3123, Acc: 61.11% | Val Loss: 0.3553, Acc: 40.74%\n",
      "Epoch 50: Train Loss: 0.2865, Acc: 64.81% | Val Loss: 0.3387, Acc: 40.74%\n",
      "Epoch 60: Train Loss: 0.2608, Acc: 62.96% | Val Loss: 0.3217, Acc: 37.04%\n",
      "Epoch 70: Train Loss: 0.2308, Acc: 65.74% | Val Loss: 0.3058, Acc: 48.15%\n",
      "Epoch 80: Train Loss: 0.2031, Acc: 67.59% | Val Loss: 0.3004, Acc: 55.56%\n",
      "Epoch 90: Train Loss: 0.1831, Acc: 71.30% | Val Loss: 0.3005, Acc: 59.26%\n",
      "Epoch 100: Train Loss: 0.1689, Acc: 75.93% | Val Loss: 0.2991, Acc: 62.96%\n",
      "Epoch 110: Train Loss: 0.1589, Acc: 76.85% | Val Loss: 0.2968, Acc: 62.96%\n",
      "Epoch 120: Train Loss: 0.1508, Acc: 76.85% | Val Loss: 0.2952, Acc: 62.96%\n",
      "Epoch 130: Train Loss: 0.1434, Acc: 79.63% | Val Loss: 0.2951, Acc: 62.96%\n",
      "Epoch 140: Train Loss: 0.1371, Acc: 82.41% | Val Loss: 0.2957, Acc: 62.96%\n",
      "Final Validation Accuracy: 0.6296\n",
      " Fold 4/5 | Validation Accuracy: 0.6296\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "Data shapes:\n",
      "X_train: (108, 17), y_train: (108,)\n",
      "Hyperparams: {'hidden_size': [4], 'hidden_activation': [<class 'src.activation_functions.Activation_Leaky_ReLU'>], 'batch_norm': [True], 'learning_rate': 0.01, 'l1': 1e-05, 'l2': 0.0001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 150, 'weight_decay': 0.001, 'patience': 30, 'CC': False}\n",
      "Epoch 0: Train Loss: 0.2913, Acc: 51.85% | Val Loss: 0.3257, Acc: 62.96%\n",
      "Epoch 10: Train Loss: 0.2581, Acc: 59.26% | Val Loss: 0.3149, Acc: 62.96%\n",
      "Epoch 20: Train Loss: 0.2353, Acc: 62.96% | Val Loss: 0.2997, Acc: 66.67%\n",
      "Epoch 30: Train Loss: 0.2163, Acc: 65.74% | Val Loss: 0.2886, Acc: 62.96%\n",
      "Epoch 40: Train Loss: 0.2019, Acc: 68.52% | Val Loss: 0.2804, Acc: 70.37%\n",
      "Epoch 50: Train Loss: 0.1889, Acc: 69.44% | Val Loss: 0.2714, Acc: 77.78%\n",
      "Epoch 60: Train Loss: 0.1773, Acc: 69.44% | Val Loss: 0.2638, Acc: 77.78%\n",
      "Epoch 70: Train Loss: 0.1676, Acc: 73.15% | Val Loss: 0.2602, Acc: 70.37%\n",
      "Epoch 80: Train Loss: 0.1585, Acc: 74.07% | Val Loss: 0.2592, Acc: 70.37%\n",
      "Epoch 90: Train Loss: 0.1481, Acc: 75.00% | Val Loss: 0.2593, Acc: 70.37%\n",
      "Epoch 100: Train Loss: 0.1368, Acc: 75.93% | Val Loss: 0.2630, Acc: 74.07%\n",
      "Epoch 110: Train Loss: 0.1248, Acc: 77.78% | Val Loss: 0.2674, Acc: 74.07%\n",
      "Early stopping at epoch 117\n",
      "Restoring model weights from epoch 87\n",
      "Final Validation Accuracy: 0.7407\n",
      " Fold 5/5 | Validation Accuracy: 0.7407\n",
      "\n",
      " Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.6889\n",
      "{'hidden_size': [4], 'hidden_activation': ['Activation_Leaky_ReLU'], 'batch_norm': [True], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 50, 'CC': False, 'val_accuracy': np.float64(0.8666666666666668)}\n"
     ]
    }
   ],
   "source": [
    "best_hyperparams, best_performance = random_search(X_train=X_train, y_train=y_train, param_distributions=param_distributions, n_iters=30)  # adjust n_iters as needed\n",
    "\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_size': [4], 'hidden_activation': ['Activation_Leaky_ReLU'], 'batch_norm': [True], 'learning_rate': 0.1, 'l1': 0.0, 'l2': 0.0001, 'dropout_rate': 0.0, 'batch_size': 1000, 'n_epochs': 200, 'weight_decay': 0.01, 'patience': 50, 'CC': False, 'val_accuracy': np.float64(0.8666666666666668)}\n"
     ]
    }
   ],
   "source": [
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "    'Activation_ReLU': Activation_ReLU,\n",
    "    'Activation_Tanh': Activation_Tanh,\n",
    "    'Activation_ELU': Activation_ELU,\n",
    "    'Activation_Leaky_ReLU': Activation_Leaky_ReLU,\n",
    "    'Activation_Sigmoid': Activation_Sigmoid\n",
    "}\n",
    "\n",
    "hidden_activation = [activation_map[act] for act in best_hyperparams['hidden_activation']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n"
     ]
    }
   ],
   "source": [
    "if best_hyperparams['CC']:\n",
    "    model = CascadeCorrelation(input_size = 17, output_size= 1, activation=Activation_Leaky_ReLU, output_activation = Activation_Sigmoid)\n",
    "else:\n",
    "    model = NN(\n",
    "        l1=best_hyperparams['l1'],\n",
    "        l2=best_hyperparams['l2'],\n",
    "        input_size=17,\n",
    "        hidden_sizes=best_hyperparams['hidden_size'],\n",
    "        output_size=1,\n",
    "        hidden_activations=hidden_activation,\n",
    "        dropout_rates=[best_hyperparams['dropout_rate']],\n",
    "        use_batch_norm=best_hyperparams['batch_norm']\n",
    "    )\n",
    "batch_size = best_hyperparams['batch_size']\n",
    "learning_rate = best_hyperparams['learning_rate']\n",
    "n_epochs = best_hyperparams['n_epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleNN:\n",
    "    def __init__(self, n_models=5):\n",
    "        self.models = []\n",
    "        self.n_models = n_models\n",
    "        self.loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def create_and_train_models(self, hyperparams, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Create and train multiple models with diverse configurations\"\"\"\n",
    "\n",
    "        # Create diverse configurations for better ensemble performance\n",
    "        base_configs = [\n",
    "            {**hyperparams, 'hidden_sizes': [64,\n",
    "                                             32], 'dropout_rates': [0.2, 0.3]},\n",
    "            {**hyperparams, 'hidden_sizes': [128,\n",
    "                                             64], 'dropout_rates': [0.3, 0.4]},\n",
    "            {**hyperparams, 'hidden_sizes': [32,\n",
    "                                             16], 'dropout_rates': [0.4, 0.2]},\n",
    "            {**hyperparams,\n",
    "                'learning_rate': hyperparams['learning_rate'] * 0.8},\n",
    "            {**hyperparams,\n",
    "                'learning_rate': hyperparams['learning_rate'] * 1.2},\n",
    "        ]\n",
    "\n",
    "        for i in range(self.n_models):\n",
    "            # Use different config for each model to add diversity\n",
    "            current_config = base_configs[i % len(base_configs)]\n",
    "\n",
    "            # Create model with diverse architecture\n",
    "            model = NN(\n",
    "                l1=current_config.get('l1', 0.0),\n",
    "                l2=current_config.get('l2', 0.0),\n",
    "                input_size=17,\n",
    "                hidden_sizes=current_config.get(\n",
    "                    'hidden_sizes', hyperparams.get('hidden_sizes', [64])),\n",
    "                output_size=1,\n",
    "                hidden_activations=current_config.get(\n",
    "                    'hidden_activation', hyperparams.get('hidden_activation')),\n",
    "                dropout_rates=current_config.get(\n",
    "                    'dropout_rates', [hyperparams.get('dropout_rate', 0.0)]),\n",
    "                use_batch_norm=current_config.get(\n",
    "                    'use_batch_norm', hyperparams.get('use_batch_norm', False))\n",
    "            )\n",
    "\n",
    "            print(f\"Training model {i+1}/{self.n_models}\")\n",
    "\n",
    "            # Create trainer instance\n",
    "            trainer = Train(hyperparameters=current_config, model=model)\n",
    "\n",
    "            # Add bootstrap sampling for data diversity\n",
    "            n_samples = len(X_train)\n",
    "            bootstrap_indices = np.random.choice(\n",
    "                n_samples, n_samples, replace=True)\n",
    "            X_train_boot = X_train.iloc[bootstrap_indices] if hasattr(\n",
    "                X_train, 'iloc') else X_train[bootstrap_indices]\n",
    "            y_train_boot = y_train.iloc[bootstrap_indices] if hasattr(\n",
    "                y_train, 'iloc') else y_train[bootstrap_indices]\n",
    "\n",
    "            # Train model\n",
    "            trained_model, val_accuracy = trainer.train_and_evaluate(\n",
    "                X_train_boot, y_train_boot, X_val, y_val\n",
    "            )\n",
    "\n",
    "            self.models.append(trained_model)\n",
    "            print(f\"Model {i+1} validation accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using majority voting\"\"\"\n",
    "        if not self.models:\n",
    "            raise ValueError(\n",
    "                \"No trained models found. Call create_and_train_models first.\")\n",
    "\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            model.forward(X, training=False)\n",
    "\n",
    "            # For binary classification with sigmoid output\n",
    "            if model.output.shape[1] == 1:\n",
    "                pred = (model.output.squeeze() > 0.5).astype(int)\n",
    "            else:\n",
    "                # For multi-class with softmax\n",
    "                pred = np.argmax(model.output, axis=1)\n",
    "\n",
    "            predictions.append(pred)\n",
    "\n",
    "        # Majority voting\n",
    "        predictions = np.array(predictions)\n",
    "        final_predictions = np.apply_along_axis(\n",
    "            lambda x: np.bincount(x).argmax(),\n",
    "            axis=0,\n",
    "            arr=predictions\n",
    "        )\n",
    "        return final_predictions\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities from all models\"\"\"\n",
    "        if not self.models:\n",
    "            raise ValueError(\n",
    "                \"No trained models found. Call create_and_train_models first.\")\n",
    "\n",
    "        all_probas = []\n",
    "        for model in self.models:\n",
    "            model.forward(X, training=False)\n",
    "            all_probas.append(model.output.copy())\n",
    "\n",
    "        # Average probabilities\n",
    "        avg_probas = np.mean(all_probas, axis=0)\n",
    "        return avg_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 32] ['Activation_Leaky_ReLU'] [0.2, 0.3] [False, False]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m ensemble = EnsembleNN(n_models=\u001b[32m5\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Pass the data to the method\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mensemble\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_and_train_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_hyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[32m     23\u001b[39m test_predictions = ensemble.predict(X_test)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mEnsembleNN.create_and_train_models\u001b[39m\u001b[34m(self, hyperparams, X_train, y_train, X_val, y_val)\u001b[39m\n\u001b[32m     26\u001b[39m current_config = base_configs[i % \u001b[38;5;28mlen\u001b[39m(base_configs)]\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Create model with diverse architecture\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m model = \u001b[43mNN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43ml1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ml1\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43ml2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43ml2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m17\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhidden_sizes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhidden_sizes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_activations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhidden_activation\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_activation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_rates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdropout_rates\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdropout_rate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_batch_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muse_batch_norm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muse_batch_norm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_models\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Create trainer instance\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:17\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, l1, l2, input_size, hidden_sizes, output_size, hidden_activations, dropout_rates, use_batch_norm, output_activation)\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "# ensemble = EnsembleNN(n_models=5)\n",
    "\n",
    "# ensemble.create_and_train_models(best_hyperparams)\n",
    "\n",
    "# test_predictions = ensemble.predict(X_test)\n",
    "# test_accuracy = np.mean(test_predictions == y_test)\n",
    "\n",
    "# print(f\"Ensemble Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Usage example\n",
    "ensemble = EnsembleNN(n_models=5)\n",
    "\n",
    "# Pass the data to the method\n",
    "ensemble.create_and_train_models(\n",
    "    hyperparams=best_hyperparams,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "test_predictions = ensemble.predict(X_test)\n",
    "test_accuracy = np.mean(test_predictions == y_test)\n",
    "print(f\"Ensemble Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Optional: Get prediction probabilities\n",
    "test_probas = ensemble.predict_proba(X_test)\n",
    "print(\n",
    "    f\"Average prediction confidence: {np.mean(np.max(test_probas, axis=1)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:\n",
      "X_train: (135, 17), y_train: (135,)\n",
      "Sample prediction: [[0.64252104]]\n",
      "Initial loss: 0.4128332908085566\n",
      "Epoch 0: Train Loss: 0.2960, Acc: 51.11% | Val Loss: 0.2885, Acc: 52.94%\n",
      "Epoch 10: Train Loss: 0.2170, Acc: 66.67% | Val Loss: 0.2837, Acc: 44.12%\n",
      "Epoch 20: Train Loss: 0.2081, Acc: 66.67% | Val Loss: 0.2992, Acc: 44.12%\n",
      "Epoch 30: Train Loss: 0.1758, Acc: 63.70% | Val Loss: 0.2912, Acc: 55.88%\n",
      "Epoch 40: Train Loss: 0.1220, Acc: 86.67% | Val Loss: 0.3154, Acc: 79.41%\n",
      "Epoch 50: Train Loss: 0.0391, Acc: 100.00% | Val Loss: 0.3478, Acc: 100.00%\n",
      "Epoch 60: Train Loss: 0.0091, Acc: 100.00% | Val Loss: 0.4060, Acc: 100.00%\n",
      "Epoch 70: Train Loss: 0.0072, Acc: 100.00% | Val Loss: 0.4110, Acc: 100.00%\n",
      "Epoch 80: Train Loss: 0.0105, Acc: 100.00% | Val Loss: 0.3985, Acc: 100.00%\n",
      "Epoch 90: Train Loss: 0.0084, Acc: 100.00% | Val Loss: 0.4068, Acc: 100.00%\n",
      "Epoch 100: Train Loss: 0.0072, Acc: 100.00% | Val Loss: 0.4154, Acc: 100.00%\n",
      "Epoch 110: Train Loss: 0.0067, Acc: 100.00% | Val Loss: 0.4147, Acc: 100.00%\n",
      "Early stopping at epoch 111\n",
      "Restoring model weights from epoch 11\n",
      "Final Validation Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "loss_function = MSE()\n",
    "optimizer = Optimizer_Adam(learning_rate=learning_rate, decay=best_hyperparams['weight_decay'])\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(patience=100, min_delta_loss=1e-5, min_delta_accuracy=0.001)\n",
    "\n",
    "# Before training loop:\n",
    "print(\"Data shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Sample prediction: {model.forward(X_train[:1])}\")\n",
    "print(f\"Initial loss: {loss_function.forward(model.output, y_train[:1])}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    batch_losses = []\n",
    "    batch_accuracies = []\n",
    "\n",
    "    for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "        # Forward pass\n",
    "        model.forward(X_batch, training=True)\n",
    "\n",
    "        # Loss and accuracy\n",
    "        loss = loss_function.forward(model.output, y_batch)\n",
    "        predictions = np.round(model.output.squeeze())\n",
    "        accuracy = np.mean(predictions == y_batch.squeeze())\n",
    "\n",
    "        # Backward pass\n",
    "        loss_function.backward(model.output, y_batch)\n",
    "        dvalues = loss_function.dinputs\n",
    "\n",
    "        assert dvalues.shape == model.output.shape, \\\n",
    "            f\"Gradient shape mismatch: {dvalues.shape} vs {model.output.shape}\"\n",
    "\n",
    "        for layer in reversed(model.layers):\n",
    "            layer.backward(dvalues)\n",
    "            dvalues = layer.dinputs\n",
    "\n",
    "            # Regularization\n",
    "            if isinstance(layer, Layer_Dense):\n",
    "                if layer.l1 > 0:\n",
    "                    layer.dweights += layer.l1 * np.sign(layer.weights)\n",
    "                if layer.l2 > 0:\n",
    "                    layer.dweights += 2 * layer.l2 * layer.weights\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.pre_update_params()\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, Layer_Dense):\n",
    "                optimizer.update_params(layer)\n",
    "        optimizer.post_update_params()\n",
    "\n",
    "        batch_losses.append(loss)\n",
    "        batch_accuracies.append(accuracy)\n",
    "\n",
    "    # Epoch summary\n",
    "    epoch_loss = np.mean(batch_losses)\n",
    "    epoch_acc = np.mean(batch_accuracies)\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "\n",
    "    # Validation\n",
    "    X_val_input = X_val.values if isinstance(X_val, pd.DataFrame) else X_val\n",
    "    y_val_input = y_val.values if isinstance(y_val, (pd.Series, pd.DataFrame)) else y_val\n",
    "\n",
    "    model.forward(X_val_input, training=False)\n",
    "    val_loss = loss_function.forward(model.output, y_val_input)\n",
    "    val_predictions = np.round(model.output.squeeze())\n",
    "    val_accuracy = np.mean(val_predictions == y_val.squeeze())\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: \", end=\"\")\n",
    "        print(f\"Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Acc: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "    # Early stopping check\n",
    "    early_stopping.on_epoch_end(\n",
    "        current_loss=val_loss,\n",
    "        current_accuracy=val_accuracy,\n",
    "        model=model,\n",
    "        epoch=epoch\n",
    "    )\n",
    "\n",
    "    if early_stopping.stop_training:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "# Restore best weights\n",
    "if early_stopping.stop_training:\n",
    "    print(f\"Restoring model weights from epoch {early_stopping.best_epoch}\")\n",
    "    early_stopping.restore_weights(model)\n",
    "\n",
    "# Final evaluation\n",
    "model.forward(X_val_input, training=False)\n",
    "final_val_loss = loss_function.forward(model.output, y_val_input)\n",
    "final_val_accuracy = np.mean(np.round(model.output.squeeze()) == y_val.squeeze())\n",
    "print(f\"Final Validation Accuracy: {final_val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Test set evaluation\n",
    "model.forward(X_test, training=False)\n",
    "test_loss = loss_function.forward(model.output.squeeze(), y_test)\n",
    "\n",
    "predictions = np.round(model.output.squeeze())\n",
    "y_true = np.argmax(y_test, axis=1) if y_test.ndim > 1 else y_test\n",
    "test_accuracy = np.mean(predictions == y_true)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plot training progress\n",
    "plot_accuracies(train_accuracies, val_accuracies, test_accuracy,\n",
    "                label1=\"Training Accuracies\", label2=\"Validation Accuracies\",\n",
    "                title=\"Accuracy Over Epochs\")\n",
    "\n",
    "plot_losses(train_losses, val_losses, test_loss,\n",
    "            label1=\"Training Loss\", label2=\"Validation Loss\",\n",
    "            title=\"Loss Over Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29600771049822716, 0.25736314139376304, 0.24062161199299553, 0.23310743097198866, 0.2284371865986754, 0.2253574937478637, 0.22294384542137258, 0.2205165302809892, 0.21828049729513066, 0.2169839563350389, 0.21695676533635183, 0.21785278613478976, 0.21885262504485184, 0.21911474402034997, 0.21836965793990182, 0.21704306488489558, 0.21553979296785586, 0.21385926228608548, 0.2118741849287852, 0.20993992598717065, 0.20810168942308419, 0.20590784460990666, 0.20326005727206678, 0.20036442316546424, 0.19730637042044297, 0.19401190103252045, 0.1905582812346805, 0.18704094762671458, 0.18341912196375926, 0.1796149789540949, 0.1757960046391666, 0.17166196147126972, 0.1669234451639423, 0.16193577019091937, 0.15650021416594884, 0.15079595727259168, 0.1450662846555766, 0.13943468361520445, 0.13401596763373388, 0.12832983679693508, 0.12198483339217268, 0.11471188542325189, 0.10688128605238971, 0.09906022820722615, 0.09058294613029975, 0.08153434902686038, 0.07212594991109327, 0.06353650916084477, 0.05506463976732961, 0.04690933871982451, 0.039095464373483754, 0.03247883539357074, 0.02729174565198217, 0.023368109448076976, 0.020053124491503246, 0.017198432829048673, 0.014689222444705797, 0.012578010576605266, 0.010955854233429712, 0.00980710362143868, 0.009108788572404618, 0.0086691737668195, 0.008390437342606385, 0.008323246095056405, 0.008223507020663596, 0.007910112955558848, 0.007714212342966672, 0.0075745476352522545, 0.007295012193479729, 0.007214918390716174, 0.007237628130776604, 0.00748761599783392, 0.007989792239632035, 0.008107052308580564, 0.008524968025716688, 0.008764193420689273, 0.009232309650504335, 0.009626504226185337, 0.01016176229572426, 0.010368855986496946, 0.01053601016173618, 0.010394312893229618, 0.01031917326640404, 0.010277685449231957, 0.010171155845540786, 0.00993145767227011, 0.009804282644197639, 0.00951392329323609, 0.009113134953503757, 0.008859472119566985, 0.008414059761151105, 0.008026087070723274, 0.00781429871785813, 0.00760699997355865, 0.0074822458318815396, 0.007415307519299546, 0.007158957192914605, 0.007156247573569215, 0.007176944440701861, 0.007084209643780424, 0.007160322129610134, 0.007138235154941493, 0.00701934969752985, 0.007032759824274636, 0.007082753042322397, 0.007059246415300907, 0.00699358379780509, 0.007098655312168392, 0.007087949665712238, 0.006976443464919014, 0.006689134066173069, 0.006628285935996397] [0.288534306696523, 0.3040617285111405, 0.3178039806933289, 0.32560186743175323, 0.3277844770676422, 0.32501288048721083, 0.31807445819058705, 0.30863390186231926, 0.29859983520980454, 0.28985624471493465, 0.2836563933655452, 0.28078996843269344, 0.2810207550015277, 0.28417513302023756, 0.2899006300560235, 0.2968582634293874, 0.3022384062327573, 0.30337042143489956, 0.3013722085744697, 0.2992985286158778, 0.299151910380159, 0.30075329949813395, 0.30269656023366537, 0.3030880036484514, 0.30074954491289124, 0.2965928656978335, 0.2924899569668723, 0.29027148342806786, 0.29051576042912564, 0.29176741560482594, 0.29121470628942797, 0.2890367855601941, 0.28799792446141786, 0.29044690976291954, 0.29549090643451736, 0.30065168032937595, 0.3048668559154565, 0.3088178757898023, 0.3129340215381152, 0.31585331292267016, 0.3154002989541712, 0.311934462393779, 0.3086318033263439, 0.30796388646903944, 0.30814938353053795, 0.3083844816981648, 0.31189020341494994, 0.318988678115585, 0.32797878259960894, 0.3381675375673381, 0.3478154385119048, 0.35637145713840557, 0.3647906425297798, 0.3734474951604414, 0.38128976837480233, 0.38781993344030363, 0.39319079164509274, 0.3976181363346451, 0.4012192528826716, 0.4039440640535855, 0.4060055542676179, 0.40748806663428583, 0.4081077975433317, 0.40864847596643206, 0.40972161387132666, 0.4106842957948246, 0.41097097457578846, 0.4109552013856185, 0.41063955114640055, 0.41075963942630156, 0.4109749434957188, 0.41029409691857854, 0.40863842097346587, 0.40578289261476796, 0.40574544581810545, 0.4067325739843329, 0.4050700033249691, 0.40129357698521667, 0.4004835599141749, 0.40048143118209195, 0.398513465573107, 0.39833559330347657, 0.3987862869573873, 0.39661471192284814, 0.39575470288061276, 0.39551892879700007, 0.3949574802572684, 0.3978860683866036, 0.40171376902232026, 0.40345161967309695, 0.4068282633075225, 0.4096568323341366, 0.4106889533282997, 0.41298090052159736, 0.41443150918415783, 0.41314587605200964, 0.41269991827926733, 0.41450679233338483, 0.4142398994130065, 0.41447341720342457, 0.4154309631500926, 0.4144819196453786, 0.4146310918476856, 0.41566579221596045, 0.41368020349090057, 0.4139793920507941, 0.4141900706992728, 0.4115411225191375, 0.41500121136244295, 0.41396708174902985, 0.4147054948634791, 0.41766193796215006]\n"
     ]
    }
   ],
   "source": [
    "print(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model.forward(X_test, training=False)\n",
    "# Compute softmax probabilities for the test output\n",
    "# print(X_test.shape, y_test.shape)\n",
    "# print(model.output, y_test)\n",
    "loss_function.forward(model.output.squeeze(), y_test)\n",
    "# Calculate accuracy for the test set\n",
    "predictions = np.round(model.output.squeeze())\n",
    "if len(y_test.shape) == 2:\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    y_true = y_test\n",
    "\n",
    "# Compute test accuracy\n",
    "test_accuracy = np.mean(predictions == y_true)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EnsembleNN:\n",
    "#     def __init__(self, n_models=5):\n",
    "#         self.models = []\n",
    "#         self.n_models = n_models\n",
    "#         self.loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "        \n",
    "            \n",
    "\n",
    "#     def create_and_train_models(self, hyperparams):\n",
    "#         # Create and train multiple models with the same hyperparameters\n",
    "#         for i in range(self.n_models):\n",
    "#             model = NN(\n",
    "#                 l1=l1,\n",
    "#                 l2=l2,\n",
    "#                 input_size=17,\n",
    "#                 hidden_sizes=hidden_size,\n",
    "#                 output_size=1,\n",
    "#                 hidden_activations=hidden_activation,\n",
    "#                 dropout_rates=[dropout_rate],\n",
    "#                 use_batch_norm=use_batch_norm\n",
    "#             )\n",
    "#             print(f\"Training model {i+1}/{self.n_models}\")\n",
    "#             # Train model using existing train_and_evaluate function\n",
    "#             model, val_accuracy = train_and_evaluate(\n",
    "#                 learning_rate=hyperparams['learning_rate'],\n",
    "#                 # l1=hyperparams['l1'],\n",
    "#                 # l2=hyperparams['l2'],\n",
    "#                 # dropout_rate=hyperparams['dropout_rate'],\n",
    "#                 batch_size=hyperparams['batch_size'],\n",
    "#                 n_epochs=hyperparams['n_epochs'],\n",
    "#                 weight_decay=hyperparams['weight_decay'],\n",
    "#                 # model=hyperparams['model']\n",
    "#                 # activation=hyperparams['activation']\n",
    "#                 X_train=X_train,\n",
    "#                 y_train=y_train,\n",
    "#                 X_val=X_val,\n",
    "#                 y_val=y_val,\n",
    "#                 model=model,\n",
    "#             )\n",
    "#             self.models.append(model)\n",
    "#             print(f\"Model {i+1} validation accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"Make predictions using majority voting\"\"\"\n",
    "#         predictions = []\n",
    "#         for model in self.models:\n",
    "#             model.forward(X, training=False)\n",
    "#             self.loss_activation.forward(\n",
    "#                 model.output, np.zeros((X.shape[0], 2)))  # Dummy y values\n",
    "#             pred = np.argmax(self.loss_activation.output, axis=1)\n",
    "#             predictions.append(pred)\n",
    "\n",
    "#         # Majority voting\n",
    "#         predictions = np.array(predictions)\n",
    "#         final_predictions = np.apply_along_axis(\n",
    "#             lambda x: np.bincount(x).argmax(),\n",
    "#             axis=0,\n",
    "#             arr=predictions\n",
    "#         )\n",
    "#         return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble = EnsembleNN(n_models=5)\n",
    "\n",
    "# ensemble.create_and_train_models(best_hyperparams)\n",
    "\n",
    "# test_predictions = ensemble.predict(X_test)\n",
    "# test_accuracy = np.mean(test_predictions == y_test)\n",
    "\n",
    "# print(f\"Ensemble Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
