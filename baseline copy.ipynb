{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import csv\n",
    "import itertools\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from loss_functions import *\n",
    "from src.activation_functions import *\n",
    "from src.batch_normalization import *\n",
    "from src.data_preprocessing import *\n",
    "from src.ensemble.cascade_correlation import CascadeCorrelation\n",
    "from src.k_fold_cross_validation import *\n",
    "from src.layer import *\n",
    "from src.early_stopping import EarlyStopping\n",
    "from src.neural_network import *\n",
    "from src.optimizers import *\n",
    "from src.random_search import *\n",
    "from src.train_and_evaluate import Train\n",
    "from src.utils import *\n",
    "\n",
    "# from src.random_search import *\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### Data pre-processing for MONK Datasets  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_data(MONK_NUM=1)\n",
    "X_test, y_test = load_data(MONK_NUM=1, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42)\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_val = np.asarray(X_val)\n",
    "y_val = np.asarray(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))\n",
    "print(\"Training set overlap with validation set:\",\n",
    "      np.intersect1d(X_train, X_val).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the resulting datasets\n",
    "for _ in [X_train, X_val, y_train, y_val]:\n",
    "    print(f\"the shape: \", _.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'hidden_size': [[3], [4], [5], [6], [8]],\n",
    "    'hidden_activation': [[Activation_Tanh], [Activation_Leaky_ReLU], [Activation_Sigmoid], [Activation_ReLU]],\n",
    "    'batch_norm': [[True], [False]],\n",
    "    'learning_rate': [1e-2, 1e-4, 1e-3, 2e-4,1e-6, 1e-5],\n",
    "    'l1': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'l2': [1e-6, 1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'dropout_rate': [0.0, 0.1, 0.3],\n",
    "    'batch_size': [8, 16, 32],\n",
    "    'n_epochs': [100, 150, 200, 250, 300],\n",
    "    'weight_decay': [0, 5e-2, 1e-2, 1e-3, 1e-4, 1e-5],\n",
    "    'patience': [0, 20, 30, 50],\n",
    "    'CC': [False],\n",
    "    'weights_init': ['gaussian', 'gaussian_scaled', 'xavier', 'he', 'random'],\n",
    "    # # Define combinations of hidden layer sizes and corresponding activations\n",
    "    # 'hidden_configs': [\n",
    "    #     {'hidden_size': [10], 'hidden_activation': [Activation_Tanh], 'batch_norm' : [True]},\n",
    "    #     {'hidden_size': [10, 10], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, True]},\n",
    "    #     {'hidden_size': [10, 10], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, True]},\n",
    "    #     {'hidden_size': [20, 20], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, False]},\n",
    "    #     {'hidden_size': [20, 20], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, True]},\n",
    "    #     {'hidden_size': [20, 20], 'hidden_activation': [Activation_ELU, Activation_ELU], 'batch_norm' : [True, False]},\n",
    "    #     {'hidden_size': [10, 10], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, False]},\n",
    "    #     {'hidden_size': [32, 16], 'hidden_activation': [Activation_Leaky_ReLU, Activation_Sigmoid], 'batch_norm' : [False, True]},  \n",
    "    #     {'hidden_size': [30, 30], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, False]},\n",
    "    #     {'hidden_size': [64, 32, 16], 'hidden_activation': [Activation_ReLU, Activation_ReLU, Activation_Tanh], 'batch_norm' : [True, False, True]},\n",
    "    #     {'hidden_size': [30, 30], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm': [True, False]}\n",
    "    # ]   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "Create a seperate best_results csv file for each MONK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams, best_performance = random_search(X_train=X_train, y_train=y_train, param_distributions=param_distributions, n_iters=32)  # adjust n_iters as needed\n",
    "\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "    'Activation_ReLU': Activation_ReLU,\n",
    "    'Activation_Tanh': Activation_Tanh,\n",
    "    'Activation_ELU': Activation_ELU,\n",
    "    'Activation_Leaky_ReLU': Activation_Leaky_ReLU,\n",
    "    'Activation_Sigmoid': Activation_Sigmoid\n",
    "}\n",
    "\n",
    "hidden_activation = [activation_map[act] for act in best_hyperparams['hidden_activation']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_hyperparams['CC']:\n",
    "    model = CascadeCorrelation(input_size = 17, output_size= 1, activation=Activation_Leaky_ReLU, output_activation = Activation_Sigmoid)\n",
    "else:\n",
    "    model = NN(\n",
    "        l1=best_hyperparams['l1'],\n",
    "        l2=best_hyperparams['l2'],\n",
    "        input_size=17,\n",
    "        hidden_sizes=best_hyperparams['hidden_size'],\n",
    "        output_size=1,\n",
    "        hidden_activations=hidden_activation,\n",
    "        dropout_rates=[best_hyperparams['dropout_rate']],\n",
    "        use_batch_norm=best_hyperparams['batch_norm']\n",
    "    )\n",
    "batch_size = best_hyperparams['batch_size']\n",
    "learning_rate = best_hyperparams['learning_rate']\n",
    "n_epochs = best_hyperparams['n_epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "loss_function = MSE()\n",
    "optimizer = Optimizer_Adam(learning_rate=learning_rate, decay=best_hyperparams['weight_decay'])\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(patience=30, min_delta_loss=1e-5, min_delta_accuracy=0.001)\n",
    "\n",
    "# Before training loop:\n",
    "print(\"Data shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Hyperparams: {best_hyperparams}\")\n",
    "# print(f\"Sample prediction: {model.forward(X_train[:1])}\")\n",
    "# print(f\"Initial loss: {loss_function.forward(model.output, y_train[:1])}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    batch_losses = []\n",
    "    batch_accuracies = []\n",
    "\n",
    "    for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "        # Forward pass\n",
    "        model.forward(X_batch, training=True)\n",
    "\n",
    "        # Loss and accuracy\n",
    "        loss = loss_function.forward(model.output, y_batch)\n",
    "        predictions = np.round(model.output.squeeze())\n",
    "        accuracy = np.mean(predictions == y_batch.squeeze())\n",
    "\n",
    "        # Backward pass\n",
    "        loss_function.backward(model.output, y_batch)\n",
    "        dvalues = loss_function.dinputs\n",
    "\n",
    "        assert dvalues.shape == model.output.shape, \\\n",
    "            f\"Gradient shape mismatch: {dvalues.shape} vs {model.output.shape}\"\n",
    "        \n",
    "        i = 0\n",
    "        for layer in reversed(model.layers):\n",
    "            i=-1\n",
    "            layer.backward(dvalues)\n",
    "            dvalues = np.array(layer.dinputs)\n",
    "\n",
    "            # Regularization\n",
    "            if isinstance(layer, Layer_Dense):\n",
    "                if layer.l1 > 0:\n",
    "                    layer.dweights += layer.l1 * np.sign(layer.weights)\n",
    "                if layer.l2 > 0:\n",
    "                    layer.dweights += 2 * layer.l2 * layer.weights\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.pre_update_params()\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, Layer_Dense):\n",
    "                optimizer.update_params(layer)\n",
    "        optimizer.post_update_params()\n",
    "\n",
    "        batch_losses.append(loss)\n",
    "        batch_accuracies.append(accuracy)\n",
    "\n",
    "    # Epoch summary\n",
    "    epoch_loss = np.mean(batch_losses)\n",
    "    epoch_acc = np.mean(batch_accuracies)\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "\n",
    "    # Validation\n",
    "    X_val_input = X_val.values if isinstance(X_val, pd.DataFrame) else X_val\n",
    "    y_val_input = y_val.values if isinstance(y_val, (pd.Series, pd.DataFrame)) else y_val\n",
    "\n",
    "    model.forward(X_val_input, training=False)\n",
    "    val_loss = loss_function.forward(model.output, y_val_input)\n",
    "    val_predictions = np.round(model.output.squeeze())\n",
    "    val_accuracy = np.mean(val_predictions == y_val.squeeze())\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: \", end=\"\")\n",
    "        print(f\"Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Acc: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "    # Early stopping check\n",
    "    early_stopping.on_epoch_end(\n",
    "        current_loss=val_loss,\n",
    "        current_accuracy=val_accuracy,\n",
    "        model=model,\n",
    "        epoch=epoch\n",
    "    )\n",
    "\n",
    "    if early_stopping.stop_training:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        # Restore best weights\n",
    "        print(f\"Restoring model weights from epoch {early_stopping.best_epoch}\")\n",
    "        early_stopping.restore_weights(model)\n",
    "        # Cascade correlation\n",
    "        if isinstance(model, CascadeCorrelation):\n",
    "            if model.is_limit_reached():\n",
    "                break\n",
    "            \n",
    "            model.add_neuron()\n",
    "            early_stopping.wait = 0\n",
    "            early_stopping.patience -= int(early_stopping.patience / 10)\n",
    "            early_stopping.stop_training = False\n",
    "            print(f\"Added new neuron at epoch {epoch} wiht val_loss {val_losses[-1]:.4f}\")\n",
    "            continue\n",
    "        break\n",
    "\n",
    "# Final evaluation\n",
    "model.forward(X_val_input, training=False)\n",
    "final_val_loss = loss_function.forward(model.output, y_val_input)\n",
    "final_val_accuracy = np.mean(np.round(model.output.squeeze()) == y_val.squeeze())\n",
    "print(f\"Final Validation Accuracy: {final_val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Test set evaluation\n",
    "model.forward(X_test, training=False)\n",
    "test_loss = loss_function.forward(model.output.squeeze(), y_test)\n",
    "\n",
    "predictions = np.round(model.output.squeeze())\n",
    "y_true = np.argmax(y_test, axis=1) if y_test.ndim > 1 else y_test\n",
    "test_accuracy = np.mean(predictions == y_true)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plot training progress\n",
    "plot_losses(train_losses, val_losses, test_loss,\n",
    "            label1=\"Training Loss\", label2=\"Validation Loss\",\n",
    "            title=\"Loss Over Epochs\")\n",
    "\n",
    "plot_accuracies(train_accuracies, val_accuracies, test_accuracy,\n",
    "                label1=\"Training Accuracies\", label2=\"Validation Accuracies\",\n",
    "                title=\"Accuracy Over Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(X_test, training=False)\n",
    "# Compute softmax probabilities for the test output\n",
    "# print(X_test.shape, y_test.shape)\n",
    "# print(model.output, y_test)\n",
    "loss_function.forward(model.output.squeeze(), y_test)\n",
    "# Calculate accuracy for the test set\n",
    "predictions = np.round(model.output.squeeze())\n",
    "if len(y_test.shape) == 2:\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    y_true = y_test\n",
    "\n",
    "# Compute test accuracy\n",
    "test_accuracy = np.mean(predictions == y_true)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleNN:\n",
    "    def __init__(self, n_models=5):\n",
    "        self.models = []\n",
    "        self.n_models = n_models\n",
    "        self.loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "        \n",
    "\n",
    "    def create_and_train_models(self, hyperparams):\n",
    "        # Create and train multiple models with the same hyperparameters\n",
    "        for i in range(self.n_models):\n",
    "            model = NN(\n",
    "                l1=hyperparams['l1'],\n",
    "                l2=hyperparams['l2'],\n",
    "                input_size=17,\n",
    "                hidden_sizes=hyperparams['hidden_size'],\n",
    "                output_size=1,\n",
    "                hidden_activations=hidden_activation,\n",
    "                dropout_rates=[hyperparams['dropout_rate']],\n",
    "                use_batch_norm=hyperparams['batch_norm']\n",
    "            )\n",
    "            print(f\"Training model {i+1}/{self.n_models}\")\n",
    "            # Train model using existing train_and_evaluate function\n",
    "            self.train = Train(hyperparams, model)\n",
    "            model, val_accuracy = self.train.train_and_evaluate(\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_val=X_val,\n",
    "                y_val=y_val,\n",
    "            )\n",
    "            self.models.append(model)\n",
    "            print(f\"Model {i+1} validation accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions using majority voting\"\"\"\n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            model.forward(X, training=False)\n",
    "            self.loss_activation.forward(\n",
    "                model.output, np.zeros((X.shape[0], 2)))  # Dummy y values\n",
    "            pred = np.argmax(self.loss_activation.output, axis=1)\n",
    "            predictions.append(pred)\n",
    "\n",
    "        # Majority voting\n",
    "        predictions = np.array(predictions)\n",
    "        final_predictions = np.apply_along_axis(\n",
    "            lambda x: np.bincount(x).argmax(),\n",
    "            axis=0,\n",
    "            arr=predictions\n",
    "        )\n",
    "        return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = EnsembleNN(n_models=5)\n",
    "\n",
    "ensemble.create_and_train_models(best_hyperparams)\n",
    "\n",
    "_ , test_accuracy = ensemble.train.test(X_test, y_test)\n",
    "\n",
    "print(f\"Ensemble Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "ensemble.train.plot(accuracy=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
