{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Base_NN' from partially initialized module 'src.neural_network' (most likely due to a circular import) (/home/piri/Documenti/uni/Pisa/2024/ML/ML_project/src/neural_network.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_normalization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_preprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcascade_correlation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CascadeCorrelation\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mk_fold_cross_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Documenti/uni/Pisa/2024/ML/ML_project/src/ensemble/cascade_correlation.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Layer_Dense\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_network\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Base_NN\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCascadeCorrelation\u001b[39;00m(Base_NN):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, output_size, activation, output_activation, max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n",
      "File \u001b[0;32m~/Documenti/uni/Pisa/2024/ML/ML_project/src/neural_network.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_preprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_regularization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dropout\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Documenti/uni/Pisa/2024/ML/ML_project/src/model_regularization.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mactivation_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Activation\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcascade_correlation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CascadeCorrelation\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mEarlyStopping\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, min_delta_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, min_delta_accuracy\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n",
      "File \u001b[0;32m~/Documenti/uni/Pisa/2024/ML/ML_project/src/ensemble/cascade_correlation.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Layer_Dense\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneural_network\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Base_NN\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCascadeCorrelation\u001b[39;00m(Base_NN):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, output_size, activation, output_activation, max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Base_NN' from partially initialized module 'src.neural_network' (most likely due to a circular import) (/home/piri/Documenti/uni/Pisa/2024/ML/ML_project/src/neural_network.py)"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import csv\n",
    "import itertools\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from loss_functions import *\n",
    "from src.activation_functions import *\n",
    "from src.batch_normalization import *\n",
    "from src.data_preprocessing import *\n",
    "from src.ensemble.cascade_correlation import CascadeCorrelation\n",
    "from src.k_fold_cross_validation import *\n",
    "from src.layer import *\n",
    "from src.model_regularization import *\n",
    "from src.neural_network import *\n",
    "from src.optimizers import *\n",
    "from src.random_search import *\n",
    "from src.train_and_evaluate import *\n",
    "from src.utils import *\n",
    "\n",
    "# from src.random_search import *\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### Data pre-processing for MONK Datasets  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot encoded data:  (124, 17)\n",
      "one hot encoded data:  (432, 17)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_data(MONK_NUM=1)\n",
    "X_test, y_test = load_data(MONK_NUM=1, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42)\n",
    "X_train = np.asarray(X_train)\n",
    "y_train = np.asarray(y_train)\n",
    "X_val = np.asarray(X_val)\n",
    "y_val = np.asarray(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 99\n",
      "Validation set size: 25\n",
      "Training set overlap with validation set: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set size:\", len(X_train))\n",
    "print(\"Validation set size:\", len(X_val))\n",
    "print(\"Training set overlap with validation set:\",\n",
    "      np.intersect1d(X_train, X_val).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape:  (99, 17)\n",
      "the shape:  (25, 17)\n",
      "the shape:  (99,)\n",
      "the shape:  (25,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape of the resulting datasets\n",
    "for _ in [X_train, X_val, y_train, y_val]:\n",
    "    print(f\"the shape: \", _.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distributions = {\n",
    "    'hidden_size': [[3], [4], [5], [6]],\n",
    "    'hidden_activation': [[Activation_Tanh], [Activation_Leaky_ReLU], [Activation_Sigmoid], [Activation_ReLU]],\n",
    "    'batch_norm': [[True], [False]],\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-6, 1e-5],\n",
    "    'l1': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'l2': [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'dropout_rate': [0.0, 0.1, 0.3],\n",
    "    'batch_size': [8, 16, 32],\n",
    "    'n_epochs': [150, 200],\n",
    "    'weight_decay': [0, 5e-2, 1e-2, 1e-3, 1e-5]\n",
    "    # # Define combinations of hidden layer sizes and corresponding activations\n",
    "    # 'hidden_configs': [\n",
    "    #     {'hidden_size': [10], 'hidden_activation': [Activation_Tanh], 'batch_norm' : [True]},\n",
    "    #     {'hidden_size': [10, 10], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, True]},\n",
    "    #     {'hidden_size': [10, 10], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, True]},\n",
    "    #     {'hidden_size': [20, 20], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, False]},\n",
    "    #     {'hidden_size': [20, 20], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, True]},\n",
    "    #     {'hidden_size': [20, 20], 'hidden_activation': [Activation_ELU, Activation_ELU], 'batch_norm' : [True, False]},\n",
    "    #     {'hidden_size': [10, 10], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, False]},\n",
    "    #     {'hidden_size': [32, 16], 'hidden_activation': [Activation_Leaky_ReLU, Activation_Sigmoid], 'batch_norm' : [False, True]},  \n",
    "    #     {'hidden_size': [30, 30], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm' : [True, False]},\n",
    "    #     {'hidden_size': [64, 32, 16], 'hidden_activation': [Activation_ReLU, Activation_ReLU, Activation_Tanh], 'batch_norm' : [True, False, True]},\n",
    "    #     {'hidden_size': [30, 30], 'hidden_activation': [Activation_Leaky_ReLU, Activation_ELU], 'batch_norm': [True, False]}\n",
    "    # ]   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "\n",
    "Create a seperate best_results csv file for each MONK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "✅ Fold 1/5 | Validation Accuracy: 0.7000\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "✅ Fold 2/5 | Validation Accuracy: 0.3000\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "✅ Fold 3/5 | Validation Accuracy: 0.7000\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "✅ Fold 4/5 | Validation Accuracy: 0.6000\n",
      "[3] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.0] [True]\n",
      "✅ Fold 5/5 | Validation Accuracy: 0.3684\n",
      "\n",
      "📊 Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5337\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [False]\n",
      "✅ Fold 1/5 | Validation Accuracy: 0.7000\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [False]\n",
      "✅ Fold 2/5 | Validation Accuracy: 0.6000\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [False]\n",
      "✅ Fold 3/5 | Validation Accuracy: 0.3500\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [False]\n",
      "✅ Fold 4/5 | Validation Accuracy: 0.2500\n",
      "[4] [<class 'src.activation_functions.Activation_Sigmoid'>] [0.3] [False]\n",
      "✅ Fold 5/5 | Validation Accuracy: 0.6316\n",
      "\n",
      "📊 Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5063\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "✅ Fold 1/5 | Validation Accuracy: 0.7500\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "✅ Fold 2/5 | Validation Accuracy: 0.3500\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "✅ Fold 3/5 | Validation Accuracy: 0.5500\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "✅ Fold 4/5 | Validation Accuracy: 0.5000\n",
      "[3] [<class 'src.activation_functions.Activation_ReLU'>] [0.3] [False]\n",
      "✅ Fold 5/5 | Validation Accuracy: 0.5789\n",
      "\n",
      "📊 Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5458\n",
      "[4] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [False]\n",
      "✅ Fold 1/5 | Validation Accuracy: 0.6500\n",
      "[4] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [False]\n",
      "✅ Fold 2/5 | Validation Accuracy: 0.5500\n",
      "[4] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [False]\n",
      "✅ Fold 3/5 | Validation Accuracy: 0.5000\n",
      "[4] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [False]\n",
      "✅ Fold 4/5 | Validation Accuracy: 0.6000\n",
      "[4] [<class 'src.activation_functions.Activation_Tanh'>] [0.0] [False]\n",
      "✅ Fold 5/5 | Validation Accuracy: 0.4737\n",
      "\n",
      "📊 Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.5547\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.1] [True]\n",
      "✅ Fold 1/5 | Validation Accuracy: 0.6000\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.1] [True]\n",
      "✅ Fold 2/5 | Validation Accuracy: 0.3500\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.1] [True]\n",
      "✅ Fold 3/5 | Validation Accuracy: 0.4500\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.1] [True]\n",
      "✅ Fold 4/5 | Validation Accuracy: 0.4500\n",
      "[4] [<class 'src.activation_functions.Activation_Leaky_ReLU'>] [0.1] [True]\n",
      "✅ Fold 5/5 | Validation Accuracy: 0.3158\n",
      "\n",
      "📊 Manual K-Fold | Mean Validation Accuracy over 5 folds: 0.4332\n",
      "{'learning_rate': 0.0001, 'l1': 0.001, 'l2': 0.001, 'dropout_rate': 0.0, 'batch_size': 16, 'n_epochs': 200, 'hidden_size': [4], 'hidden_activation': ['Activation_Tanh'], 'batch_norm': [False], 'weight_decay': 1e-05, 'val_accuracy': np.float64(0.5547368421052632)}\n"
     ]
    }
   ],
   "source": [
    "best_hyperparams, best_performance = random_search(X_train=X_train,\n",
    "                                                   y_train=y_train,\n",
    "                                                   param_distributions=param_distributions,\n",
    "                                                   n_iters=5)  # adjust n_iters as needed\n",
    "\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.0001, 'l1': 0.001, 'l2': 0.001, 'dropout_rate': 0.0, 'batch_size': 16, 'n_epochs': 200, 'hidden_size': [4], 'hidden_activation': ['Activation_Tanh'], 'batch_norm': [False], 'weight_decay': 1e-05, 'val_accuracy': np.float64(0.5547368421052632)}\n"
     ]
    }
   ],
   "source": [
    "learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, hidden_size, hidden_activation, use_batch_norm, weight_decay, validation_accuracy = best_hyperparams.values()\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_map = {\n",
    "    'Activation_ReLU': Activation_ReLU,\n",
    "    'Activation_Tanh': Activation_Tanh,\n",
    "    'Activation_ELU': Activation_ELU,\n",
    "    'Activation_Leaky_ReLU': Activation_Leaky_ReLU,\n",
    "    'Activation_Sigmoid': Activation_Sigmoid\n",
    "}\n",
    "\n",
    "hidden_activation = [activation_map[act] for act in hidden_activation]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = NN(\n",
    "#     l1=l1,\n",
    "#     l2=l2,\n",
    "#     input_size=17,\n",
    "#     hidden_sizes=hidden_size,\n",
    "#     output_size=1,\n",
    "#     hidden_activations=hidden_activation,\n",
    "#     dropout_rates=[dropout_rate],\n",
    "#     use_batch_norm=use_batch_norm\n",
    "# )\n",
    "model = CascadeCorrelation(input_size = 17, output_size= 1, activation=Activation_Leaky_ReLU, output_activation = Activation_Sigmoid)\n",
    "batch_size = batch_size\n",
    "learning_rate = learning_rate\n",
    "n_epochs = n_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:\n",
      "X_train: (99, 17), y_train: (99,)\n",
      "Hyperparams: {'learning_rate': 0.0001, 'l1': 0.001, 'l2': 0.001, 'dropout_rate': 0.0, 'batch_size': 16, 'n_epochs': 200, 'hidden_size': [4], 'hidden_activation': ['Activation_Tanh'], 'batch_norm': [False], 'weight_decay': 1e-05, 'val_accuracy': np.float64(0.5547368421052632)}\n",
      "Epoch 0: Train Loss: 0.6997, Acc: 53.27% | Val Loss: 0.7154, Acc: 40.00%\n",
      "Epoch 10: Train Loss: 0.7046, Acc: 54.17% | Val Loss: 0.7158, Acc: 40.00%\n",
      "Epoch 20: Train Loss: 0.7230, Acc: 51.19% | Val Loss: 0.7162, Acc: 40.00%\n",
      "Epoch 30: Train Loss: 0.6912, Acc: 55.95% | Val Loss: 0.7165, Acc: 40.00%\n",
      "Epoch 40: Train Loss: 0.7118, Acc: 55.95% | Val Loss: 0.7170, Acc: 44.00%\n",
      "Early stopping at epoch 40\n",
      "Restoring model weights from epoch 0\n",
      "Added new neuron at epoch 40 wiht val_loss 0.7170\n",
      "Epoch 50: Train Loss: 0.7804, Acc: 44.05% | Val Loss: 0.6934, Acc: 48.00%\n",
      "Epoch 60: Train Loss: 0.7833, Acc: 44.05% | Val Loss: 0.6935, Acc: 48.00%\n",
      "Epoch 70: Train Loss: 0.7327, Acc: 51.79% | Val Loss: 0.6936, Acc: 44.00%\n",
      "Early stopping at epoch 77\n",
      "Restoring model weights from epoch 41\n",
      "Added new neuron at epoch 77 wiht val_loss 0.6937\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (16,18) and (19,1) not aligned: 18 (dim 1) != 19 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m batch_accuracies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m create_batches(X_train, y_train, batch_size):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Loss and accuracy\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function\u001b[38;5;241m.\u001b[39mforward(model\u001b[38;5;241m.\u001b[39moutput, y_batch)\n",
      "File \u001b[0;32m~/Documenti/uni/Pisa/2024/ML/ML_project/src/neural_network.py:20\u001b[0m, in \u001b[0;36mBase_NN.forward\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: np\u001b[38;5;241m.\u001b[39mndarray, training: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 20\u001b[0m         \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39moutput\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m inputs\n",
      "File \u001b[0;32m~/Documenti/uni/Pisa/2024/ML/ML_project/src/ensemble/cascade_correlation.py:73\u001b[0m, in \u001b[0;36mCascadeCorrelationLayer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    Fowards the output of the neuron with the input\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_output:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\n",
      "File \u001b[0;32m~/Documenti/uni/Pisa/2024/ML/ML_project/src/layer.py:21\u001b[0m, in \u001b[0;36mLayer_Dense.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (16,18) and (19,1) not aligned: 18 (dim 1) != 19 (dim 0)"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "loss_function = MSE()\n",
    "optimizer = Optimizer_Adam(learning_rate=learning_rate, decay=weight_decay)\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(patience=30, min_delta_loss=1e-5, min_delta_accuracy=0.001)\n",
    "\n",
    "# Before training loop:\n",
    "print(\"Data shapes:\")\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Hyperparams: {best_hyperparams}\")\n",
    "# print(f\"Sample prediction: {model.forward(X_train[:1])}\")\n",
    "# print(f\"Initial loss: {loss_function.forward(model.output, y_train[:1])}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    batch_losses = []\n",
    "    batch_accuracies = []\n",
    "\n",
    "    for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "        # Forward pass\n",
    "        model.forward(X_batch, training=True)\n",
    "\n",
    "        # Loss and accuracy\n",
    "        loss = loss_function.forward(model.output, y_batch)\n",
    "        predictions = np.round(model.output.squeeze())\n",
    "        accuracy = np.mean(predictions == y_batch.squeeze())\n",
    "\n",
    "        # Backward pass\n",
    "        loss_function.backward(model.output, y_batch)\n",
    "        dvalues = loss_function.dinputs\n",
    "\n",
    "        assert dvalues.shape == model.output.shape, \\\n",
    "            f\"Gradient shape mismatch: {dvalues.shape} vs {model.output.shape}\"\n",
    "        \n",
    "        i = 0\n",
    "        for layer in reversed(model.layers):\n",
    "            i=-1\n",
    "            layer.backward(dvalues)\n",
    "            dvalues = np.array(layer.dinputs)\n",
    "\n",
    "            # Regularization\n",
    "            if isinstance(layer, Layer_Dense):\n",
    "                if layer.l1 > 0:\n",
    "                    layer.dweights += layer.l1 * np.sign(layer.weights)\n",
    "                if layer.l2 > 0:\n",
    "                    layer.dweights += 2 * layer.l2 * layer.weights\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.pre_update_params()\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, Layer_Dense):\n",
    "                optimizer.update_params(layer)\n",
    "        optimizer.post_update_params()\n",
    "\n",
    "        batch_losses.append(loss)\n",
    "        batch_accuracies.append(accuracy)\n",
    "\n",
    "    # Epoch summary\n",
    "    epoch_loss = np.mean(batch_losses)\n",
    "    epoch_acc = np.mean(batch_accuracies)\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "\n",
    "    # Validation\n",
    "    X_val_input = X_val.values if isinstance(X_val, pd.DataFrame) else X_val\n",
    "    y_val_input = y_val.values if isinstance(y_val, (pd.Series, pd.DataFrame)) else y_val\n",
    "\n",
    "    model.forward(X_val_input, training=False)\n",
    "    val_loss = loss_function.forward(model.output, y_val_input)\n",
    "    val_predictions = np.round(model.output.squeeze())\n",
    "    val_accuracy = np.mean(val_predictions == y_val.squeeze())\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: \", end=\"\")\n",
    "        print(f\"Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Acc: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "    # Early stopping check\n",
    "    early_stopping.on_epoch_end(\n",
    "        current_loss=val_loss,\n",
    "        current_accuracy=val_accuracy,\n",
    "        model=model,\n",
    "        epoch=epoch\n",
    "    )\n",
    "\n",
    "    if early_stopping.stop_training:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        # Restore best weights\n",
    "        print(f\"Restoring model weights from epoch {early_stopping.best_epoch}\")\n",
    "        early_stopping.restore_weights(model)\n",
    "        # Cascade correlation\n",
    "        if isinstance(model, CascadeCorrelation):\n",
    "            if model.is_limit_reached():\n",
    "                break\n",
    "            \n",
    "            model.add_neuron()\n",
    "            early_stopping.wait = 0\n",
    "            early_stopping.patience -= int(early_stopping.patience / 10)\n",
    "            early_stopping.stop_training = False\n",
    "            print(f\"Added new neuron at epoch {epoch} wiht val_loss {val_losses[-1]:.4f}\")\n",
    "            continue\n",
    "        break\n",
    "\n",
    "# Final evaluation\n",
    "model.forward(X_val_input, training=False)\n",
    "final_val_loss = loss_function.forward(model.output, y_val_input)\n",
    "final_val_accuracy = np.mean(np.round(model.output.squeeze()) == y_val.squeeze())\n",
    "print(f\"Final Validation Accuracy: {final_val_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "# Test set evaluation\n",
    "model.forward(X_test, training=False)\n",
    "test_loss = loss_function.forward(model.output.squeeze(), y_test)\n",
    "\n",
    "predictions = np.round(model.output.squeeze())\n",
    "y_true = np.argmax(y_test, axis=1) if y_test.ndim > 1 else y_test\n",
    "test_accuracy = np.mean(predictions == y_true)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Plot training progress\n",
    "plot_losses(train_losses, val_losses, test_loss,\n",
    "            label1=\"Training Loss\", label2=\"Validation Loss\",\n",
    "            title=\"Loss Over Epochs\")\n",
    "\n",
    "plot_accuracies(train_accuracies, val_accuracies, test_accuracy,\n",
    "                label1=\"Training Accuracies\", label2=\"Validation Accuracies\",\n",
    "                title=\"Accuracy Over Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(X_test, training=False)\n",
    "# Compute softmax probabilities for the test output\n",
    "# print(X_test.shape, y_test.shape)\n",
    "# print(model.output, y_test)\n",
    "loss_function.forward(model.output.squeeze(), y_test)\n",
    "# Calculate accuracy for the test set\n",
    "predictions = np.round(model.output.squeeze())\n",
    "if len(y_test.shape) == 2:\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    y_true = y_test\n",
    "\n",
    "# Compute test accuracy\n",
    "test_accuracy = np.mean(predictions == y_true)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EnsembleNN:\n",
    "#     def __init__(self, n_models=5):\n",
    "#         self.models = []\n",
    "#         self.n_models = n_models\n",
    "#         self.loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "        \n",
    "            \n",
    "\n",
    "#     def create_and_train_models(self, hyperparams):\n",
    "#         # Create and train multiple models with the same hyperparameters\n",
    "#         for i in range(self.n_models):\n",
    "#             model = NN(\n",
    "#                 l1=l1,\n",
    "#                 l2=l2,\n",
    "#                 input_size=17,\n",
    "#                 hidden_sizes=hidden_size,\n",
    "#                 output_size=1,\n",
    "#                 hidden_activations=hidden_activation,\n",
    "#                 dropout_rates=[dropout_rate],\n",
    "#                 use_batch_norm=use_batch_norm\n",
    "#             )\n",
    "#             print(f\"Training model {i+1}/{self.n_models}\")\n",
    "#             # Train model using existing train_and_evaluate function\n",
    "#             model, val_accuracy = train_and_evaluate(\n",
    "#                 learning_rate=hyperparams['learning_rate'],\n",
    "#                 # l1=hyperparams['l1'],\n",
    "#                 # l2=hyperparams['l2'],\n",
    "#                 # dropout_rate=hyperparams['dropout_rate'],\n",
    "#                 batch_size=hyperparams['batch_size'],\n",
    "#                 n_epochs=hyperparams['n_epochs'],\n",
    "#                 weight_decay=hyperparams['weight_decay'],\n",
    "#                 # model=hyperparams['model']\n",
    "#                 # activation=hyperparams['activation']\n",
    "#                 X_train=X_train,\n",
    "#                 y_train=y_train,\n",
    "#                 X_val=X_val,\n",
    "#                 y_val=y_val,\n",
    "#                 model=model,\n",
    "#             )\n",
    "#             self.models.append(model)\n",
    "#             print(f\"Model {i+1} validation accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         \"\"\"Make predictions using majority voting\"\"\"\n",
    "#         predictions = []\n",
    "#         for model in self.models:\n",
    "#             model.forward(X, training=False)\n",
    "#             self.loss_activation.forward(\n",
    "#                 model.output, np.zeros((X.shape[0], 2)))  # Dummy y values\n",
    "#             pred = np.argmax(self.loss_activation.output, axis=1)\n",
    "#             predictions.append(pred)\n",
    "\n",
    "#         # Majority voting\n",
    "#         predictions = np.array(predictions)\n",
    "#         final_predictions = np.apply_along_axis(\n",
    "#             lambda x: np.bincount(x).argmax(),\n",
    "#             axis=0,\n",
    "#             arr=predictions\n",
    "#         )\n",
    "#         return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble = EnsembleNN(n_models=5)\n",
    "\n",
    "# ensemble.create_and_train_models(best_hyperparams)\n",
    "\n",
    "# test_predictions = ensemble.predict(X_test)\n",
    "# test_accuracy = np.mean(test_predictions == y_test)\n",
    "\n",
    "# print(f\"Ensemble Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
