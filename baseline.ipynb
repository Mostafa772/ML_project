{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.optimizers import *\n",
    "from src.activation_functions import * \n",
    "from src.utils import *\n",
    "from src.model_regularization import *\n",
    "from src.layer import *\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data pre-processing for MONK Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mostafa\\AppData\\Local\\Temp\\ipykernel_21912\\2770782428.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'index\n",
      "data_2     -1.255411\n",
      "data_3     -1.255411\n",
      "data_4     -1.255411\n",
      "data_5     -1.255411\n",
      "data_7     -1.255411\n",
      "              ...   \n",
      "data_420    1.214913\n",
      "data_422    1.214913\n",
      "data_425    1.214913\n",
      "data_430    1.214913\n",
      "data_432    1.214913\n",
      "Name: 2, Length: 122, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[:, i] = (X.iloc[:, i] - np.mean(X.iloc[:, i])) / \\\n",
      "C:\\Users\\Mostafa\\AppData\\Local\\Temp\\ipykernel_21912\\2770782428.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'index\n",
      "data_2     -0.936442\n",
      "data_3     -0.936442\n",
      "data_4     -0.936442\n",
      "data_5     -0.936442\n",
      "data_7     -0.936442\n",
      "              ...   \n",
      "data_420    1.067872\n",
      "data_422    1.067872\n",
      "data_425    1.067872\n",
      "data_430    1.067872\n",
      "data_432    1.067872\n",
      "Name: 3, Length: 122, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[:, i] = (X.iloc[:, i] - np.mean(X.iloc[:, i])) / \\\n",
      "C:\\Users\\Mostafa\\AppData\\Local\\Temp\\ipykernel_21912\\2770782428.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'index\n",
      "data_2     -1.245633\n",
      "data_3     -1.245633\n",
      "data_4     -1.245633\n",
      "data_5     -1.245633\n",
      "data_7     -1.245633\n",
      "              ...   \n",
      "data_420   -0.039544\n",
      "data_422   -0.039544\n",
      "data_425    1.166546\n",
      "data_430    1.166546\n",
      "data_432    1.166546\n",
      "Name: 4, Length: 122, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[:, i] = (X.iloc[:, i] - np.mean(X.iloc[:, i])) / \\\n",
      "C:\\Users\\Mostafa\\AppData\\Local\\Temp\\ipykernel_21912\\2770782428.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'index\n",
      "data_2     -1.302980\n",
      "data_3     -0.419849\n",
      "data_4     -0.419849\n",
      "data_5      0.463282\n",
      "data_7      1.346413\n",
      "              ...   \n",
      "data_420   -0.419849\n",
      "data_422    0.463282\n",
      "data_425   -1.302980\n",
      "data_430    0.463282\n",
      "data_432    1.346413\n",
      "Name: 5, Length: 122, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[:, i] = (X.iloc[:, i] - np.mean(X.iloc[:, i])) / \\\n",
      "C:\\Users\\Mostafa\\AppData\\Local\\Temp\\ipykernel_21912\\2770782428.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'index\n",
      "data_2      0.967733\n",
      "data_3     -1.033342\n",
      "data_4      0.967733\n",
      "data_5     -1.033342\n",
      "data_7     -1.033342\n",
      "              ...   \n",
      "data_420    0.967733\n",
      "data_422    0.967733\n",
      "data_425   -1.033342\n",
      "data_430    0.967733\n",
      "data_432    0.967733\n",
      "Name: 6, Length: 122, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X.iloc[:, i] = (X.iloc[:, i] - np.mean(X.iloc[:, i])) / \\\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data_2</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.255411</td>\n",
       "      <td>-0.936442</td>\n",
       "      <td>-1.245633</td>\n",
       "      <td>-1.302980</td>\n",
       "      <td>0.967733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.255411</td>\n",
       "      <td>-0.936442</td>\n",
       "      <td>-1.245633</td>\n",
       "      <td>-0.419849</td>\n",
       "      <td>-1.033342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.255411</td>\n",
       "      <td>-0.936442</td>\n",
       "      <td>-1.245633</td>\n",
       "      <td>-0.419849</td>\n",
       "      <td>0.967733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_5</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.255411</td>\n",
       "      <td>-0.936442</td>\n",
       "      <td>-1.245633</td>\n",
       "      <td>0.463282</td>\n",
       "      <td>-1.033342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_7</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.255411</td>\n",
       "      <td>-0.936442</td>\n",
       "      <td>-1.245633</td>\n",
       "      <td>1.346413</td>\n",
       "      <td>-1.033342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1         2         3         4         5         6\n",
       "index                                                      \n",
       "data_2  1 -1.255411 -0.936442 -1.245633 -1.302980  0.967733\n",
       "data_3  1 -1.255411 -0.936442 -1.245633 -0.419849 -1.033342\n",
       "data_4  1 -1.255411 -0.936442 -1.245633 -0.419849  0.967733\n",
       "data_5  1 -1.255411 -0.936442 -1.245633  0.463282 -1.033342\n",
       "data_7  1 -1.255411 -0.936442 -1.245633  1.346413 -1.033342"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################\n",
    "### Don't forget to change the path to the data file ###\n",
    "########################################################\n",
    "\n",
    "df = pd.read_csv(\"../ML_project/data/Monk_3/monks-3.train\", names=[0,1,2,3,4,5,6,\"index\"], delimiter= \" \")\n",
    "# df = pd.read_csv(\"../ML_project/data/Monk_2/monks-2.train\",\n",
    "#                  names=[0, 1, 2, 3, 4, 5, 6, \"index\"], delimiter=\" \")\n",
    "df.set_index(\"index\", inplace=True)\n",
    "y = df.iloc[:, 0]  # First column as target\n",
    "X = df.iloc[:, 1:]  # All other columns as features\n",
    "for i in range(1, X.shape[1]):\n",
    "    X.iloc[:, i] = (X.iloc[:, i] - np.mean(X.iloc[:, i])) / \\\n",
    "        np.std(X.iloc[:, i])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mostafa\\AppData\\Local\\Temp\\ipykernel_21912\\2279363569.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'index\n",
      "data_1     -1.224745\n",
      "data_2     -1.224745\n",
      "data_3     -1.224745\n",
      "data_4     -1.224745\n",
      "data_5     -1.224745\n",
      "              ...   \n",
      "data_428    1.224745\n",
      "data_429    1.224745\n",
      "data_430    1.224745\n",
      "data_431    1.224745\n",
      "data_432    1.224745\n",
      "Name: 2, Length: 432, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test.iloc[:, i] = (X_test.iloc[:, i] - np.mean(X_test.iloc[:, i])) / np.std(X_test.iloc[:, i])\n",
      "C:\\Users\\Mostafa\\AppData\\Local\\Temp\\ipykernel_21912\\2279363569.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'index\n",
      "data_1     -1.0\n",
      "data_2     -1.0\n",
      "data_3     -1.0\n",
      "data_4     -1.0\n",
      "data_5     -1.0\n",
      "           ... \n",
      "data_428    1.0\n",
      "data_429    1.0\n",
      "data_430    1.0\n",
      "data_431    1.0\n",
      "data_432    1.0\n",
      "Name: 3, Length: 432, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test.iloc[:, i] = (X_test.iloc[:, i] - np.mean(X_test.iloc[:, i])) / np.std(X_test.iloc[:, i])\n",
      "C:\\Users\\Mostafa\\AppData\\Local\\Temp\\ipykernel_21912\\2279363569.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'index\n",
      "data_1     -1.224745\n",
      "data_2     -1.224745\n",
      "data_3     -1.224745\n",
      "data_4     -1.224745\n",
      "data_5     -1.224745\n",
      "              ...   \n",
      "data_428    1.224745\n",
      "data_429    1.224745\n",
      "data_430    1.224745\n",
      "data_431    1.224745\n",
      "data_432    1.224745\n",
      "Name: 4, Length: 432, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test.iloc[:, i] = (X_test.iloc[:, i] - np.mean(X_test.iloc[:, i])) / np.std(X_test.iloc[:, i])\n",
      "C:\\Users\\Mostafa\\AppData\\Local\\Temp\\ipykernel_21912\\2279363569.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'index\n",
      "data_1     -1.341641\n",
      "data_2     -1.341641\n",
      "data_3     -0.447214\n",
      "data_4     -0.447214\n",
      "data_5      0.447214\n",
      "              ...   \n",
      "data_428   -0.447214\n",
      "data_429    0.447214\n",
      "data_430    0.447214\n",
      "data_431    1.341641\n",
      "data_432    1.341641\n",
      "Name: 5, Length: 432, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test.iloc[:, i] = (X_test.iloc[:, i] - np.mean(X_test.iloc[:, i])) / np.std(X_test.iloc[:, i])\n",
      "C:\\Users\\Mostafa\\AppData\\Local\\Temp\\ipykernel_21912\\2279363569.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'index\n",
      "data_1     -1.0\n",
      "data_2      1.0\n",
      "data_3     -1.0\n",
      "data_4      1.0\n",
      "data_5     -1.0\n",
      "           ... \n",
      "data_428    1.0\n",
      "data_429   -1.0\n",
      "data_430    1.0\n",
      "data_431   -1.0\n",
      "data_432    1.0\n",
      "Name: 6, Length: 432, dtype: float64' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  X_test.iloc[:, i] = (X_test.iloc[:, i] - np.mean(X_test.iloc[:, i])) / np.std(X_test.iloc[:, i])\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "### Don't forget to change the path to the data file ###\n",
    "########################################################\n",
    "\n",
    "test_data = pd.read_csv(\"../ML_project/data/Monk_3/monks-3.test\",\n",
    "names=[0, 1, 2, 3, 4, 5, 6, \"index\"], delimiter=\" \")\n",
    "test_data.set_index(\"index\", inplace=True)\n",
    "# test_data.head()\n",
    "y_test = test_data.iloc[:, 0]\n",
    "X_test = test_data.iloc[:, 1:]\n",
    "for i in range(1, X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = (X_test.iloc[:, i] - np.mean(X_test.iloc[:, i])) / np.std(X_test.iloc[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (97, 6)\n",
      "Validation Features Shape: (25, 6)\n",
      "Training Target Shape: (97,)\n",
      "Validation Target Shape: (25,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of the resulting datasets\n",
    "print(\"Training Features Shape:\", X_train.shape)\n",
    "print(\"Validation Features Shape:\", X_val.shape)\n",
    "print(\"Training Target Shape:\", y_train.shape)\n",
    "print(\"Validation Target Shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidence = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidence = np.sum(y_pred_clipped * y_true, axis=1) \n",
    "\n",
    "        negative_log_likelihoods = np.log(correct_confidence)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class MSE:\n",
    "    def __init__(self):\n",
    "        self.dinputs = None\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Remove the shape condition - always calculate loss\n",
    "        self.output = np.mean((y_pred - y_true)**2)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        self.dinputs = 2 * (dvalues - y_true) / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, l1, l2, input_size, hidden_sizes, output_size, \n",
    "                 hidden_activations=None, dropout_rates=None, output_activation=Activation_Sigmoid()):\n",
    "        self.layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Default to ReLU if no activations specified\n",
    "        if hidden_activations is None:\n",
    "            hidden_activations = [Activation_ReLU() for _ in hidden_sizes]\n",
    "        \n",
    "        # Default to no dropout\n",
    "        if dropout_rates is None:\n",
    "            dropout_rates = [0.0] * len(hidden_sizes)\n",
    "            \n",
    "        # Create hidden layers\n",
    "        for size, activation, rate in zip(hidden_sizes, hidden_activations, dropout_rates):\n",
    "            self.layers.append(Layer_Dense(prev_size, size,l1=l1, l2=l2))\n",
    "            self.layers.append(activation())\n",
    "            if rate > 0:\n",
    "                self.layers.append(Dropout(rate))\n",
    "            prev_size = size\n",
    "        \n",
    "        # Output layer (no activation)\n",
    "        self.layers.append(Layer_Dense(prev_size, output_size))\n",
    "        # self.layers.append(output_activation)\n",
    "        \n",
    "    def forward(self, inputs, training=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                layer.forward(inputs, training)\n",
    "            else:\n",
    "                layer.forward(inputs)\n",
    "            inputs = layer.output\n",
    "        self.output = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_evaluate(learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, activation):\n",
    "#     # Initialize components\n",
    "#     model = NN(\n",
    "#         l1=l1,\n",
    "#         l2=l2,\n",
    "#         input_size=6,\n",
    "#         hidden_sizes=[10],\n",
    "#         output_size=2,\n",
    "#         hidden_activations=[activation],\n",
    "#         dropout_rates=[dropout_rate]\n",
    "#     )\n",
    "    \n",
    "#     loss_activation = MSE()\n",
    "#     optimizer = Optimizer_Adam(learning_rate=learning_rate, decay=1e-3)\n",
    "\n",
    "#     train_losses = []\n",
    "#     train_accuracies = []\n",
    "#     val_losses = []\n",
    "#     val_accuracies = []\n",
    "\n",
    "#     for epoch in range(n_epochs):\n",
    "#         batch_losses = []\n",
    "#         batch_accuracies = []\n",
    "        \n",
    "#         for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "#             # Forward pass through model\n",
    "#             model.forward(X_batch, training=True)\n",
    "            \n",
    "#             # Calculate loss through separate loss activation\n",
    "#             loss = loss_activation.forward(model.output, y_batch)\n",
    "#             # print(y_batch.shape)\n",
    "#             # Calculate accuracy\n",
    "#             predictions = np.argmax(loss_activation.output, axis=1)\n",
    "#             accuracy = np.mean(predictions == y_batch)\n",
    "            \n",
    "#             # Backward pass\n",
    "#             loss_activation.backward(loss_activation.output, y_batch)\n",
    "#             dvalues = loss_activation.dinputs\n",
    "            \n",
    "#             # Propagate gradients through model layers in reverse\n",
    "#             for layer in reversed(model.layers):\n",
    "#                 layer.backward(dvalues)\n",
    "#                 dvalues = layer.dinputs\n",
    "                \n",
    "#             # Update parameters\n",
    "#             optimizer.pre_update_params()\n",
    "#             for layer in model.layers:\n",
    "#                 if isinstance(layer, Layer_Dense):\n",
    "#                     optimizer.update_params(layer)\n",
    "#             optimizer.post_update_params()\n",
    "            \n",
    "#             batch_losses.append(loss)\n",
    "#             batch_accuracies.append(accuracy)\n",
    "\n",
    "#         # Epoch metrics\n",
    "#         epoch_loss = np.mean(batch_losses)\n",
    "#         epoch_accuracy = np.mean(batch_accuracies)\n",
    "#         train_losses.append(epoch_loss)\n",
    "#         train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "#         # Validation pass\n",
    "#         model.forward(X_val, training=False)\n",
    "#         val_loss = loss_activation.forward(model.output, y_val)\n",
    "#         val_predictions = np.argmax(loss_activation.output, axis=1)\n",
    "#         val_accuracy = np.mean(val_predictions == y_val)\n",
    "#         val_losses.append(val_loss)\n",
    "#         val_accuracies.append(val_accuracy)\n",
    "\n",
    "#     return val_accuracies[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(target, output):\n",
    "    predictions = np.round(output.squeeze())\n",
    "    return np.mean(predictions == target.squeeze()) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, activation):\n",
    "    # Initialize components\n",
    "    model = NN(\n",
    "        l1=l1,\n",
    "        l2=l2,\n",
    "        input_size=6,\n",
    "        hidden_sizes=[10],\n",
    "        output_size=1,\n",
    "        hidden_activations=[activation],\n",
    "        dropout_rates=[dropout_rate],\n",
    "        output_activation=Activation_Sigmoid()\n",
    "    )\n",
    "    \n",
    "    loss_function = MSE()\n",
    "    optimizer = Optimizer_Adam(learning_rate=learning_rate, decay=1e-3)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        batch_losses = []\n",
    "        batch_accuracies = []\n",
    "        \n",
    "        for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "            # Forward pass\n",
    "            model.forward(X_batch, training=True)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_function.forward(model.output, y_batch)\n",
    "            \n",
    "            predictions = np.round(model.output.squeeze())\n",
    "            accuracy = np.mean(predictions == y_batch.squeeze())\n",
    "\n",
    "            # Backward pass with shape validation\n",
    "            loss_function.backward(model.output, y_batch)\n",
    "            dvalues = loss_function.dinputs\n",
    "            \n",
    "            # Verify gradient shape matches output\n",
    "            assert dvalues.shape == model.output.shape, \\\n",
    "                f\"Gradient shape mismatch: {dvalues.shape} vs {model.output.shape}\"\n",
    "            \n",
    "            # Propagate gradients\n",
    "            for layer in reversed(model.layers):\n",
    "                layer.backward(dvalues)\n",
    "                dvalues = layer.dinputs\n",
    "                \n",
    "                # Ensure numpy arrays\n",
    "                if isinstance(dvalues, pd.DataFrame):\n",
    "                    dvalues = dvalues.values\n",
    "                elif isinstance(dvalues, pd.Series):\n",
    "                    dvalues = dvalues.values.reshape(-1, 1)\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.pre_update_params()\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, Layer_Dense):\n",
    "                    optimizer.update_params(layer)\n",
    "            optimizer.post_update_params()\n",
    "            \n",
    "            batch_losses.append(loss)\n",
    "            batch_accuracies.append(accuracy)\n",
    "\n",
    "        # Epoch metrics\n",
    "        epoch_loss = np.mean(batch_losses)\n",
    "        epoch_acc = np.mean(batch_accuracies)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "\n",
    "        # Validation pass\n",
    "        model.forward(X_val.values if isinstance(X_val, pd.DataFrame) else X_val, \n",
    "                     training=False)\n",
    "        val_loss = loss_function.forward(model.output, \n",
    "                                        y_val.values if isinstance(y_val, (pd.Series, pd.DataFrame)) else y_val)\n",
    "        val_predictions = np.round(model.output.squeeze())\n",
    "        val_accuracy = np.mean(val_predictions == y_val.squeeze())\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: \"\n",
    "                  f\"Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc*100:.2f}% | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Acc: {val_accuracy*100:.2f}%\")\n",
    "    # print(val_accuracies, val_accuracies[-1])\n",
    "    return val_losses[-1], val_accuracies[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter_grid = {\n",
    "#     'learning_rate': [0.1,0.01,0.001, 0.003],\n",
    "#     'l1': [0.0, 1e-5],\n",
    "#     'l2': [0.0, 1e-4],\n",
    "#     'dropout_rate': [0.1, 0.3, 0.5],\n",
    "#     'batch_size': [1, 4, 16, 32],\n",
    "#     'n_epochs': [50, 100, 200, 400],\n",
    "#     'activation': [Activation_Sigmoid, Activation_Leaky_ReLU, Activation_Tanh]\n",
    "# }\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    'learning_rate': [0.1],\n",
    "    'l1': [0.0, 1e-5],\n",
    "    'l2': [0.0, 1e-4],\n",
    "    'dropout_rate': [0.1,],\n",
    "    'batch_size': [1,],\n",
    "    'n_epochs': [50, 100],\n",
    "    'activation': [Activation_Sigmoid]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(val_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss: 0.2889, Acc: 60.82% | Val Loss: 0.3414, Acc: 72.00%\n",
      "Epoch 0: Train Loss: 0.2382, Acc: 63.92% | Val Loss: 0.3927, Acc: 80.00%\n",
      "Epoch 0: Train Loss: 0.2392, Acc: 70.10% | Val Loss: 0.3317, Acc: 68.00%\n",
      "Epoch 0: Train Loss: 0.2755, Acc: 64.95% | Val Loss: 0.3593, Acc: 60.00%\n",
      "Epoch 0: Train Loss: 0.2734, Acc: 64.95% | Val Loss: 0.3664, Acc: 68.00%\n",
      "Epoch 0: Train Loss: 0.2238, Acc: 69.07% | Val Loss: 0.3600, Acc: 76.00%\n",
      "Epoch 0: Train Loss: 0.2892, Acc: 59.79% | Val Loss: 0.2930, Acc: 80.00%\n",
      "Epoch 0: Train Loss: 0.2893, Acc: 58.76% | Val Loss: 0.4467, Acc: 52.00%\n",
      "Best Hyperparameters: [{'learning_rate': 0.1, 'l1': 1e-05, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1, 'n_epochs': 50, 'activation': <class 'src.activation_functions.Activation_Sigmoid'>}]\n",
      "Best Validation Accuracy: (0.476158113033482, 0.96)\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to store the best hyperparameters and performance\n",
    "best_hyperparams = []\n",
    "best_performance = (-np.inf, -np.inf)  # Assuming we are maximizing validation accuracy\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for params in product(*hyperparameter_grid.values()):\n",
    "    # Unpack the hyperparameters\n",
    "    learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, activation = params\n",
    "    # Train and evaluate the model\n",
    "    val_accuracy = train_and_evaluate(learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, activation)\n",
    "    # Update the best hyperparameters if the current model is better\n",
    "    if val_accuracy[1] > best_performance[1]:\n",
    "        best_hyperparams.clear()\n",
    "        best_performance = val_accuracy\n",
    "        best_hyperparams.append({\n",
    "            'learning_rate': learning_rate,\n",
    "            'l1': l1,\n",
    "            'l2': l2,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'n_epochs': n_epochs,\n",
    "            'activation': activation\n",
    "        })\n",
    "    elif val_accuracy[1] == 1.0:\n",
    "        best_hyperparams.append({\n",
    "            'learning_rate': learning_rate,\n",
    "            'l1': l1,\n",
    "            'l2': l2,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'n_epochs': n_epochs,\n",
    "            'activation': activation\n",
    "        })\n",
    "\n",
    "# Print the best hyperparameters and performance\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "print(\"Best Validation Accuracy:\", best_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'learning_rate': 0.1, 'l1': 1e-05, 'l2': 0.0, 'dropout_rate': 0.1, 'batch_size': 1, 'n_epochs': 50, 'activation': <class 'src.activation_functions.Activation_Sigmoid'>}]\n"
     ]
    }
   ],
   "source": [
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model_performance = train_and_evaluate(**best_hyperparams)\n",
    "# print(\"Final Model Performance:\", final_model_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, activation = best_hyperparams.values()\n",
    "# learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:\n",
      "X_train: (97, 6), y_train: (97,)\n",
      "Sample prediction: None\n",
      "Initial loss: 0.001363589250398477\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for abs(): 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Backward pass with shape validation\u001b[39;00m\n\u001b[0;32m     47\u001b[0m loss_function\u001b[38;5;241m.\u001b[39mbackward(model\u001b[38;5;241m.\u001b[39moutput, y_batch)\n\u001b[1;32m---> 49\u001b[0m max_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdweights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLayer_Dense\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax gradient: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_grad\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m dvalues \u001b[38;5;241m=\u001b[39m loss_function\u001b[38;5;241m.\u001b[39mdinputs\n",
      "Cell \u001b[1;32mIn[18], line 50\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Backward pass with shape validation\u001b[39;00m\n\u001b[0;32m     47\u001b[0m loss_function\u001b[38;5;241m.\u001b[39mbackward(model\u001b[38;5;241m.\u001b[39moutput, y_batch)\n\u001b[0;32m     49\u001b[0m max_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m---> 50\u001b[0m     np\u001b[38;5;241m.\u001b[39mmax(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdweights\u001b[49m\u001b[43m)\u001b[49m) \n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers \n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Layer_Dense)\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax gradient: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_grad\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m dvalues \u001b[38;5;241m=\u001b[39m loss_function\u001b[38;5;241m.\u001b[39mdinputs\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for abs(): 'NoneType'"
     ]
    }
   ],
   "source": [
    "for i in best_hyperparams: \n",
    "    learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, activation = i.values()\n",
    "    model = NN(\n",
    "        l1=l1,\n",
    "        l2=l2,\n",
    "        input_size=6,\n",
    "        hidden_sizes=[10],\n",
    "        output_size=1,\n",
    "        hidden_activations=[activation],\n",
    "        dropout_rates=[dropout_rate]\n",
    "    )\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    # early_stopping = EarlyStopping(\n",
    "    #     patience=20,\n",
    "    #     min_delta_loss=0.0001,\n",
    "    #     min_delta_accuracy=0.0001,\n",
    "    #     restore_best_weights=True\n",
    "    # )\n",
    "    # loss_activation = MSE()\n",
    "    loss_function = MSE()\n",
    "    optimizer = Optimizer_Adam(learning_rate=learning_rate)\n",
    "    # Before training loop:\n",
    "    print(\"Data shapes:\")\n",
    "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"Sample prediction: {model.forward(X_train[:1])}\")  # Should output ~0.5\n",
    "    print(f\"Initial loss: {loss_function.forward(model.output, y_train[:1].squeeze())}\")\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        batch_losses = []\n",
    "        batch_accuracies = []\n",
    "        \n",
    "        for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "            # Forward pass\n",
    "            model.forward(X_batch, training=True)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_function.forward(model.output, y_batch)\n",
    "            \n",
    "            predictions = np.round(model.output.squeeze())\n",
    "            accuracy = np.mean(predictions == y_batch.squeeze())\n",
    "\n",
    "            # Backward pass with shape validation\n",
    "            loss_function.backward(model.output, y_batch)\n",
    "\n",
    "            max_grad = max(\n",
    "                np.max(np.abs(layer.dweights)) \n",
    "                for layer in model.layers \n",
    "                if isinstance(layer, Layer_Dense)\n",
    "            )\n",
    "            print(f\"Max gradient: {max_grad:.4f}\")\n",
    "            dvalues = loss_function.dinputs\n",
    "            \n",
    "            # Verify gradient shape matches output\n",
    "            assert dvalues.shape == model.output.shape, \\\n",
    "                f\"Gradient shape mismatch: {dvalues.shape} vs {model.output.shape}\"\n",
    "            \n",
    "            # Propagate gradients\n",
    "            for layer in reversed(model.layers):\n",
    "                layer.backward(dvalues)\n",
    "                dvalues = layer.dinputs\n",
    "                \n",
    "                # Ensure numpy arrays\n",
    "                if isinstance(dvalues, pd.DataFrame):\n",
    "                    dvalues = dvalues.values\n",
    "                elif isinstance(dvalues, pd.Series):\n",
    "                    dvalues = dvalues.values.reshape(-1, 1)\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.pre_update_params()\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, Layer_Dense):\n",
    "                    optimizer.update_params(layer)\n",
    "            optimizer.post_update_params()\n",
    "            \n",
    "            batch_losses.append(loss)\n",
    "            batch_accuracies.append(accuracy)\n",
    "\n",
    "        # Epoch metrics\n",
    "        epoch_loss = np.mean(batch_losses)\n",
    "        epoch_acc = np.mean(batch_accuracies)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        print(epoch_loss, epoch_acc)\n",
    "        # Validation pass\n",
    "        model.forward(X_val.values if isinstance(X_val, pd.DataFrame) else X_val, \n",
    "                        training=False)\n",
    "        # print(model.output)\n",
    "        val_loss = loss_function.forward(model.output, y_val.values if isinstance(y_val, (pd.Series, pd.DataFrame)) else y_val)\n",
    "        val_predictions = np.round(model.output.squeeze())\n",
    "        val_accuracy = np.mean(val_predictions == y_val.squeeze())\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "    # print(len(val_accuracies), len(train_accuracies))\n",
    "    plot_accuracies(train_accuracies, val_accuracies, label1=\"Training Accuracies\", label2=\"Validation Accuracies\", title=\"Accuracy Over Epochs\")\n",
    "    plot_losses(train_losses, val_losses, label1=\"Training Loss\", label2=\"Validation Loss\", title=\"Loss Over Epochs\")\n",
    "    \n",
    "    model.forward(X_test, training=False)\n",
    "    # Compute softmax probabilities for the test output\n",
    "    # print(X_test.shape, y_test.shape)\n",
    "    # print(model.output, y_test)\n",
    "    loss_function.forward(model.output.squeeze(), y_test) \n",
    "    # Calculate accuracy for the test set\n",
    "    predictions = np.round(model.output.squeeze())\n",
    "    if len(y_test.shape) == 2:\n",
    "        y_true = np.argmax(y_test, axis=1) \n",
    "    else:\n",
    "        y_true = y_test\n",
    "\n",
    "    # Compute test accuracy\n",
    "    test_accuracy = np.mean(predictions == y_true)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.3408472197476576, 0.39220134100533427, 0.3517592001330052, 0.359354626973759, 0.445301660384225, 0.46967723340757234, 0.30671469885389113, 0.3675657169746863, 0.437299500739877, 0.5756837948880944, 0.5379057026864668, 0.4683187149758564, 0.4278020091041673, 0.40494186152012446, 0.453661344935394, 0.4523533935901557, 0.539615405863994, 0.3614852711991134, 0.39071796202154496, 0.3841183670576393, 0.4307056684828311, 0.3853687111984917, 0.36221899390770357, 0.33369875162904833, 0.560752678750922, 0.45625943946873454, 0.4556209647444425, 0.4685704252473555, 0.4492434175558663, 0.4503054233621944, 0.40910254011228997, 0.5036253061968651, 0.4382209674054665, 0.5001077702966245, 0.43038748428287243, 0.37163782020395264, 0.3436477572927342, 0.4054560148225245, 0.3513557713835197, 0.3763797134081373, 0.42860558878257576, 0.4171505740806912, 0.3762509532990682, 0.4299824560405011, 0.48987450584562425, 0.35424650418614084, 0.36580797117384034, 0.43854379546599465, 0.3182861872076362, 0.4354436115167082, 0.36309356661912856, 0.4207300780713717, 0.39725867131898784, 0.4020942678345992, 0.34073062821103817, 0.4553834882547711, 0.4201704032140094, 0.4194155613991514, 0.3885457041464039, 0.4268431486652321, 0.47799452869333786, 0.38310761842818414, 0.4426531168084145, 0.3717354811713308, 0.3979915629519244, 0.3690701645883604, 0.6467585793483315, 0.41460042371231065, 0.3642872069502267, 0.4039331756494145, 0.4826857526981624, 0.4351229605479606, 0.48308399098245414, 0.4115753056698327, 0.38136554403331685, 0.4309713091511191, 0.45439453407735697, 0.3388982128561001, 0.4631484633937102, 0.32878870341126554, 0.4906821687118671, 0.41764803559694974, 0.38885415736416545, 0.45266245331475097, 0.4809924550342244, 0.40481644638697556, 0.41699207249928166, 0.47658056469131427, 0.4152948864133276, 0.4158404515763791, 0.4814161275263894, 0.4080888745022744, 0.38634472145074744, 0.5034601417875361, 0.4331431205067807, 0.43712503851480466, 0.48689733634299165, 0.38960480923315016, 0.47174658328115016] [0.3408472197476577, 0.3922013410053344, 0.3517592001330052, 0.35935462697375903, 0.44530166038422486, 0.4696772334075724, 0.30671469885389113, 0.3675657169746863, 0.437299500739877, 0.5756837948880943, 0.5379057026864668, 0.46831871497585636, 0.4278020091041673, 0.40494186152012435, 0.453661344935394, 0.4523533935901558, 0.5396154058639939, 0.3614852711991133, 0.3907179620215449, 0.38411836705763924, 0.43070566848283104, 0.38536871119849175, 0.36221899390770357, 0.3336987516290483, 0.560752678750922, 0.4562594394687345, 0.45562096474444264, 0.46857042524735554, 0.4492434175558663, 0.4503054233621945, 0.40910254011229, 0.5036253061968652, 0.43822096740546657, 0.5001077702966245, 0.43038748428287243, 0.37163782020395275, 0.3436477572927342, 0.4054560148225244, 0.3513557713835198, 0.3763797134081373, 0.4286055887825759, 0.41715057408069117, 0.37625095329906816, 0.42998245604050106, 0.48987450584562414, 0.3542465041861409, 0.3658079711738404, 0.43854379546599465, 0.31828618720763624, 0.4354436115167082, 0.3630935666191285, 0.4207300780713718, 0.39725867131898784, 0.4020942678345993, 0.3407306282110381, 0.45538348825477115, 0.4201704032140094, 0.4194155613991515, 0.3885457041464039, 0.4268431486652322, 0.4779945286933379, 0.38310761842818425, 0.44265311680841457, 0.3717354811713308, 0.3979915629519244, 0.3690701645883604, 0.6467585793483317, 0.41460042371231065, 0.36428720695022676, 0.4039331756494146, 0.4826857526981623, 0.4351229605479606, 0.48308399098245425, 0.4115753056698328, 0.38136554403331685, 0.43097130915111903, 0.4543945340773569, 0.3388982128561, 0.46314846339371013, 0.3287887034112655, 0.49068216871186704, 0.41764803559694974, 0.38885415736416545, 0.4526624533147509, 0.48099245503422444, 0.4048164463869756, 0.4169920724992815, 0.47658056469131416, 0.4152948864133276, 0.415840451576379, 0.4814161275263894, 0.4080888745022744, 0.38634472145074744, 0.5034601417875361, 0.4331431205067806, 0.43712503851480466, 0.4868973363429917, 0.3896048092331502, 0.4717465832811501, 0.4845454774203516]\n"
     ]
    }
   ],
   "source": [
    "print(train_losses, val_losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(test_accuracies))\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_accuracies\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_accuracies))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_hyperparams[\u001b[38;5;241m15\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(np.argmax(test_accuracies))\n",
    "print(test_accuracies[15])\n",
    "print(len(test_accuracies))\n",
    "print(best_hyperparams[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8727\n"
     ]
    }
   ],
   "source": [
    "model.forward(X_test, training=False)\n",
    "# Compute softmax probabilities for the test output\n",
    "# print(X_test.shape, y_test.shape)\n",
    "# print(model.output, y_test)\n",
    "loss_function.forward(model.output.squeeze(), y_test) \n",
    "# Calculate accuracy for the test set\n",
    "predictions = np.round(model.output.squeeze())\n",
    "if len(y_test.shape) == 2:\n",
    "    y_true = np.argmax(y_test, axis=1) \n",
    "else:\n",
    "    y_true = y_test\n",
    "\n",
    "# Compute test accuracy\n",
    "test_accuracy = np.mean(predictions == y_true)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.1, 1e-05, 0.0, 0.3, 16, 100, src.activation_functions.Activation_Leaky_ReLU) MONK2 Cross entropy ADAM\n",
    "\n",
    "(0.1, 0.0, 0.0, 0.1, 16, 200, src.activation_functions.Activation_Tanh) MONK3 Cross entropy ADAM\n",
    "test_result: 0.9144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MONK1:\n",
    "    \n",
    "    \n",
    "MONK2:\n",
    "\n",
    "\n",
    "\n",
    "MONK3:\n",
    "Cross Entropy: \n",
    "(0.1, 0.0, 0.0, 0.1, 1, 100, src.activation_functions.Activation_Sigmoid) RMSprop\n",
    "Test Accuracy: 0.9213\n",
    "(0.1, 0.0, 0.0, 0.1, 1, 400, src.activation_functions.Activation_Sigmoid) Adam\n",
    "Test Accuracy: 0.9606\n",
    "\n",
    "MSE: \n",
    "{'learning_rate': 0.1, 'l1': 1e-05, 'l2': 0.0001, 'dropout_rate': 0.1, 'batch_size': 4, 'n_epochs': 50, 'activation': <class 'src.activation_functions.Activation_Tanh'>} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 350\n",
    "# batch_size = 1\n",
    "\n",
    "\n",
    "# train_losses = []\n",
    "# train_accuracies = []\n",
    "# val_losses = []\n",
    "# val_accuracies = []\n",
    "\n",
    "# # early_stopping = EarlyStopping(\n",
    "# #     patience=20,\n",
    "# #     min_delta_loss=0.0001,\n",
    "# #     min_delta_accuracy=0.0001,\n",
    "# #     restore_best_weights=True\n",
    "# # )\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(n_epochs):\n",
    "#     batch_losses = []\n",
    "#     batch_accuracies = []\n",
    "    \n",
    "#     # Mini-batch training\n",
    "#     for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "#         # Forward pass\n",
    "#         dense1.forward(X_batch)\n",
    "#         activation4.forward(dense1.output)\n",
    "#         dense2.forward(activation4.output)\n",
    "#         loss = loss_activation.forward(dense2.output, y_batch)\n",
    "        \n",
    "#         # Calculate accuracy for this batch\n",
    "#         predictions = np.argmax(loss_activation.output, axis=1)\n",
    "#         if len(y_batch.shape) == 2:\n",
    "#             y_true = np.argmax(y_batch, axis=1)\n",
    "#         else:\n",
    "#             y_true = y_batch\n",
    "#         accuracy = np.mean(predictions == y_true)\n",
    "        \n",
    "#         # Backward pass\n",
    "#         loss_activation.backward(loss_activation.output, y_batch)\n",
    "#         dense2.backward(loss_activation.dinputs)\n",
    "#         activation4.backward(dense2.dinputs)\n",
    "#         dense1.backward(activation4.dinputs)\n",
    "        \n",
    "#         # Update weights and biases\n",
    "#         optimizer.pre_update_params()\n",
    "#         optimizer.update_params(dense1)\n",
    "#         optimizer.update_params(dense2)\n",
    "#         optimizer.post_update_params()\n",
    "        \n",
    "#         batch_losses.append(loss)\n",
    "#         batch_accuracies.append(accuracy)\n",
    "    \n",
    "#     # Calculate epoch-level training metrics\n",
    "#     epoch_loss = np.mean(batch_losses)\n",
    "#     epoch_accuracy = np.mean(batch_accuracies)\n",
    "#     train_losses.append(epoch_loss)\n",
    "#     train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "#     # Validation pass (entire validation dataset)\n",
    "#     dense1.forward(X_val)\n",
    "#     activation4.forward(dense1.output)\n",
    "#     dense2.forward(activation4.output)\n",
    "#     val_loss = loss_activation.forward(dense2.output, y_val)\n",
    "    \n",
    "#     # Calculate validation accuracy\n",
    "#     val_predictions = np.argmax(loss_activation.output, axis=1)\n",
    "#     if len(y_val.shape) == 2:\n",
    "#         y_val_true = np.argmax(y_val, axis=1)\n",
    "#     else:\n",
    "#         y_val_true = y_val\n",
    "#     val_accuracy = np.mean(val_predictions == y_val_true)\n",
    "    \n",
    "#     # Append validation metrics\n",
    "#     val_losses.append(val_loss)\n",
    "#     val_accuracies.append(val_accuracy)\n",
    "    \n",
    "#     # early_stopping.on_epoch_end(\n",
    "#     #     current_loss=val_loss,\n",
    "#     #     current_accuracy=val_accuracy,\n",
    "#     #     model=[dense1, dense2], \n",
    "#     #     epoch=epoch\n",
    "#     # )\n",
    "#     # if early_stopping.stop_training:\n",
    "#     #     print(f\"Early stopping at epoch {epoch}\")\n",
    "#     #     break\n",
    "    \n",
    "#     # Print progress\n",
    "#     if not epoch % 100:\n",
    "#         print(f\"epoch: {epoch}, \"\n",
    "#               f\"train_acc: {epoch_accuracy:.3f}, train_loss: {epoch_loss:.3f}, \"\n",
    "#               f\"val_acc: {val_accuracy:.3f}, val_loss: {val_loss:.3f}, \"\n",
    "#               f\"learning_rate: {optimizer.current_learning_rate}\")\n",
    "\n",
    "# plot_accuracies(train_accuracies, val_accuracies, label1=\"Training Accuracies\", label2=\"Validation Accuracies\", title=\"Accuracy Over Epochs\")\n",
    "# plot_accuracies(train_losses, val_losses, label1=\"Training Loss\", label2=\"Validation Loss\", title=\"Loss Over Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
