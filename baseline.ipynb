{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.optimizers import *\n",
    "from src.activation_functions import * \n",
    "from src.utils import *\n",
    "from src.model_regularization import *\n",
    "from src.layer import *\n",
    "from src.random_search import *\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data pre-processing for MONK Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "### Don't forget to change the path to the data file ###\n",
    "########################################################\n",
    "\n",
    "df = pd.read_csv(\"../ML_project/data/Monk_2/monks-2.train\", names=[0,1,2,3,4,5,6,\"index\"], delimiter= \" \")\n",
    "# df = pd.read_csv(\"../ML_project/data/Monk_2/monks-2.train\",\n",
    "#                  names=[0, 1, 2, 3, 4, 5, 6, \"index\"], delimiter=\" \")\n",
    "df.set_index(\"index\", inplace=True)\n",
    "y = df.iloc[:, 0]  # First column as target\n",
    "X = df.iloc[:, 1:]  # All other columns as features\n",
    "for i in range(1, X.shape[1]):\n",
    "    X.iloc[:, i] = (X.iloc[:, i] - np.mean(X.iloc[:, i])) / \\\n",
    "        np.std(X.iloc[:, i])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "### Don't forget to change the path to the data file ###\n",
    "########################################################\n",
    "\n",
    "test_data = pd.read_csv(\"../ML_project/data/Monk_2/monks-2.test\",\n",
    "names=[0, 1, 2, 3, 4, 5, 6, \"index\"], delimiter=\" \")\n",
    "test_data.set_index(\"index\", inplace=True)\n",
    "# test_data.head()\n",
    "y_test = test_data.iloc[:, 0]\n",
    "X_test = test_data.iloc[:, 1:]\n",
    "for i in range(1, X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = (X_test.iloc[:, i] - np.mean(X_test.iloc[:, i])) / np.std(X_test.iloc[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of the resulting datasets\n",
    "print(\"Training Features Shape:\", X_train.shape)\n",
    "print(\"Validation Features Shape:\", X_val.shape)\n",
    "print(\"Training Target Shape:\", y_train.shape)\n",
    "print(\"Validation Target Shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidence = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidence = np.sum(y_pred_clipped * y_true, axis=1) \n",
    "\n",
    "        negative_log_likelihoods = np.log(correct_confidence)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class MSE:\n",
    "    def __init__(self):\n",
    "        self.dinputs = 0\n",
    "        self.loss = 0\n",
    "        self.output = 0\n",
    "        \n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.output = np.mean((y_pred - y_true)**2)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, l1, l2, input_size, hidden_sizes, output_size, \n",
    "                 hidden_activations=None, dropout_rates=None):\n",
    "        self.layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Default to ReLU if no activations specified\n",
    "        if hidden_activations is None:\n",
    "            hidden_activations = [Activation_ReLU() for _ in hidden_sizes]\n",
    "        \n",
    "        # Default to no dropout\n",
    "        if dropout_rates is None:\n",
    "            dropout_rates = [0.0] * len(hidden_sizes)\n",
    "            \n",
    "        # Create hidden layers\n",
    "        for size, activation, rate in zip(hidden_sizes, hidden_activations, dropout_rates):\n",
    "            self.layers.append(Layer_Dense(prev_size, size,l1=l1, l2=l2))\n",
    "            self.layers.append(activation())\n",
    "            if rate > 0:\n",
    "                self.layers.append(Dropout(rate))\n",
    "            prev_size = size\n",
    "        \n",
    "        # Output layer (no activation)\n",
    "        self.layers.append(Layer_Dense(prev_size, output_size))\n",
    "        \n",
    "    def forward(self, inputs, training=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                layer.forward(inputs, training)\n",
    "            else:\n",
    "                layer.forward(inputs)\n",
    "            inputs = layer.output\n",
    "        self.output = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, activation):\n",
    "    # Initialize components\n",
    "    model = NN(\n",
    "        l1=l1,\n",
    "        l2=l2,\n",
    "        input_size=6,\n",
    "        hidden_sizes=[10],\n",
    "        output_size=2,\n",
    "        hidden_activations=[activation],\n",
    "        dropout_rates=[dropout_rate]\n",
    "    )\n",
    "    \n",
    "    loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "    optimizer = Optimizer_Adam(learning_rate=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        batch_losses = []\n",
    "        batch_accuracies = []\n",
    "        \n",
    "        for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "            # Forward pass through model\n",
    "            model.forward(X_batch, training=True)\n",
    "            \n",
    "            # Calculate loss through separate loss activation\n",
    "            loss = loss_activation.forward(model.output, y_batch)\n",
    "            # print(y_batch.shape)\n",
    "            # Calculate accuracy\n",
    "            predictions = np.argmax(loss_activation.output, axis=1)\n",
    "            accuracy = np.mean(predictions == y_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss_activation.backward(loss_activation.output, y_batch)\n",
    "            dvalues = loss_activation.dinputs\n",
    "            \n",
    "            # Propagate gradients through model layers in reverse\n",
    "            for layer in reversed(model.layers):\n",
    "                layer.backward(dvalues)\n",
    "                dvalues = layer.dinputs\n",
    "                \n",
    "                # # Apply L1/L2 regularization to dense layers\n",
    "                # if isinstance(layer, Layer_Dense):\n",
    "                #     if layer.l1 > 0:\n",
    "                #         layer.dweights += layer.l1 * np.sign(layer.weights)\n",
    "                #     if layer.l2 > 0:\n",
    "                #         layer.dweights += 2 * layer.l2 * layer.weights\n",
    "            \n",
    "            # Update parameters\n",
    "            optimizer.pre_update_params()\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, Layer_Dense):\n",
    "                    optimizer.update_params(layer)\n",
    "            optimizer.post_update_params()\n",
    "            \n",
    "            batch_losses.append(loss)\n",
    "            batch_accuracies.append(accuracy)\n",
    "\n",
    "        # Epoch metrics\n",
    "        epoch_loss = np.mean(batch_losses)\n",
    "        epoch_accuracy = np.mean(batch_accuracies)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "        # Validation pass\n",
    "        model.forward(X_val, training=False)\n",
    "        val_loss = loss_activation.forward(model.output, y_val)\n",
    "        val_predictions = np.argmax(loss_activation.output, axis=1)\n",
    "        val_accuracy = np.mean(val_predictions == y_val)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    return val_accuracies[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is grid search. had to comment it.\n",
    "\n",
    "# hyperparameter_grid = {\n",
    "#     'learning_rate': [0.001],\n",
    "#     'l1': [0.0, 1e-5],\n",
    "#     'l2': [0.0, 1e-4],\n",
    "#     'dropout_rate': [0.1, 0.5],\n",
    "#     'batch_size': [4],\n",
    "#     'n_epochs': [100, 200],\n",
    "#     'activation': [Activation_Sigmoid, Activation_Leaky_ReLU]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Random search for finding the best hyperparameters\n",
    "# def random_search(param_distributions, n_iters):\n",
    "#     best_hyperparams = None\n",
    "#     best_performance = -np.inf\n",
    "    \n",
    "#     for _ in range(n_iters):\n",
    "#         # Let's have sample hyperparameters from distributions\n",
    "#         params = {\n",
    "#             'learning_rate': random.choice(param_distributions['learning_rate']),\n",
    "#             'l1': random.choice(param_distributions['l1']),\n",
    "#             'l2': random.choice(param_distributions['l2']),\n",
    "#             'dropout_rate': random.choice(param_distributions['dropout_rate']),\n",
    "#             'batch_size': random.choice(param_distributions['batch_size']),\n",
    "#             'n_epochs': random.choice(param_distributions['n_epochs']),\n",
    "#             'activation': random.choice(param_distributions['activation']),\n",
    "#         }\n",
    "#         # We train and evaluate the model\n",
    "#         val_accuracy = train_and_evaluate(**params)\n",
    "        \n",
    "#         # Update the hyperparamters if the current model is doing great\n",
    "#         if val_accuracy > best_performance:\n",
    "#             best_performance = val_accuracy\n",
    "#             best_hyperparams = params\n",
    "#     return best_hyperparams, best_performance\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter distributions for random search\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'l1': [0.0, 1e-5, 1e-4],\n",
    "    'l2': [0.0, 1e-5, 1e-4],\n",
    "    'dropout_rate': [0.0, 0.1, 0.2, 0.5],\n",
    "    'batch_size': [2, 4, 8, 16],\n",
    "    'n_epochs': [50, 100, 150, 200],\n",
    "    'activation': [Activation_Sigmoid, Activation_Leaky_ReLU, Activation_ReLU]\n",
    "}\n",
    "\n",
    "# Define the hyperparameter distributions for random search\n",
    "param_distributions = {\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2, 1e-1],  # Logarithmic scale\n",
    "    'l1': [0.0, 1e-5, 1e-4, 1e-3, 1e-2],  # Wider range\n",
    "    'l2': [0.0, 1e-5, 1e-4, 1e-3, 1e-2],  # Wider range\n",
    "    'dropout_rate': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],  # Wider range\n",
    "    'batch_size': [2, 4, 8, 16, 32],  # Wider range\n",
    "    'n_epochs': [50, 100, 150, 200, 300],  # Wider range\n",
    "    'activation': [Activation_Sigmoid, Activation_Leaky_ReLU, Activation_ReLU]\n",
    "}\n",
    "\n",
    "# We then run random search algorithm\n",
    "n_iters = 50  # Number of random combinations we can try\n",
    "best_hyperparams, best_performance = random_search(\n",
    "    param_distributions, n_iters)\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "print(\"Best Validation Accuracy:\", best_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize variables to store the best hyperparameters and performance\n",
    "# best_hyperparams = None\n",
    "# best_performance = -np.inf  # Assuming we are maximizing validation accuracy\n",
    "\n",
    "# # Iterate over all combinations of hyperparameters\n",
    "# for params in product(*hyperparameter_grid.values()):\n",
    "#     # Unpack the hyperparameters\n",
    "#     learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, activation = params\n",
    "#     # Train and evaluate the model\n",
    "#     val_accuracy = train_and_evaluate(learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, activation)\n",
    "    \n",
    "#     # Update the best hyperparameters if the current model is better\n",
    "#     if val_accuracy > best_performance:\n",
    "#         best_performance = val_accuracy\n",
    "#         best_hyperparams = {\n",
    "#             'learning_rate': learning_rate,\n",
    "#             'l1': l1,\n",
    "#             'l2': l2,\n",
    "#             'dropout_rate': dropout_rate,\n",
    "#             'batch_size': batch_size,\n",
    "#             'n_epochs': n_epochs,\n",
    "#             'activation': activation\n",
    "#         }\n",
    "\n",
    "# # Print the best hyperparameters and performance\n",
    "# print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "# print(\"Best Validation Accuracy:\", best_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model_performance = train_and_evaluate(**best_hyperparams)\n",
    "# print(\"Final Model Performance:\", final_model_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, activation = best_hyperparams.values()\n",
    "learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(\n",
    "    l1=l1,\n",
    "    l2=l2,\n",
    "    input_size=6,\n",
    "    hidden_sizes=[10],\n",
    "    output_size=2,\n",
    "    hidden_activations=[activation],\n",
    "    dropout_rates=[dropout_rate]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     patience=20,\n",
    "#     min_delta_loss=0.0001,\n",
    "#     min_delta_accuracy=0.0001,\n",
    "#     restore_best_weights=True\n",
    "# )\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "optimizer = Optimizer_Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    batch_losses = []\n",
    "    batch_accuracies = []\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "    # Forward pass through model\n",
    "        model.forward(X_batch, training=True)\n",
    "        \n",
    "        # Calculate loss through separate loss activation\n",
    "        loss = loss_activation.forward(model.output, y_batch)\n",
    "        # print(y_batch.shape)\n",
    "        # Calculate accuracy\n",
    "        predictions = np.argmax(loss_activation.output, axis=1)\n",
    "        accuracy = np.mean(predictions == y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        loss_activation.backward(loss_activation.output, y_batch)\n",
    "        dvalues = loss_activation.dinputs\n",
    "\n",
    "        # Propagate gradients through model layers in reverse\n",
    "        for layer in reversed(model.layers):\n",
    "            layer.backward(dvalues)\n",
    "            dvalues = layer.dinputs\n",
    "\n",
    "            # # Apply L1/L2 regularization to dense layers\n",
    "            # if isinstance(layer, Layer_Dense):\n",
    "            #     if layer.l1 > 0:\n",
    "            #         layer.dweights += layer.l1 * np.sign(layer.weights)\n",
    "            #     if layer.l2 > 0:\n",
    "            #         layer.dweights += 2 * layer.l2 * layer.weights\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.pre_update_params()\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, Layer_Dense):\n",
    "                optimizer.update_params(layer)\n",
    "        optimizer.post_update_params()\n",
    "\n",
    "        batch_losses.append(loss)\n",
    "        batch_accuracies.append(accuracy)\n",
    "\n",
    "    epoch_loss = np.mean(batch_losses)\n",
    "    epoch_accuracy = np.mean(batch_accuracies)\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "    # Validation pass\n",
    "    model.forward(X_val, training=False)\n",
    "    val_loss = loss_activation.forward(model.output, y_val)\n",
    "    val_predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    val_accuracy = np.mean(val_predictions == y_val)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    # Calculate validation accuracy\n",
    "    val_predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y_val.shape) == 2:\n",
    "        y_val_true = np.argmax(y_val, axis=1)\n",
    "    else:\n",
    "        y_val_true = y_val\n",
    "    val_accuracy = np.mean(val_predictions == y_val_true)\n",
    "    \n",
    "    \n",
    "    # early_stopping.on_epoch_end(\n",
    "    #     current_loss=val_loss,\n",
    "    #     current_accuracy=val_accuracy,\n",
    "    #     model=[dense1, dense2], \n",
    "    #     epoch=epoch\n",
    "    # )\n",
    "    # if early_stopping.stop_training:\n",
    "    #     print(f\"Early stopping at epoch {epoch}\")\n",
    "    #     break\n",
    "\n",
    "    # Print progress\n",
    "    if not epoch % 100:\n",
    "        print(f\"epoch: {epoch}, \"\n",
    "              f\"train_acc: {epoch_accuracy:.3f}, train_loss: {epoch_loss:.3f}, \"\n",
    "              f\"val_acc: {val_accuracy:.3f}, val_loss: {val_loss:.3f}, \"\n",
    "              f\"learning_rate: {optimizer.current_learning_rate}\")\n",
    "print(len(val_accuracies), len(train_accuracies))\n",
    "plot_accuracies(train_accuracies, val_accuracies, label1=\"Training Accuracies\", label2=\"Validation Accuracies\", title=\"Accuracy Over Epochs\")\n",
    "plot_accuracies(train_losses, val_losses, label1=\"Training Loss\", label2=\"Validation Loss\", title=\"Loss Over Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(X_test, training=False)\n",
    "# Compute softmax probabilities for the test output\n",
    "print(X_test.shape, y_test.shape)\n",
    "loss_activation.forward(model.output, y_test) \n",
    "# Calculate accuracy for the test set\n",
    "predictions = np.argmax(loss_activation.output, axis=1)\n",
    "if len(y_test.shape) == 2:\n",
    "    y_true = np.argmax(y_test, axis=1) \n",
    "else:\n",
    "    print(\"we go here\")\n",
    "    y_true = y_test\n",
    "\n",
    "# Compute test accuracy\n",
    "test_accuracy = np.mean(predictions == y_true)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 350\n",
    "# batch_size = 1\n",
    "\n",
    "\n",
    "# train_losses = []\n",
    "# train_accuracies = []\n",
    "# val_losses = []\n",
    "# val_accuracies = []\n",
    "\n",
    "# # early_stopping = EarlyStopping(\n",
    "# #     patience=20,\n",
    "# #     min_delta_loss=0.0001,\n",
    "# #     min_delta_accuracy=0.0001,\n",
    "# #     restore_best_weights=True\n",
    "# # )\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(n_epochs):\n",
    "#     batch_losses = []\n",
    "#     batch_accuracies = []\n",
    "    \n",
    "#     # Mini-batch training\n",
    "#     for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "#         # Forward pass\n",
    "#         dense1.forward(X_batch)\n",
    "#         activation4.forward(dense1.output)\n",
    "#         dense2.forward(activation4.output)\n",
    "#         loss = loss_activation.forward(dense2.output, y_batch)\n",
    "        \n",
    "#         # Calculate accuracy for this batch\n",
    "#         predictions = np.argmax(loss_activation.output, axis=1)\n",
    "#         if len(y_batch.shape) == 2:\n",
    "#             y_true = np.argmax(y_batch, axis=1)\n",
    "#         else:\n",
    "#             y_true = y_batch\n",
    "#         accuracy = np.mean(predictions == y_true)\n",
    "        \n",
    "#         # Backward pass\n",
    "#         loss_activation.backward(loss_activation.output, y_batch)\n",
    "#         dense2.backward(loss_activation.dinputs)\n",
    "#         activation4.backward(dense2.dinputs)\n",
    "#         dense1.backward(activation4.dinputs)\n",
    "        \n",
    "#         # Update weights and biases\n",
    "#         optimizer.pre_update_params()\n",
    "#         optimizer.update_params(dense1)\n",
    "#         optimizer.update_params(dense2)\n",
    "#         optimizer.post_update_params()\n",
    "        \n",
    "#         batch_losses.append(loss)\n",
    "#         batch_accuracies.append(accuracy)\n",
    "    \n",
    "#     # Calculate epoch-level training metrics\n",
    "#     epoch_loss = np.mean(batch_losses)\n",
    "#     epoch_accuracy = np.mean(batch_accuracies)\n",
    "#     train_losses.append(epoch_loss)\n",
    "#     train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "#     # Validation pass (entire validation dataset)\n",
    "#     dense1.forward(X_val)\n",
    "#     activation4.forward(dense1.output)\n",
    "#     dense2.forward(activation4.output)\n",
    "#     val_loss = loss_activation.forward(dense2.output, y_val)\n",
    "    \n",
    "#     # Calculate validation accuracy\n",
    "#     val_predictions = np.argmax(loss_activation.output, axis=1)\n",
    "#     if len(y_val.shape) == 2:\n",
    "#         y_val_true = np.argmax(y_val, axis=1)\n",
    "#     else:\n",
    "#         y_val_true = y_val\n",
    "#     val_accuracy = np.mean(val_predictions == y_val_true)\n",
    "    \n",
    "#     # Append validation metrics\n",
    "#     val_losses.append(val_loss)\n",
    "#     val_accuracies.append(val_accuracy)\n",
    "    \n",
    "#     # early_stopping.on_epoch_end(\n",
    "#     #     current_loss=val_loss,\n",
    "#     #     current_accuracy=val_accuracy,\n",
    "#     #     model=[dense1, dense2], \n",
    "#     #     epoch=epoch\n",
    "#     # )\n",
    "#     # if early_stopping.stop_training:\n",
    "#     #     print(f\"Early stopping at epoch {epoch}\")\n",
    "#     #     break\n",
    "    \n",
    "#     # Print progress\n",
    "#     if not epoch % 100:\n",
    "#         print(f\"epoch: {epoch}, \"\n",
    "#               f\"train_acc: {epoch_accuracy:.3f}, train_loss: {epoch_loss:.3f}, \"\n",
    "#               f\"val_acc: {val_accuracy:.3f}, val_loss: {val_loss:.3f}, \"\n",
    "#               f\"learning_rate: {optimizer.current_learning_rate}\")\n",
    "\n",
    "# plot_accuracies(train_accuracies, val_accuracies, label1=\"Training Accuracies\", label2=\"Validation Accuracies\", title=\"Accuracy Over Epochs\")\n",
    "# plot_accuracies(train_losses, val_losses, label1=\"Training Loss\", label2=\"Validation Loss\", title=\"Loss Over Epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
