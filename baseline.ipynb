{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.optimizers import *\n",
    "from src.activation_functions import * \n",
    "from src.utils import *\n",
    "from src.layer import *\n",
    "from src.model import NN\n",
    "from src.trainer import Trainer\n",
    "from src.losses import *\n",
    "\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data pre-processing for MONK Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "### Don't forget to change the path to the data file ###\n",
    "########################################################\n",
    "monk = \"1\"\n",
    "df = pd.read_csv(f\"../ML_project/data/Monk_{monk}/monks-{monk}.train\", names=[0,1,2,3,4,5,6,\"index\"], delimiter= \" \")\n",
    "# df = pd.read_csv(\"../ML_project/data/Monk_2/monks-2.train\",\n",
    "#                  names=[0, 1, 2, 3, 4, 5, 6, \"index\"], delimiter=\" \")\n",
    "df.set_index(\"index\", inplace=True)\n",
    "y = df.iloc[:, 0]  # First column as target\n",
    "X = df.iloc[:, 1:]  # All other columns as features\n",
    "for i in range(1, X.shape[1]):\n",
    "    X.iloc[:, i] = (X.iloc[:, i] - np.mean(X.iloc[:, i])) / \\\n",
    "        np.std(X.iloc[:, i])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "### Don't forget to change the path to the data file ###\n",
    "########################################################\n",
    "\n",
    "test_data = pd.read_csv(f\"../ML_project/data/Monk_{monk}/monks-{monk}.test\",\n",
    "names=[0, 1, 2, 3, 4, 5, 6, \"index\"], delimiter=\" \")\n",
    "test_data.set_index(\"index\", inplace=True)\n",
    "# test_data.head()\n",
    "y_test = test_data.iloc[:, 0]\n",
    "X_test = test_data.iloc[:, 1:]\n",
    "for i in range(1, X_test.shape[1]):\n",
    "    X_test.iloc[:, i] = (X_test.iloc[:, i] - np.mean(X_test.iloc[:, i])) / np.std(X_test.iloc[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of the resulting datasets\n",
    "print(\"Training Features Shape:\", X_train.shape)\n",
    "print(\"Validation Features Shape:\", X_val.shape)\n",
    "print(\"Training Target Shape:\", y_train.shape)\n",
    "print(\"Validation Target Shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(learning_rate: float, l1: float, l2: float, dropout_rate: float, batch_size: int, n_epochs: int, activation: Activation):\n",
    "    # Initialize components\n",
    "    model = NN(\n",
    "        l1=l1,\n",
    "        l2=l2,\n",
    "        input_size=6,\n",
    "        hidden_sizes=[10, 5, 2],\n",
    "        output_size=1,\n",
    "        hidden_activations=[activation],\n",
    "        dropout_rates=[dropout_rate],\n",
    "        output_activation=Activation_Sigmoid()\n",
    "    )\n",
    "    \n",
    "    loss_function = MSE()\n",
    "    optimizer = Optimizer_Adam(learning_rate=learning_rate, decay=1e-3)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        batch_losses = []\n",
    "        batch_accuracies = []\n",
    "        \n",
    "        for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "            # Forward pass\n",
    "            output = model.forward(X_batch, training=True)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_function.forward(model.output, y_batch)\n",
    "            \n",
    "            predictions = np.round(model.output.squeeze())\n",
    "            accuracy = np.mean(predictions == y_batch.squeeze())\n",
    "\n",
    "            # Backward pass with shape validation\n",
    "            dvalues = loss_function.backward(model.output, y_batch)\n",
    "              \n",
    "            # Verify gradient shape matches output\n",
    "            assert dvalues.shape == model.output.shape, \\\n",
    "                f\"Gradient shape mismatch: {dvalues.shape} vs {model.output.shape}\"\n",
    "            \n",
    "            # Propagate gradients\n",
    "            for layer in reversed(model.layers):\n",
    "                \n",
    "                dvalues = layer.backward(dvalues)\n",
    "                \n",
    "                # Ensure numpy arrays\n",
    "                if isinstance(dvalues, pd.DataFrame):\n",
    "                    dvalues = dvalues.values\n",
    "                elif isinstance(dvalues, pd.Series):\n",
    "                    dvalues = dvalues.values.reshape(-1, 1)\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.pre_update_params()\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, Layer_Dense):\n",
    "                    optimizer.update_params(layer)\n",
    "            optimizer.post_update_params()\n",
    "            \n",
    "            batch_losses.append(loss)\n",
    "            batch_accuracies.append(accuracy)\n",
    "\n",
    "        # Epoch metrics\n",
    "        epoch_loss = np.mean(batch_losses)\n",
    "        epoch_acc = np.mean(batch_accuracies)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "\n",
    "        # Validation pass\n",
    "        model.forward(X_val.values if isinstance(X_val, pd.DataFrame) else X_val, \n",
    "                     training=False)\n",
    "        val_loss = loss_function.forward(model.output, \n",
    "                                        y_val.values if isinstance(y_val, (pd.Series, pd.DataFrame)) else y_val)\n",
    "        val_predictions = np.round(model.output.squeeze())\n",
    "        val_accuracy = np.mean(val_predictions == y_val.squeeze())\n",
    "        \n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: \"\n",
    "                  f\"Train Loss: {epoch_loss:.4f}, Acc: {epoch_acc*100:.2f}% | \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Acc: {val_accuracy*100:.2f}%\")\n",
    "    # print(val_accuracies, val_accuracies[-1])\n",
    "    return val_losses[-1], val_accuracies[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_grid = {\n",
    "    'learning_rate': [0.001],\n",
    "    'l1': [0.0000001],\n",
    "    'l2': [0.00001],\n",
    "    'dropout_rate': [0],\n",
    "    'batch_size': [1,],\n",
    "    'n_epochs': [300],\n",
    "    'patience': [100],\n",
    "    'hidden_sizess': [[3, 2]],\n",
    "    'output_size': [1],\n",
    "    'activation': [Activation_Sigmoid()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to store the best hyperparameters and performance\n",
    "best_hyperparams = []\n",
    "best_performance = (-np.inf, -np.inf)  # Assuming we are maximizing validation accuracy\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for params in product(*hyperparameter_grid.values()):\n",
    "    # Unpack the hyperparameters\n",
    "    learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, patience, hidden_sizes, output_size, activation = params\n",
    "    best_hyperparams.append({\n",
    "            'learning_rate': learning_rate,\n",
    "            'l1': l1,\n",
    "            'l2': l2,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'n_epochs': n_epochs,\n",
    "            'patience': patience,\n",
    "            'hidden_sizes': hidden_sizes,\n",
    "            'output_size': output_size,\n",
    "            'activation': activation\n",
    "        })\n",
    "    break\n",
    "    # Train and evaluate the model\n",
    "    val_accuracy = train_and_evaluate(learning_rate, l1, l2, dropout_rate, batch_size, n_epochs, activation)\n",
    "    # Update the best hyperparameters if the current model is better\n",
    "    if val_accuracy[1] > best_performance[1]:\n",
    "        best_hyperparams.clear()\n",
    "        best_performance = val_accuracy\n",
    "        best_hyperparams.append({\n",
    "            'learning_rate': learning_rate,\n",
    "            'l1': l1,\n",
    "            'l2': l2,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'n_epochs': n_epochs,\n",
    "            'activation': activation\n",
    "        })\n",
    "    elif val_accuracy[1] == 1.0:\n",
    "        best_hyperparams.append({\n",
    "            'learning_rate': learning_rate,\n",
    "            'l1': l1,\n",
    "            'l2': l2,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'n_epochs': n_epochs,\n",
    "            'activation': activation\n",
    "        })\n",
    "\n",
    "# Print the best hyperparameters and performance\n",
    "print(\"Best Hyperparameters:\", best_hyperparams)\n",
    "print(\"Best Validation Accuracy:\", best_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = {\n",
    "    \"X_train\": X_train,\n",
    "    \"y_train\": y_train,\n",
    "    \"X_val\": X_val,\n",
    "    \"y_val\": y_val,\n",
    "    \"X_test\": X_test,\n",
    "    \"y_test\": y_test\n",
    "}\n",
    "\n",
    "trainer = Trainer(best_hyperparams[0], sets, Loss_CategoricalCrossentropy())\n",
    "metrics = trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics[\"test_loss\"])\n",
    "print(metrics[\"test_accuracy\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics[\"validation_accuracy\"])\n",
    "print(np.argmax(metrics[\"validation_accuracy\"]))\n",
    "print(len(metrics[\"validation_accuracy\"]))\n",
    "print(best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, _ = trainer.test()\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(0.1, 1e-05, 0.0, 0.3, 16, 100, src.activation_functions.Activation_Leaky_ReLU) MONK2 Cross entropy ADAM\n",
    "\n",
    "(0.1, 0.0, 0.0, 0.1, 16, 200, src.activation_functions.Activation_Tanh) MONK3 Cross entropy ADAM\n",
    "test_result: 0.9144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MONK1:\n",
    "    \n",
    "    \n",
    "MONK2:\n",
    "\n",
    "\n",
    "\n",
    "MONK3:\n",
    "Cross Entropy: \n",
    "(0.1, 0.0, 0.0, 0.1, 1, 100, src.activation_functions.Activation_Sigmoid) RMSprop\n",
    "Test Accuracy: 0.9213\n",
    "(0.1, 0.0, 0.0, 0.1, 1, 400, src.activation_functions.Activation_Sigmoid) Adam\n",
    "Test Accuracy: 0.9606\n",
    "\n",
    "MSE: \n",
    "{'learning_rate': 0.1, 'l1': 1e-05, 'l2': 0.0001, 'dropout_rate': 0.1, 'batch_size': 4, 'n_epochs': 50, 'activation': <class 'src.activation_functions.Activation_Tanh'>} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_epochs = 350\n",
    "# batch_size = 1\n",
    "\n",
    "\n",
    "# train_losses = []\n",
    "# train_accuracies = []\n",
    "# val_losses = []\n",
    "# val_accuracies = []\n",
    "\n",
    "# # early_stopping = EarlyStopping(\n",
    "# #     patience=20,\n",
    "# #     min_delta_loss=0.0001,\n",
    "# #     min_delta_accuracy=0.0001,\n",
    "# #     restore_best_weights=True\n",
    "# # )\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(n_epochs):\n",
    "#     batch_losses = []\n",
    "#     batch_accuracies = []\n",
    "    \n",
    "#     # Mini-batch training\n",
    "#     for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "#         # Forward pass\n",
    "#         dense1.forward(X_batch)\n",
    "#         activation4.forward(dense1.output)\n",
    "#         dense2.forward(activation4.output)\n",
    "#         loss = loss_activation.forward(dense2.output, y_batch)\n",
    "        \n",
    "#         # Calculate accuracy for this batch\n",
    "#         predictions = np.argmax(loss_activation.output, axis=1)\n",
    "#         if len(y_batch.shape) == 2:\n",
    "#             y_true = np.argmax(y_batch, axis=1)\n",
    "#         else:\n",
    "#             y_true = y_batch\n",
    "#         accuracy = np.mean(predictions == y_true)\n",
    "        \n",
    "#         # Backward pass\n",
    "#         loss_activation.backward(loss_activation.output, y_batch)\n",
    "#         dense2.backward(loss_activation.dinputs)\n",
    "#         activation4.backward(dense2.dinputs)\n",
    "#         dense1.backward(activation4.dinputs)\n",
    "        \n",
    "#         # Update weights and biases\n",
    "#         optimizer.pre_update_params()\n",
    "#         optimizer.update_params(dense1)\n",
    "#         optimizer.update_params(dense2)\n",
    "#         optimizer.post_update_params()\n",
    "        \n",
    "#         batch_losses.append(loss)\n",
    "#         batch_accuracies.append(accuracy)\n",
    "    \n",
    "#     # Calculate epoch-level training metrics\n",
    "#     epoch_loss = np.mean(batch_losses)\n",
    "#     epoch_accuracy = np.mean(batch_accuracies)\n",
    "#     train_losses.append(epoch_loss)\n",
    "#     train_accuracies.append(epoch_accuracy)\n",
    "\n",
    "#     # Validation pass (entire validation dataset)\n",
    "#     dense1.forward(X_val)\n",
    "#     activation4.forward(dense1.output)\n",
    "#     dense2.forward(activation4.output)\n",
    "#     val_loss = loss_activation.forward(dense2.output, y_val)\n",
    "    \n",
    "#     # Calculate validation accuracy\n",
    "#     val_predictions = np.argmax(loss_activation.output, axis=1)\n",
    "#     if len(y_val.shape) == 2:\n",
    "#         y_val_true = np.argmax(y_val, axis=1)\n",
    "#     else:\n",
    "#         y_val_true = y_val\n",
    "#     val_accuracy = np.mean(val_predictions == y_val_true)\n",
    "    \n",
    "#     # Append validation metrics\n",
    "#     val_losses.append(val_loss)\n",
    "#     val_accuracies.append(val_accuracy)\n",
    "    \n",
    "#     # early_stopping.on_epoch_end(\n",
    "#     #     current_loss=val_loss,\n",
    "#     #     current_accuracy=val_accuracy,\n",
    "#     #     model=[dense1, dense2], \n",
    "#     #     epoch=epoch\n",
    "#     # )\n",
    "#     # if early_stopping.stop_training:\n",
    "#     #     print(f\"Early stopping at epoch {epoch}\")\n",
    "#     #     break\n",
    "    \n",
    "#     # Print progress\n",
    "#     if not epoch % 100:\n",
    "#         print(f\"epoch: {epoch}, \"\n",
    "#               f\"train_acc: {epoch_accuracy:.3f}, train_loss: {epoch_loss:.3f}, \"\n",
    "#               f\"val_acc: {val_accuracy:.3f}, val_loss: {val_loss:.3f}, \"\n",
    "#               f\"learning_rate: {optimizer.current_learning_rate}\")\n",
    "\n",
    "# plot_accuracies(train_accuracies, val_accuracies, label1=\"Training Accuracies\", label2=\"Validation Accuracies\", title=\"Accuracy Over Epochs\")\n",
    "# plot_accuracies(train_losses, val_losses, label1=\"Training Loss\", label2=\"Validation Loss\", title=\"Loss Over Epochs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
