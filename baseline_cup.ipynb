{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "src_path = os.path.join(os.getcwd(), 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.optimizers import *\n",
    "from src.activation_functions import * \n",
    "from src.utils import *\n",
    "from src.model_regularization import *\n",
    "from src.layer import *\n",
    "from src.batch_normalization import *\n",
    "np.random.seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data pre-processing for CUP DATASET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"../ML_project/data/cup/ML-CUP24-TR.csv\"\n",
    "df = pd.read_csv(filepath, skiprows=7, header=None)\n",
    "df.columns = [\"ID\"] + [f'input_{i}' for i in range(12)] + ['target_x', 'target_y', 'target_z']\n",
    "df = df.drop('ID', axis=1)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(df, train_percent=80, target=['target_x', 'target_y', 'target_z'])\n",
    "X_train = normalize(data=X_train, type='z-score')\n",
    "X_validation = normalize(data=X_validation, type='z-score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test.shape:  (500, 12)\n"
     ]
    }
   ],
   "source": [
    "filepath = \"../ML_project/data/cup/ML-CUP24-TS.csv\"\n",
    "df = pd.read_csv(filepath, skiprows=7, header=None)\n",
    "df.columns = [\"ID\"] + [f'input_{i}' for i in range(12)]\n",
    "df = df.drop('ID', axis=1)\n",
    "X_test = df.iloc[:, :12]  \n",
    "X_test = normalize(data=X_test, type='z-score')\n",
    "print(\"X_test.shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (200, 12)\n",
      "Validation Features Shape: (50, 12)\n",
      "Training Target Shape: (200, 3)\n",
      "Validation Target Shape: (50, 3)\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Print the shape of the resulting datasets\n",
    "print(\"Training Features Shape:\", X_train.shape)\n",
    "print(\"Validation Features Shape:\", X_validation.shape)\n",
    "print(\"Training Target Shape:\", y_train.shape)\n",
    "print(\"Validation Target Shape:\", y_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, l1, l2, input_size, hidden_sizes, output_size, \n",
    "                 hidden_activations=None, dropout_rates=None, \n",
    "                 use_batch_norm=None, output_activation=Activation_Sigmoid()):\n",
    "        self.layers = []\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Default activations to ReLU\n",
    "        if hidden_activations is None:\n",
    "            hidden_activations = [Activation_ReLU() for _ in hidden_sizes]\n",
    "\n",
    "        # Default dropout rates to 0\n",
    "        if dropout_rates is None:\n",
    "            dropout_rates = [0.0] * len(hidden_sizes)\n",
    "\n",
    "        # Default batch_norm to False for all layers\n",
    "        if use_batch_norm is None:\n",
    "            use_batch_norm = [False] * len(hidden_sizes)\n",
    "        else:\n",
    "            assert len(use_batch_norm) == len(hidden_sizes), \\\n",
    "                \"use_batch_norm must have the same length as hidden_sizes\"\n",
    "\n",
    "        # Create hidden layers\n",
    "        for size, activation, rate, bn_flag in zip(hidden_sizes, hidden_activations, \n",
    "                                                 dropout_rates, use_batch_norm):\n",
    "            # Add dense layer\n",
    "            self.layers.append(Layer_Dense(prev_size, size, l1=l1, l2=l2))\n",
    "            \n",
    "            # Add batch normalization if specified\n",
    "            if bn_flag:\n",
    "                self.layers.append(BatchNormalization())\n",
    "            \n",
    "            # Add activation\n",
    "            self.layers.append(activation())\n",
    "            \n",
    "            # Add dropout if rate > 0\n",
    "            if rate > 0:\n",
    "                self.layers.append(Dropout(rate))\n",
    "            \n",
    "            prev_size = size\n",
    "\n",
    "        # Output layer\n",
    "        self.layers.append(Layer_Dense(prev_size, output_size))\n",
    "        # self.layers.append(output_activation)\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        for layer in self.layers:\n",
    "            # Pass training flag to relevant layers\n",
    "            if isinstance(layer, (Dropout, BatchNormalization)):\n",
    "                layer.forward(inputs, training=training)\n",
    "            else:\n",
    "                layer.forward(inputs)\n",
    "            inputs = layer.output\n",
    "        self.output = inputs\n",
    "        return self.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidence = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidence = np.sum(y_pred_clipped * y_true, axis=1) \n",
    "\n",
    "        negative_log_likelihoods = np.log(correct_confidence)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class MSE:\n",
    "    def __init__(self):\n",
    "        self.dinputs = 0\n",
    "        self.loss = 0\n",
    "        self.output = 0\n",
    "        \n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.output = np.mean((y_pred - y_true)**2)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        self.dinputs = -2 * (y_true - dvalues) / outputs\n",
    "        self.dinputs = self.dinputs / samples\n",
    "        \n",
    "class MEE:\n",
    "    def __init__(self):\n",
    "        self.dinputs = 0\n",
    "        self.loss = 0\n",
    "        self.output = 0\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        self.output = np.mean(np.sqrt(np.sum((y_pred - y_true)**2, axis=1)))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples and outputs\n",
    "        samples = len(dvalues)\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        differences = dvalues - y_true\n",
    "        euclidean_distances = np.sqrt(np.sum(differences**2, axis=1, keepdims=True))\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        euclidean_distances = np.maximum(euclidean_distances, 1e-7)\n",
    "\n",
    "        self.dinputs = differences / euclidean_distances\n",
    "        \n",
    "        # Normalize by number of samples and outputs\n",
    "        self.dinputs = self.dinputs / (samples * outputs)\n",
    "        \n",
    "        return self.dinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'l1': [0.0, 0.01],\n",
    "    'l2': [0.0, 0.01],\n",
    "    'dropout_rate': [0.0, 0.3],\n",
    "    'batch_size': [32],\n",
    "    'n_epochs': [100],\n",
    "    'hidden_sizes': [[12, 10]],\n",
    "    'activations': [Activation_ReLU, Activation_Leaky_ReLU]\n",
    "}\n",
    "\n",
    "# Generate parameter combinations\n",
    "keys = param_grid.keys()\n",
    "values = param_grid.values()\n",
    "param_combinations = [dict(zip(keys, combo)) for combo in itertools.product(*values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "# y = y.values if isinstance(y, (pd.Series, pd.DataFrame)) else y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(l1=0.0001,\n",
    "    l2=0.000001,\n",
    "    input_size=12,\n",
    "    hidden_sizes=[32, 16, 8],\n",
    "    output_size=3,\n",
    "    hidden_activations=[Activation_ReLU, Activation_Leaky_ReLU, Activation_ReLU, Activation_Leaky_ReLU],\n",
    "    dropout_rates=[0.2, 0.1], \n",
    "    use_batch_norm=[True,True, True],\n",
    ")\n",
    "\n",
    "batch_size = 10000\n",
    "learning_rate = 0.001\n",
    "n_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 LR: 0.001000\n",
      "Gradient norms: [3.6264056030837946, 3.540756607943742, 5.4004822975800275]\n",
      "61.028810551574615\n",
      "Validation MSE: 62.0369\n",
      "Epoch 1 LR: 0.001000\n",
      "Gradient norms: [3.3952792745646936, 4.066080480655644, 5.3782690552008425]\n",
      "61.24009156805011\n",
      "Validation MSE: 61.7280\n",
      "Epoch 2 LR: 0.001000\n",
      "Gradient norms: [4.332386104612545, 4.156168182100171, 4.525472181318665]\n",
      "60.65972582815573\n",
      "Validation MSE: 61.3986\n",
      "Epoch 3 LR: 0.001000\n",
      "Gradient norms: [3.0307716373670868, 3.512478261137978, 5.130676753375184]\n",
      "60.815408608211584\n",
      "Validation MSE: 61.1326\n",
      "Epoch 4 LR: 0.001000\n",
      "Gradient norms: [4.07885908395999, 3.617694082078415, 5.137195737404804]\n",
      "60.44323142859516\n",
      "Validation MSE: 60.8598\n",
      "Epoch 5 LR: 0.001000\n",
      "Gradient norms: [3.411669127900686, 3.95240718230675, 5.238944089613115]\n",
      "60.28894083447197\n",
      "Validation MSE: 60.5962\n",
      "Epoch 6 LR: 0.000999\n",
      "Gradient norms: [3.643551628852192, 3.6240371790422214, 5.320600922807165]\n",
      "59.582499205721476\n",
      "Validation MSE: 60.3846\n",
      "Epoch 7 LR: 0.000999\n",
      "Gradient norms: [4.46113228821297, 2.9965240584878825, 4.75101192905928]\n",
      "60.01394209339933\n",
      "Validation MSE: 60.1687\n",
      "Epoch 8 LR: 0.000999\n",
      "Gradient norms: [3.677092105640352, 3.315672494354834, 5.618572874993402]\n",
      "60.238236233200965\n",
      "Validation MSE: 59.9234\n",
      "Epoch 9 LR: 0.000999\n",
      "Gradient norms: [3.8022662109480185, 3.203717582336202, 4.857249668983657]\n",
      "59.56984823304221\n",
      "Validation MSE: 59.7510\n",
      "Epoch 10 LR: 0.000999\n",
      "Gradient norms: [3.6936512768670897, 3.42440275956657, 5.466582365738802]\n",
      "59.946196441725576\n",
      "Validation MSE: 59.5363\n",
      "Epoch 11 LR: 0.000999\n",
      "Gradient norms: [4.077964290617196, 3.082571375779977, 4.762029434351215]\n",
      "59.68820412042551\n",
      "Validation MSE: 59.3697\n",
      "Epoch 12 LR: 0.000999\n",
      "Gradient norms: [4.038446822916356, 3.4224196522808836, 5.382703611452458]\n",
      "59.72049055550804\n",
      "Validation MSE: 59.2417\n",
      "Epoch 13 LR: 0.000999\n",
      "Gradient norms: [3.5386991400267065, 3.8662792780163047, 4.775965691587849]\n",
      "60.22610937561725\n",
      "Validation MSE: 59.1108\n",
      "Epoch 14 LR: 0.000999\n",
      "Gradient norms: [4.05572822768154, 3.1180831735886225, 5.631290114082132]\n",
      "59.33684858289292\n",
      "Validation MSE: 58.9464\n",
      "Epoch 15 LR: 0.000999\n",
      "Gradient norms: [3.1211873220227484, 2.957679356109881, 4.593684981487316]\n",
      "59.440655198217684\n",
      "Validation MSE: 58.7834\n",
      "Epoch 16 LR: 0.000998\n",
      "Gradient norms: [3.504607068257282, 2.9356222511791734, 4.464841583068661]\n",
      "59.16198458409306\n",
      "Validation MSE: 58.6174\n",
      "Epoch 17 LR: 0.000998\n",
      "Gradient norms: [3.2545381315745514, 3.1840378503629756, 4.989167017796458]\n",
      "59.2114679182721\n",
      "Validation MSE: 58.4626\n",
      "Epoch 18 LR: 0.000998\n",
      "Gradient norms: [4.326943203771686, 4.073931752924278, 4.405589594110088]\n",
      "59.47987541899442\n",
      "Validation MSE: 58.2785\n",
      "Epoch 19 LR: 0.000998\n",
      "Gradient norms: [3.956277845027262, 3.1403112298807683, 5.191057171880656]\n",
      "60.016016040059036\n",
      "Validation MSE: 58.1701\n",
      "Epoch 20 LR: 0.000998\n",
      "Gradient norms: [3.3079947799663314, 3.228849844217866, 4.863513205415588]\n",
      "59.2191537309654\n",
      "Validation MSE: 58.0249\n",
      "Epoch 21 LR: 0.000998\n",
      "Gradient norms: [4.398396300300786, 3.907900661627478, 5.025923408849782]\n",
      "58.63630037183896\n",
      "Validation MSE: 57.8805\n",
      "Epoch 22 LR: 0.000998\n",
      "Gradient norms: [3.818794793466704, 3.131063131400697, 4.989553052232912]\n",
      "58.78312435466726\n",
      "Validation MSE: 57.7461\n",
      "Epoch 23 LR: 0.000998\n",
      "Gradient norms: [3.9259446583552924, 2.9013479108849016, 4.582887605369177]\n",
      "58.935956717018115\n",
      "Validation MSE: 57.6114\n",
      "Epoch 24 LR: 0.000998\n",
      "Gradient norms: [3.4899531712015404, 2.9237129809784284, 4.7423825455485815]\n",
      "58.4198375441619\n",
      "Validation MSE: 57.4953\n",
      "Epoch 25 LR: 0.000998\n",
      "Gradient norms: [3.3989420679933784, 3.156177384645493, 4.940653444479266]\n",
      "58.552460503967644\n",
      "Validation MSE: 57.3797\n",
      "Epoch 26 LR: 0.000997\n",
      "Gradient norms: [4.760733729490532, 2.9950321390089263, 4.15506253950245]\n",
      "59.04391853640754\n",
      "Validation MSE: 57.3029\n",
      "Epoch 27 LR: 0.000997\n",
      "Gradient norms: [4.320917112801767, 3.021849412210445, 4.97904979172016]\n",
      "57.77607922238182\n",
      "Validation MSE: 57.2015\n",
      "Epoch 28 LR: 0.000997\n",
      "Gradient norms: [3.4019009069650923, 3.3157853598214806, 4.933647511522997]\n",
      "58.64485284956051\n",
      "Validation MSE: 57.1244\n",
      "Epoch 29 LR: 0.000997\n",
      "Gradient norms: [3.5174972545328416, 3.0335649365774073, 4.713582694528254]\n",
      "59.117886331699324\n",
      "Validation MSE: 56.9919\n",
      "Epoch 30 LR: 0.000997\n",
      "Gradient norms: [3.3917075525401352, 2.9011040310687646, 4.8109151813558215]\n",
      "58.43874062817899\n",
      "Validation MSE: 56.9268\n",
      "Epoch 31 LR: 0.000997\n",
      "Gradient norms: [3.008544681965446, 3.2022875257799424, 4.25521272110061]\n",
      "58.01761900674905\n",
      "Validation MSE: 56.8351\n",
      "Epoch 32 LR: 0.000997\n",
      "Gradient norms: [3.7202392376875544, 3.518849191970146, 4.534286017051123]\n",
      "57.511991875448395\n",
      "Validation MSE: 56.7650\n",
      "Epoch 33 LR: 0.000997\n",
      "Gradient norms: [4.21026266233977, 3.257889685411434, 4.995407991391096]\n",
      "58.11226081048997\n",
      "Validation MSE: 56.7005\n",
      "Epoch 34 LR: 0.000997\n",
      "Gradient norms: [2.865358055109177, 3.024431178266846, 4.365935855169442]\n",
      "57.403991841664826\n",
      "Validation MSE: 56.6196\n",
      "Epoch 35 LR: 0.000997\n",
      "Gradient norms: [3.3250832737308316, 2.920435480822814, 4.4307916281453075]\n",
      "57.77672588746193\n",
      "Validation MSE: 56.5180\n",
      "Epoch 36 LR: 0.000996\n",
      "Gradient norms: [2.8142541422135507, 2.9386818892553817, 4.432986099590934]\n",
      "58.09424782395759\n",
      "Validation MSE: 56.4511\n",
      "Epoch 37 LR: 0.000996\n",
      "Gradient norms: [3.4394291932254375, 2.89842346517543, 4.923093954011735]\n",
      "58.323619731097715\n",
      "Validation MSE: 56.3983\n",
      "Epoch 38 LR: 0.000996\n",
      "Gradient norms: [4.020325054198708, 3.0078131533052583, 4.507784484389691]\n",
      "57.852549042209226\n",
      "Validation MSE: 56.2800\n",
      "Epoch 39 LR: 0.000996\n",
      "Gradient norms: [3.142479483423725, 2.728648073639443, 4.905247105554866]\n",
      "57.309283120284505\n",
      "Validation MSE: 56.1673\n",
      "Epoch 40 LR: 0.000996\n",
      "Gradient norms: [2.851284912792822, 2.3817044503529186, 4.157606841321697]\n",
      "57.39845041357599\n",
      "Validation MSE: 56.1040\n",
      "Epoch 41 LR: 0.000996\n",
      "Gradient norms: [3.0180352471804492, 2.629979922538973, 4.832808175847004]\n",
      "57.87421775656314\n",
      "Validation MSE: 55.9972\n",
      "Epoch 42 LR: 0.000996\n",
      "Gradient norms: [2.687417881768556, 2.8005934631683154, 4.50850917081372]\n",
      "57.52067053332754\n",
      "Validation MSE: 55.9082\n",
      "Epoch 43 LR: 0.000996\n",
      "Gradient norms: [2.6645313452736654, 3.198426847914919, 4.302233761338421]\n",
      "57.45234106197016\n",
      "Validation MSE: 55.8358\n",
      "Epoch 44 LR: 0.000996\n",
      "Gradient norms: [2.7980363048710344, 3.009522729303008, 4.624871255520459]\n",
      "57.33273869083124\n",
      "Validation MSE: 55.7807\n",
      "Epoch 45 LR: 0.000996\n",
      "Gradient norms: [2.826405274772109, 2.980129492861563, 4.232327731146852]\n",
      "57.415405009091316\n",
      "Validation MSE: 55.7164\n",
      "Epoch 46 LR: 0.000995\n",
      "Gradient norms: [2.951669698921517, 2.8648882319115887, 4.104820049593371]\n",
      "56.78695809440915\n",
      "Validation MSE: 55.6337\n",
      "Epoch 47 LR: 0.000995\n",
      "Gradient norms: [3.046810600682582, 2.545614807734266, 4.104150538381102]\n",
      "57.18701101844744\n",
      "Validation MSE: 55.5310\n",
      "Epoch 48 LR: 0.000995\n",
      "Gradient norms: [2.974469966120457, 2.6526382670960915, 4.591388868268993]\n",
      "56.604568108766195\n",
      "Validation MSE: 55.4692\n",
      "Epoch 49 LR: 0.000995\n",
      "Gradient norms: [2.8729315222360303, 2.7910155355487274, 4.615001646296618]\n",
      "56.58249560603564\n",
      "Validation MSE: 55.4146\n",
      "Epoch 50 LR: 0.000995\n",
      "Gradient norms: [3.0896628483098847, 3.0385932499902255, 4.194149372442203]\n",
      "56.8108173573796\n",
      "Validation MSE: 55.3095\n",
      "Epoch 51 LR: 0.000995\n",
      "Gradient norms: [2.8063165423265692, 2.7647328324530225, 4.333439109780506]\n",
      "56.77523797449979\n",
      "Validation MSE: 55.2553\n",
      "Epoch 52 LR: 0.000995\n",
      "Gradient norms: [2.7449228454420513, 2.6667439788354033, 4.152027417924103]\n",
      "57.1803110101374\n",
      "Validation MSE: 55.2324\n",
      "Epoch 53 LR: 0.000995\n",
      "Gradient norms: [3.535542439111963, 2.802797428039722, 4.317114446258234]\n",
      "56.54495494600292\n",
      "Validation MSE: 55.1599\n",
      "Epoch 54 LR: 0.000995\n",
      "Gradient norms: [3.6055090748172445, 2.4095218514436794, 4.527643786420368]\n",
      "56.749536206185624\n",
      "Validation MSE: 55.0841\n",
      "Epoch 55 LR: 0.000995\n",
      "Gradient norms: [2.8980370277919687, 2.4976299928517034, 4.599948073621666]\n",
      "56.26611394019323\n",
      "Validation MSE: 55.0216\n",
      "Epoch 56 LR: 0.000994\n",
      "Gradient norms: [3.091666988087894, 2.927161161072754, 4.655872748671567]\n",
      "56.340844452753686\n",
      "Validation MSE: 54.9192\n",
      "Epoch 57 LR: 0.000994\n",
      "Gradient norms: [3.8688322398661295, 2.8808310693373103, 4.305152603300357]\n",
      "56.37906047805232\n",
      "Validation MSE: 54.8137\n",
      "Epoch 58 LR: 0.000994\n",
      "Gradient norms: [3.121088097471512, 2.667068349107259, 4.783197700518848]\n",
      "55.65157029444568\n",
      "Validation MSE: 54.7212\n",
      "Epoch 59 LR: 0.000994\n",
      "Gradient norms: [3.3973455599340148, 2.4347686739265506, 3.7638524054788243]\n",
      "56.848405230852286\n",
      "Validation MSE: 54.6494\n",
      "Epoch 60 LR: 0.000994\n",
      "Gradient norms: [2.4295835390426426, 2.5097685815691615, 4.488379706256442]\n",
      "56.104063425631466\n",
      "Validation MSE: 54.5606\n",
      "Epoch 61 LR: 0.000994\n",
      "Gradient norms: [3.3340548953622555, 2.533399990533254, 4.250562866912973]\n",
      "55.881960034058956\n",
      "Validation MSE: 54.5308\n",
      "Epoch 62 LR: 0.000994\n",
      "Gradient norms: [3.033398670956858, 2.399834671103525, 4.382623266196559]\n",
      "55.89428695761929\n",
      "Validation MSE: 54.4610\n",
      "Epoch 63 LR: 0.000994\n",
      "Gradient norms: [2.808833263651833, 2.7849298955966946, 4.398173590186721]\n",
      "56.084945602432754\n",
      "Validation MSE: 54.3982\n",
      "Epoch 64 LR: 0.000994\n",
      "Gradient norms: [2.5497305242450023, 2.630834573730386, 4.891346217740332]\n",
      "55.83341826090695\n",
      "Validation MSE: 54.3442\n",
      "Epoch 65 LR: 0.000994\n",
      "Gradient norms: [3.515438612889401, 2.6529995719958754, 4.062527198526296]\n",
      "56.584605343746716\n",
      "Validation MSE: 54.2865\n",
      "Epoch 66 LR: 0.000993\n",
      "Gradient norms: [2.5929914242733445, 2.4521914174271178, 4.077802831867184]\n",
      "56.000371085612\n",
      "Validation MSE: 54.1845\n",
      "Epoch 67 LR: 0.000993\n",
      "Gradient norms: [3.3313494814613853, 2.447144180127378, 4.262960530343922]\n",
      "55.3645163872799\n",
      "Validation MSE: 54.0998\n",
      "Epoch 68 LR: 0.000993\n",
      "Gradient norms: [3.564696190980424, 2.5948786634018677, 4.489667116657694]\n",
      "55.48141595288402\n",
      "Validation MSE: 54.0525\n",
      "Epoch 69 LR: 0.000993\n",
      "Gradient norms: [3.3011530716946087, 2.4985902857631492, 4.100871721970041]\n",
      "56.01035389230495\n",
      "Validation MSE: 53.9792\n",
      "Epoch 70 LR: 0.000993\n",
      "Gradient norms: [4.172236125146236, 2.867943187365961, 4.461019849523652]\n",
      "55.63899447288798\n",
      "Validation MSE: 53.9065\n",
      "Epoch 71 LR: 0.000993\n",
      "Gradient norms: [3.341243291088388, 2.3663646357909536, 3.986035434514533]\n",
      "55.61074073181386\n",
      "Validation MSE: 53.8068\n",
      "Epoch 72 LR: 0.000993\n",
      "Gradient norms: [3.024442373437912, 2.580140976775819, 4.0493305249166145]\n",
      "55.72751815611414\n",
      "Validation MSE: 53.7252\n",
      "Epoch 73 LR: 0.000993\n",
      "Gradient norms: [3.1320064430687578, 2.7610161801634563, 4.363138188438596]\n",
      "55.246664868409\n",
      "Validation MSE: 53.7084\n",
      "Epoch 74 LR: 0.000993\n",
      "Gradient norms: [3.084574648053564, 2.3104539253755836, 4.351477785358262]\n",
      "55.3874082235166\n",
      "Validation MSE: 53.6454\n",
      "Epoch 75 LR: 0.000993\n",
      "Gradient norms: [4.386941303300972, 2.5600119681050915, 3.9137877205307188]\n",
      "55.84739430822521\n",
      "Validation MSE: 53.5904\n",
      "Epoch 76 LR: 0.000992\n",
      "Gradient norms: [3.1278431410600693, 2.5937961423016125, 4.512073929082694]\n",
      "55.46520576514451\n",
      "Validation MSE: 53.5078\n",
      "Epoch 77 LR: 0.000992\n",
      "Gradient norms: [2.944862877292733, 2.593045880014969, 4.512397370051662]\n",
      "55.158319051327226\n",
      "Validation MSE: 53.4369\n",
      "Epoch 78 LR: 0.000992\n",
      "Gradient norms: [2.8876547135377106, 2.7323743099682067, 4.575756996146297]\n",
      "54.93978399250253\n",
      "Validation MSE: 53.3980\n",
      "Epoch 79 LR: 0.000992\n",
      "Gradient norms: [3.5851895828751426, 2.3612879137170584, 4.181499248342643]\n",
      "55.58298277945266\n",
      "Validation MSE: 53.3217\n",
      "Epoch 80 LR: 0.000992\n",
      "Gradient norms: [3.0046753403088027, 2.152353572546092, 4.119274688910429]\n",
      "55.20186123878652\n",
      "Validation MSE: 53.2152\n",
      "Epoch 81 LR: 0.000992\n",
      "Gradient norms: [3.673043681144569, 2.6124396091713398, 4.230132812571171]\n",
      "55.29831689121566\n",
      "Validation MSE: 53.1151\n",
      "Epoch 82 LR: 0.000992\n",
      "Gradient norms: [3.004188709467234, 2.4590396790134785, 4.15600708055767]\n",
      "55.5287629545152\n",
      "Validation MSE: 53.0139\n",
      "Epoch 83 LR: 0.000992\n",
      "Gradient norms: [3.7336326176505947, 2.3153546871510207, 4.329484645616196]\n",
      "54.47397506211681\n",
      "Validation MSE: 52.9651\n",
      "Epoch 84 LR: 0.000992\n",
      "Gradient norms: [3.4897076637499156, 2.579663585660103, 4.538561262392958]\n",
      "54.34017187017533\n",
      "Validation MSE: 52.9117\n",
      "Epoch 85 LR: 0.000992\n",
      "Gradient norms: [4.025041111978883, 2.5580258556526996, 4.274127376564111]\n",
      "54.47538034871131\n",
      "Validation MSE: 52.8883\n",
      "Epoch 86 LR: 0.000991\n",
      "Gradient norms: [3.3995300409055322, 2.668394852601182, 3.816652130144696]\n",
      "55.670287481909256\n",
      "Validation MSE: 52.8226\n",
      "Epoch 87 LR: 0.000991\n",
      "Gradient norms: [4.400352633703264, 2.399637526001455, 4.519424871955078]\n",
      "54.44196761521594\n",
      "Validation MSE: 52.7258\n",
      "Epoch 88 LR: 0.000991\n",
      "Gradient norms: [3.883726364492496, 2.6539298459172502, 3.648380865066384]\n",
      "55.3042173462705\n",
      "Validation MSE: 52.6488\n",
      "Epoch 89 LR: 0.000991\n",
      "Gradient norms: [2.7898321932952412, 2.547220516848535, 4.4664326971154376]\n",
      "54.254026474233164\n",
      "Validation MSE: 52.5404\n",
      "Epoch 90 LR: 0.000991\n",
      "Gradient norms: [3.102898264876745, 2.5129482161017784, 4.564758144399541]\n",
      "54.35936271571998\n",
      "Validation MSE: 52.4982\n",
      "Epoch 91 LR: 0.000991\n",
      "Gradient norms: [2.729167061231796, 2.264329267436184, 4.410736432174838]\n",
      "54.33103009477244\n",
      "Validation MSE: 52.4092\n",
      "Epoch 92 LR: 0.000991\n",
      "Gradient norms: [3.449264850308374, 2.4820979521579005, 4.314948280712199]\n",
      "54.26814014903106\n",
      "Validation MSE: 52.3393\n",
      "Epoch 93 LR: 0.000991\n",
      "Gradient norms: [3.6266909225381667, 2.368047874315402, 4.3720487391820555]\n",
      "54.562549306139786\n",
      "Validation MSE: 52.2196\n",
      "Epoch 94 LR: 0.000991\n",
      "Gradient norms: [2.849935833029553, 2.4834062509280375, 4.798326451243772]\n",
      "53.44275719922275\n",
      "Validation MSE: 52.1708\n",
      "Epoch 95 LR: 0.000991\n",
      "Gradient norms: [2.604180915327066, 2.1297023574046965, 4.015343212788427]\n",
      "54.700206705354454\n",
      "Validation MSE: 52.0777\n",
      "Epoch 96 LR: 0.000990\n",
      "Gradient norms: [3.61129935138059, 2.652804191861181, 4.746123672913925]\n",
      "53.84480691275679\n",
      "Validation MSE: 52.0059\n",
      "Epoch 97 LR: 0.000990\n",
      "Gradient norms: [3.04248490485602, 2.643872796476368, 4.6511263289967095]\n",
      "53.34584340293268\n",
      "Validation MSE: 51.9590\n",
      "Epoch 98 LR: 0.000990\n",
      "Gradient norms: [2.7697297199496957, 2.5118999655778227, 4.043994802437852]\n",
      "54.31905591914069\n",
      "Validation MSE: 51.8698\n",
      "Epoch 99 LR: 0.000990\n",
      "Gradient norms: [3.1410977388720647, 2.740171708606428, 4.489548755943919]\n",
      "53.74590875714873\n",
      "Validation MSE: 51.8059\n",
      "Epoch 100 LR: 0.000990\n",
      "Gradient norms: [3.158010423110369, 2.5010385881640715, 4.262232806206543]\n",
      "54.05699714739069\n",
      "Validation MSE: 51.7980\n",
      "Epoch 101 LR: 0.000990\n",
      "Gradient norms: [2.9887631509348966, 2.2591407551862077, 4.510501174199695]\n",
      "53.7749430273509\n",
      "Validation MSE: 51.7244\n",
      "Epoch 102 LR: 0.000990\n",
      "Gradient norms: [4.056844502293888, 2.300325061118668, 4.322227015230384]\n",
      "53.45635799821699\n",
      "Validation MSE: 51.6932\n",
      "Epoch 103 LR: 0.000990\n",
      "Gradient norms: [2.998009446269968, 2.419258887932236, 4.169868403993106]\n",
      "54.00161783351561\n",
      "Validation MSE: 51.6406\n",
      "Epoch 104 LR: 0.000990\n",
      "Gradient norms: [2.68671460270138, 2.2266736211057294, 4.268141023617977]\n",
      "53.30040108233327\n",
      "Validation MSE: 51.6028\n",
      "Epoch 105 LR: 0.000990\n",
      "Gradient norms: [2.490456552897495, 2.217639838533611, 4.672446538894982]\n",
      "53.57637711048201\n",
      "Validation MSE: 51.5679\n",
      "Epoch 106 LR: 0.000990\n",
      "Gradient norms: [3.320690242716601, 1.8496731287649661, 4.1640641456862175]\n",
      "53.53611518294818\n",
      "Validation MSE: 51.5366\n",
      "Epoch 107 LR: 0.000989\n",
      "Gradient norms: [3.143041979799625, 2.64542513333453, 4.695808665966235]\n",
      "53.45248536156486\n",
      "Validation MSE: 51.5356\n",
      "Epoch 108 LR: 0.000989\n",
      "Gradient norms: [3.596433611963864, 2.409206962751668, 4.998767325631976]\n",
      "52.595708816937986\n",
      "Validation MSE: 51.4813\n",
      "Epoch 109 LR: 0.000989\n",
      "Gradient norms: [4.583384610109667, 2.649587868123122, 4.212836997422658]\n",
      "53.36340188313245\n",
      "Validation MSE: 51.4345\n",
      "Epoch 110 LR: 0.000989\n",
      "Gradient norms: [3.4417654924404584, 2.500810726711929, 4.06274805167113]\n",
      "54.21457527649204\n",
      "Validation MSE: 51.3913\n",
      "Epoch 111 LR: 0.000989\n",
      "Gradient norms: [3.1529635203420776, 2.786142604538853, 4.414899666447722]\n",
      "53.07911681157635\n",
      "Validation MSE: 51.3285\n",
      "Epoch 112 LR: 0.000989\n",
      "Gradient norms: [3.5922672439887644, 2.6410849371560263, 4.509910869231014]\n",
      "52.848642670969966\n",
      "Validation MSE: 51.3012\n",
      "Epoch 113 LR: 0.000989\n",
      "Gradient norms: [2.977563709995931, 2.42687954822963, 4.63084137562858]\n",
      "52.67273466540278\n",
      "Validation MSE: 51.2714\n",
      "Epoch 114 LR: 0.000989\n",
      "Gradient norms: [3.009623097722251, 2.321520168079639, 4.073973294323346]\n",
      "53.132802944464395\n",
      "Validation MSE: 51.2472\n",
      "Epoch 115 LR: 0.000989\n",
      "Gradient norms: [3.9298046501353157, 2.3110112211253058, 4.699484868290203]\n",
      "52.46918061256054\n",
      "Validation MSE: 51.2153\n",
      "Epoch 116 LR: 0.000989\n",
      "Gradient norms: [3.7159517295151216, 2.3867097634178807, 4.5495475985412535]\n",
      "52.9427166341773\n",
      "Validation MSE: 51.2041\n",
      "Epoch 117 LR: 0.000988\n",
      "Gradient norms: [3.509850065013588, 2.1109541155962552, 4.224050912278682]\n",
      "52.961096138519075\n",
      "Validation MSE: 51.1533\n",
      "Epoch 118 LR: 0.000988\n",
      "Gradient norms: [3.3553002416937434, 2.305372003010263, 4.479589912091908]\n",
      "52.892073798210774\n",
      "Validation MSE: 51.0943\n",
      "Epoch 119 LR: 0.000988\n",
      "Gradient norms: [3.3507354455837555, 2.0473890990213275, 4.786301050836588]\n",
      "52.2504876871921\n",
      "Validation MSE: 51.0395\n",
      "Epoch 120 LR: 0.000988\n",
      "Gradient norms: [3.621950810187307, 2.6422707621982737, 4.447841733506678]\n",
      "52.43509747751129\n",
      "Validation MSE: 51.0251\n",
      "Epoch 121 LR: 0.000988\n",
      "Gradient norms: [4.264326113093987, 2.510960157766998, 4.551416450875829]\n",
      "52.460592302628356\n",
      "Validation MSE: 50.9975\n",
      "Epoch 122 LR: 0.000988\n",
      "Gradient norms: [3.5152205822334586, 2.4328715576541593, 4.001015380681431]\n",
      "53.17566468096205\n",
      "Validation MSE: 50.9694\n",
      "Epoch 123 LR: 0.000988\n",
      "Gradient norms: [2.9379056612377252, 2.804754736696988, 4.085671598866783]\n",
      "53.05917971875139\n",
      "Validation MSE: 50.9383\n",
      "Epoch 124 LR: 0.000988\n",
      "Gradient norms: [3.762221826897472, 2.492789226673093, 4.897210414513273]\n",
      "52.032816792778256\n",
      "Validation MSE: 50.9295\n",
      "Epoch 125 LR: 0.000988\n",
      "Gradient norms: [3.8777267557124326, 2.792178911518321, 4.528575422519372]\n",
      "52.76555642081052\n",
      "Validation MSE: 50.8425\n",
      "Epoch 126 LR: 0.000988\n",
      "Gradient norms: [3.4677979879453544, 2.2649690481857094, 3.9610991758102196]\n",
      "53.01093784361023\n",
      "Validation MSE: 50.7997\n",
      "Epoch 127 LR: 0.000987\n",
      "Gradient norms: [4.49887172610647, 2.2318626793535, 3.840608628168608]\n",
      "53.0459049353667\n",
      "Validation MSE: 50.8052\n",
      "Epoch 128 LR: 0.000987\n",
      "Gradient norms: [3.903342970241867, 2.2612587410216585, 4.31659076876526]\n",
      "52.19098207305627\n",
      "Validation MSE: 50.8021\n",
      "Epoch 129 LR: 0.000987\n",
      "Gradient norms: [4.237247827432212, 2.4835619478945645, 3.908781031621441]\n",
      "52.86115766731903\n",
      "Validation MSE: 50.7760\n",
      "Epoch 130 LR: 0.000987\n",
      "Gradient norms: [3.311714788909037, 2.2409160175513763, 4.103200617546547]\n",
      "52.91265982539628\n",
      "Validation MSE: 50.7545\n",
      "Epoch 131 LR: 0.000987\n",
      "Gradient norms: [4.160596092940483, 2.9724034775465555, 4.121647236105258]\n",
      "52.49482151337009\n",
      "Validation MSE: 50.7388\n",
      "Epoch 132 LR: 0.000987\n",
      "Gradient norms: [3.642849567450574, 2.8296000822523935, 4.610826117011381]\n",
      "52.00016827421703\n",
      "Validation MSE: 50.7578\n",
      "Epoch 133 LR: 0.000987\n",
      "Gradient norms: [3.737729564119493, 2.272831147680045, 4.34247338260992]\n",
      "52.49343921749356\n",
      "Validation MSE: 50.7259\n",
      "Epoch 134 LR: 0.000987\n",
      "Gradient norms: [3.304702311485754, 2.4732177585839685, 4.444957253121764]\n",
      "52.23444937254484\n",
      "Validation MSE: 50.6859\n",
      "Epoch 135 LR: 0.000987\n",
      "Gradient norms: [2.8811750776911693, 2.5042308976529717, 4.7962505639726]\n",
      "52.19817734533625\n",
      "Validation MSE: 50.7005\n",
      "Epoch 136 LR: 0.000987\n",
      "Gradient norms: [2.9616678796554226, 2.2259642635952845, 4.438190321040395]\n",
      "52.24495357569956\n",
      "Validation MSE: 50.6927\n",
      "Epoch 137 LR: 0.000986\n",
      "Gradient norms: [3.609679748386483, 2.3639495083862494, 4.725651042346529]\n",
      "51.721749982281\n",
      "Validation MSE: 50.6813\n",
      "Epoch 138 LR: 0.000986\n",
      "Gradient norms: [3.337471476395796, 2.434418792760524, 3.9627666693588504]\n",
      "52.732585470636465\n",
      "Validation MSE: 50.6447\n",
      "Epoch 139 LR: 0.000986\n",
      "Gradient norms: [2.8239981728321255, 2.342300160377369, 4.047855963719297]\n",
      "52.449040294688075\n",
      "Validation MSE: 50.6097\n",
      "Epoch 140 LR: 0.000986\n",
      "Gradient norms: [3.380743172359441, 2.3332181751709413, 4.2412776584362595]\n",
      "51.820732609144756\n",
      "Validation MSE: 50.5574\n",
      "Epoch 141 LR: 0.000986\n",
      "Gradient norms: [3.425890613542629, 2.6074205226475042, 4.356680721688236]\n",
      "51.817722022372216\n",
      "Validation MSE: 50.5470\n",
      "Epoch 142 LR: 0.000986\n",
      "Gradient norms: [2.979323669072459, 2.1392949605753047, 4.579275338890334]\n",
      "51.443621524250446\n",
      "Validation MSE: 50.5418\n",
      "Epoch 143 LR: 0.000986\n",
      "Gradient norms: [4.571931090358443, 2.239178786598268, 4.111610937276637]\n",
      "52.78901918014525\n",
      "Validation MSE: 50.5277\n",
      "Epoch 144 LR: 0.000986\n",
      "Gradient norms: [3.7604441024075865, 2.2460336539985026, 4.423182050662693]\n",
      "51.32307903532537\n",
      "Validation MSE: 50.5258\n",
      "Epoch 145 LR: 0.000986\n",
      "Gradient norms: [3.796718186232824, 2.522152042930309, 3.629088408314964]\n",
      "52.640361657700744\n",
      "Validation MSE: 50.5394\n",
      "Epoch 146 LR: 0.000986\n",
      "Gradient norms: [3.3464548204690807, 2.089695078599357, 4.506692567320804]\n",
      "51.35911437457719\n",
      "Validation MSE: 50.5042\n",
      "Epoch 147 LR: 0.000986\n",
      "Gradient norms: [2.901581978375791, 1.9981277932359218, 4.849082056626218]\n",
      "50.787661501619844\n",
      "Validation MSE: 50.4745\n",
      "Epoch 148 LR: 0.000985\n",
      "Gradient norms: [4.446318886627847, 2.579824391059008, 4.372480273163474]\n",
      "51.35821213780202\n",
      "Validation MSE: 50.4416\n",
      "Epoch 149 LR: 0.000985\n",
      "Gradient norms: [3.1012864503749378, 2.2524622496698274, 4.193848223789838]\n",
      "51.719920439304424\n",
      "Validation MSE: 50.4544\n",
      "Epoch 150 LR: 0.000985\n",
      "Gradient norms: [3.3946763683348435, 2.166042967522019, 4.5701742831875345]\n",
      "51.321291596545144\n",
      "Validation MSE: 50.4650\n",
      "Epoch 151 LR: 0.000985\n",
      "Gradient norms: [3.8351303222870383, 2.2140217192406046, 4.762632638718298]\n",
      "51.133923531940574\n",
      "Validation MSE: 50.4528\n",
      "Epoch 152 LR: 0.000985\n",
      "Gradient norms: [3.619921932132897, 2.5891166860915735, 4.514056706853166]\n",
      "50.82467568392794\n",
      "Validation MSE: 50.4450\n",
      "Epoch 153 LR: 0.000985\n",
      "Gradient norms: [3.2567125294768786, 2.0479412617853794, 4.802197316761647]\n",
      "50.45275753302051\n",
      "Validation MSE: 50.4367\n",
      "Epoch 154 LR: 0.000985\n",
      "Gradient norms: [3.8453128763719144, 2.516594092460337, 3.7283244970872693]\n",
      "52.026424157892144\n",
      "Validation MSE: 50.4245\n",
      "Epoch 155 LR: 0.000985\n",
      "Gradient norms: [3.610159009127296, 2.3416727921663734, 4.562999871579391]\n",
      "51.08306315578761\n",
      "Validation MSE: 50.3813\n",
      "Epoch 156 LR: 0.000985\n",
      "Gradient norms: [3.565801524158975, 2.3791064874552226, 4.210336122196545]\n",
      "51.51086438959177\n",
      "Validation MSE: 50.3538\n",
      "Epoch 157 LR: 0.000985\n",
      "Gradient norms: [5.085092575840898, 2.5902506984651956, 4.887921790079127]\n",
      "50.221726066368284\n",
      "Validation MSE: 50.3732\n",
      "Epoch 158 LR: 0.000984\n",
      "Gradient norms: [3.6155289601338993, 2.0024273325789532, 4.827511122404963]\n",
      "50.26537378072159\n",
      "Validation MSE: 50.3408\n",
      "Epoch 159 LR: 0.000984\n",
      "Gradient norms: [5.027083115926444, 3.049833714361623, 3.874937574090516]\n",
      "52.06534820640309\n",
      "Validation MSE: 50.3256\n",
      "Epoch 160 LR: 0.000984\n",
      "Gradient norms: [4.5545010932230205, 3.2532378306357765, 3.888024332950995]\n",
      "51.54630273837423\n",
      "Validation MSE: 50.2781\n",
      "Epoch 161 LR: 0.000984\n",
      "Gradient norms: [3.7511438374143165, 2.539929183454203, 4.492761519293406]\n",
      "50.58889568393977\n",
      "Validation MSE: 50.2430\n",
      "Epoch 162 LR: 0.000984\n",
      "Gradient norms: [3.6016084371912447, 2.382680122041235, 4.611403490352842]\n",
      "50.82733046905434\n",
      "Validation MSE: 50.1853\n",
      "Epoch 163 LR: 0.000984\n",
      "Gradient norms: [4.109854054697783, 2.892921315170229, 4.1191840726004605]\n",
      "51.48165978204979\n",
      "Validation MSE: 50.1558\n",
      "Epoch 164 LR: 0.000984\n",
      "Gradient norms: [3.511597279412874, 2.309917805968249, 4.557914363993876]\n",
      "50.250680303896964\n",
      "Validation MSE: 50.1100\n",
      "Epoch 165 LR: 0.000984\n",
      "Gradient norms: [4.173165546872306, 2.8946365221961847, 4.320138444904873]\n",
      "51.09324373796652\n",
      "Validation MSE: 50.0624\n",
      "Epoch 166 LR: 0.000984\n",
      "Gradient norms: [3.4732144069992943, 2.345361878409372, 3.4871526266556443]\n",
      "51.71316715662223\n",
      "Validation MSE: 50.0135\n",
      "Epoch 167 LR: 0.000984\n",
      "Gradient norms: [4.076025234761017, 2.5479165531449897, 3.780298103700994]\n",
      "51.80180341500524\n",
      "Validation MSE: 50.0246\n",
      "Epoch 168 LR: 0.000983\n",
      "Gradient norms: [4.989986132760984, 2.555484494973614, 4.003173557149636]\n",
      "51.61621507477448\n",
      "Validation MSE: 49.9917\n",
      "Epoch 169 LR: 0.000983\n",
      "Gradient norms: [4.282287513014634, 2.898829106855499, 4.562795697742962]\n",
      "50.01348315716556\n",
      "Validation MSE: 49.9790\n",
      "Epoch 170 LR: 0.000983\n",
      "Gradient norms: [4.170979742667469, 2.450929066680376, 3.6302110217972237]\n",
      "51.47227423226408\n",
      "Validation MSE: 49.9193\n",
      "Epoch 171 LR: 0.000983\n",
      "Gradient norms: [4.058744972409423, 2.688903862989155, 4.08548471618181]\n",
      "51.014361057756034\n",
      "Validation MSE: 49.8880\n",
      "Epoch 172 LR: 0.000983\n",
      "Gradient norms: [3.67208218670862, 2.353150980523986, 4.041097092812794]\n",
      "50.707411896739934\n",
      "Validation MSE: 49.8558\n",
      "Epoch 173 LR: 0.000983\n",
      "Gradient norms: [4.163488710365039, 2.3700228275290773, 4.102143485562305]\n",
      "51.0474061337202\n",
      "Validation MSE: 49.7940\n",
      "Epoch 174 LR: 0.000983\n",
      "Gradient norms: [3.794617117348249, 2.75202452724047, 4.099525635315783]\n",
      "50.86440429210488\n",
      "Validation MSE: 49.8075\n",
      "Epoch 175 LR: 0.000983\n",
      "Gradient norms: [4.389704254071941, 2.1447212680742136, 3.9583676782446355]\n",
      "51.30638689842816\n",
      "Validation MSE: 49.7913\n",
      "Epoch 176 LR: 0.000983\n",
      "Gradient norms: [4.114722931033665, 2.2166644358680823, 3.571739248280806]\n",
      "51.70753911005901\n",
      "Validation MSE: 49.7962\n",
      "Epoch 177 LR: 0.000983\n",
      "Gradient norms: [4.153887838417391, 2.461562190807273, 4.062545656224065]\n",
      "50.4180351524524\n",
      "Validation MSE: 49.7670\n",
      "Epoch 178 LR: 0.000983\n",
      "Gradient norms: [3.6475296979481597, 2.851584456552778, 3.9227916335561286]\n",
      "51.181249724625864\n",
      "Validation MSE: 49.7312\n",
      "Epoch 179 LR: 0.000982\n",
      "Gradient norms: [3.847637513762487, 2.5501505721624884, 4.776552124119229]\n",
      "49.23546304106687\n",
      "Validation MSE: 49.7055\n",
      "Epoch 180 LR: 0.000982\n",
      "Gradient norms: [3.958129958445977, 2.562317782375109, 4.227058488376577]\n",
      "50.20528276510706\n",
      "Validation MSE: 49.7143\n",
      "Epoch 181 LR: 0.000982\n",
      "Gradient norms: [5.026028203086898, 2.3126404402356195, 4.5336024896386915]\n",
      "49.45073612376641\n",
      "Validation MSE: 49.6859\n",
      "Epoch 182 LR: 0.000982\n",
      "Gradient norms: [3.769189245409876, 2.2296689001134027, 3.983911423305145]\n",
      "50.752424536323225\n",
      "Validation MSE: 49.6440\n",
      "Epoch 183 LR: 0.000982\n",
      "Gradient norms: [4.907117850680446, 2.616161282634484, 4.157372174063998]\n",
      "51.03809042840583\n",
      "Validation MSE: 49.6325\n",
      "Epoch 184 LR: 0.000982\n",
      "Gradient norms: [4.227263665828324, 2.044589709651379, 4.194966754565523]\n",
      "50.770573890369604\n",
      "Validation MSE: 49.5986\n",
      "Epoch 185 LR: 0.000982\n",
      "Gradient norms: [3.319887641500809, 2.497740889949255, 3.9733027717605665]\n",
      "50.15751331166128\n",
      "Validation MSE: 49.5665\n",
      "Epoch 186 LR: 0.000982\n",
      "Gradient norms: [4.1979833680574785, 2.7070862202640202, 4.107415025767258]\n",
      "50.7566063087037\n",
      "Validation MSE: 49.5787\n",
      "Epoch 187 LR: 0.000982\n",
      "Gradient norms: [4.233205859888985, 2.3996157748852998, 4.51207513759402]\n",
      "49.50481827398894\n",
      "Validation MSE: 49.5792\n",
      "Epoch 188 LR: 0.000982\n",
      "Gradient norms: [3.346921350946216, 2.094126391725684, 3.458743532480384]\n",
      "51.06374142004473\n",
      "Validation MSE: 49.5822\n",
      "Epoch 189 LR: 0.000981\n",
      "Gradient norms: [3.8906582221263117, 2.3532003981887257, 4.381377538583856]\n",
      "49.42957372254661\n",
      "Validation MSE: 49.5636\n",
      "Epoch 190 LR: 0.000981\n",
      "Gradient norms: [3.7766058240910354, 2.3379388076027485, 4.533840900771179]\n",
      "49.22103564282897\n",
      "Validation MSE: 49.5558\n",
      "Epoch 191 LR: 0.000981\n",
      "Gradient norms: [3.4887115466532865, 1.8988311421809394, 4.091817319696338]\n",
      "49.81109279860642\n",
      "Validation MSE: 49.5105\n",
      "Epoch 192 LR: 0.000981\n",
      "Gradient norms: [4.670491809665863, 2.244084456589607, 4.078239966726422]\n",
      "50.118953388294734\n",
      "Validation MSE: 49.4778\n",
      "Epoch 193 LR: 0.000981\n",
      "Gradient norms: [4.597837230254286, 2.667634527701494, 4.7956197621568135]\n",
      "48.96864752432607\n",
      "Validation MSE: 49.4523\n",
      "Epoch 194 LR: 0.000981\n",
      "Gradient norms: [4.596951550958901, 3.0756551490944024, 4.008209554259506]\n",
      "50.33261104753196\n",
      "Validation MSE: 49.4460\n",
      "Epoch 195 LR: 0.000981\n",
      "Gradient norms: [3.4095781851194733, 2.5366099274835925, 4.165474996136054]\n",
      "49.489995053636676\n",
      "Validation MSE: 49.4415\n",
      "Epoch 196 LR: 0.000981\n",
      "Gradient norms: [3.8052757366562595, 1.828521183638974, 4.4120771198954225]\n",
      "49.18136453139503\n",
      "Validation MSE: 49.4424\n",
      "Epoch 197 LR: 0.000981\n",
      "Gradient norms: [4.573949517965913, 2.777779836712854, 4.318916872325844]\n",
      "49.592113680821484\n",
      "Validation MSE: 49.4402\n",
      "Epoch 198 LR: 0.000981\n",
      "Gradient norms: [4.75448938744333, 2.8733547270588957, 4.223508167441042]\n",
      "49.95356714492098\n",
      "Validation MSE: 49.4382\n",
      "Epoch 199 LR: 0.000980\n",
      "Gradient norms: [3.5948516404401785, 2.1606778679667697, 3.7442621208743763]\n",
      "50.46196331918324\n",
      "Validation MSE: 49.4501\n",
      "Epoch 200 LR: 0.000980\n",
      "Gradient norms: [3.723053957468891, 2.42123217119579, 3.8468901048143147]\n",
      "50.66561909544152\n",
      "Validation MSE: 49.4746\n",
      "Epoch 201 LR: 0.000980\n",
      "Gradient norms: [4.098937885137359, 2.4118669947254268, 4.139693118297636]\n",
      "49.35642178623828\n",
      "Validation MSE: 49.4404\n",
      "Epoch 202 LR: 0.000980\n",
      "Gradient norms: [4.186956503201817, 2.22977510824237, 4.866470666424695]\n",
      "48.074596783899146\n",
      "Validation MSE: 49.3879\n",
      "Epoch 203 LR: 0.000980\n",
      "Gradient norms: [4.237578559671143, 2.6964903610853717, 3.955991376429349]\n",
      "50.024602095223344\n",
      "Validation MSE: 49.3980\n",
      "Epoch 204 LR: 0.000980\n",
      "Gradient norms: [5.051475933121637, 2.620150240075504, 3.6610506391352273]\n",
      "50.2161373096694\n",
      "Validation MSE: 49.4016\n",
      "Epoch 205 LR: 0.000980\n",
      "Gradient norms: [4.3502578693927525, 2.2520677065782526, 4.0432954080009855]\n",
      "49.842596616201625\n",
      "Validation MSE: 49.3871\n",
      "Epoch 206 LR: 0.000980\n",
      "Gradient norms: [4.396995862967683, 2.7120818690030246, 4.238420439956353]\n",
      "49.386824611610656\n",
      "Validation MSE: 49.3777\n",
      "Epoch 207 LR: 0.000980\n",
      "Gradient norms: [5.079438967886469, 2.1451551117304417, 4.383645618868534]\n",
      "48.57082264428415\n",
      "Validation MSE: 49.3415\n",
      "Epoch 208 LR: 0.000980\n",
      "Gradient norms: [3.645248585032348, 2.5661073295810617, 4.405068524184531]\n",
      "48.97906098042986\n",
      "Validation MSE: 49.3792\n",
      "Epoch 209 LR: 0.000980\n",
      "Gradient norms: [4.530732532557844, 2.169668368183953, 4.194497679128115]\n",
      "49.11135836324095\n",
      "Validation MSE: 49.3929\n",
      "Epoch 210 LR: 0.000979\n",
      "Gradient norms: [4.668193692220335, 2.315489544516223, 3.79715621315303]\n",
      "49.68694056157913\n",
      "Validation MSE: 49.3710\n",
      "Epoch 211 LR: 0.000979\n",
      "Gradient norms: [4.31938627073153, 2.553582057825673, 4.6013997389980785]\n",
      "48.005877035618795\n",
      "Validation MSE: 49.3408\n",
      "Epoch 212 LR: 0.000979\n",
      "Gradient norms: [4.644653868437729, 2.5339742140172095, 3.7018973838022227]\n",
      "49.51179617651574\n",
      "Validation MSE: 49.3447\n",
      "Epoch 213 LR: 0.000979\n",
      "Gradient norms: [3.871155667172646, 2.3230513384985314, 4.080892898356797]\n",
      "49.031882904605574\n",
      "Validation MSE: 49.3014\n",
      "Epoch 214 LR: 0.000979\n",
      "Gradient norms: [3.751905829422876, 2.4548564119521457, 3.518502631589564]\n",
      "50.07497583490661\n",
      "Validation MSE: 49.3051\n",
      "Epoch 215 LR: 0.000979\n",
      "Gradient norms: [4.665971086814107, 2.6498886138444067, 4.399368672597726]\n",
      "48.25517408950507\n",
      "Validation MSE: 49.2817\n",
      "Epoch 216 LR: 0.000979\n",
      "Gradient norms: [5.291805918133691, 2.343893098717279, 3.7783947668694187]\n",
      "49.58083199671902\n",
      "Validation MSE: 49.2975\n",
      "Epoch 217 LR: 0.000979\n",
      "Gradient norms: [4.433058640208949, 2.7139980306640585, 3.569536210546037]\n",
      "50.23997618412591\n",
      "Validation MSE: 49.2357\n",
      "Epoch 218 LR: 0.000979\n",
      "Gradient norms: [4.859796144076173, 2.2103447526674422, 3.630095433069992]\n",
      "49.472398030660166\n",
      "Validation MSE: 49.2026\n",
      "Epoch 219 LR: 0.000979\n",
      "Gradient norms: [4.634199226027877, 2.4964957991161496, 4.379321975832207]\n",
      "48.43345731086682\n",
      "Validation MSE: 49.2196\n",
      "Epoch 220 LR: 0.000978\n",
      "Gradient norms: [6.0495508107493965, 2.7900522363939433, 4.9217996179175785]\n",
      "47.14358976891496\n",
      "Validation MSE: 49.1905\n",
      "Epoch 221 LR: 0.000978\n",
      "Gradient norms: [4.527552015081372, 2.7762237073311904, 3.5647905662942554]\n",
      "49.33707223537314\n",
      "Validation MSE: 49.1640\n",
      "Epoch 222 LR: 0.000978\n",
      "Gradient norms: [4.974245453637688, 2.6548029338458403, 4.541238833729902]\n",
      "47.65691471560378\n",
      "Validation MSE: 49.1817\n",
      "Epoch 223 LR: 0.000978\n",
      "Gradient norms: [5.12825853224037, 2.4190809027259847, 4.151555698884377]\n",
      "48.21563980077792\n",
      "Validation MSE: 49.1359\n",
      "Epoch 224 LR: 0.000978\n",
      "Gradient norms: [7.570967723937067, 2.4537014725873543, 3.962960233303999]\n",
      "48.674379408791054\n",
      "Validation MSE: 49.1167\n",
      "Epoch 225 LR: 0.000978\n",
      "Gradient norms: [5.404770990014418, 2.3215599050632356, 4.140727871912973]\n",
      "48.345739025292616\n",
      "Validation MSE: 49.1370\n",
      "Epoch 226 LR: 0.000978\n",
      "Gradient norms: [6.6853563744341304, 2.4666931705142225, 3.589051183756487]\n",
      "49.24013757210146\n",
      "Validation MSE: 49.1253\n",
      "Epoch 227 LR: 0.000978\n",
      "Gradient norms: [4.85926212764852, 2.0267961992426162, 3.8451391529972367]\n",
      "48.80269839962245\n",
      "Validation MSE: 49.1272\n",
      "Epoch 228 LR: 0.000978\n",
      "Gradient norms: [5.483985882540623, 2.7169738175815663, 3.753878235601678]\n",
      "48.98507666527671\n",
      "Validation MSE: 49.0865\n",
      "Epoch 229 LR: 0.000978\n",
      "Gradient norms: [5.3527525610388675, 2.809236951097206, 3.215977783276497]\n",
      "50.21841368242666\n",
      "Validation MSE: 49.0901\n",
      "Epoch 230 LR: 0.000978\n",
      "Gradient norms: [5.56933903353287, 2.5819523199601098, 4.12843926968577]\n",
      "48.19825557445538\n",
      "Validation MSE: 49.0670\n",
      "Epoch 231 LR: 0.000977\n",
      "Gradient norms: [4.912633315895776, 2.1900669258607297, 3.6722980319760867]\n",
      "49.12898819314232\n",
      "Validation MSE: 49.0178\n",
      "Epoch 232 LR: 0.000977\n",
      "Gradient norms: [5.356292699545071, 2.699096721293084, 4.044081485734783]\n",
      "48.193529963912745\n",
      "Validation MSE: 48.9864\n",
      "Epoch 233 LR: 0.000977\n",
      "Gradient norms: [5.809891031007557, 2.447630727022005, 3.6378981746537375]\n",
      "48.9969479832613\n",
      "Validation MSE: 48.9473\n",
      "Epoch 234 LR: 0.000977\n",
      "Gradient norms: [5.535369658393867, 2.579783166808291, 3.7414638085248906]\n",
      "48.75975092656281\n",
      "Validation MSE: 48.9425\n",
      "Epoch 235 LR: 0.000977\n",
      "Gradient norms: [5.24068804224781, 2.9645574393661795, 3.7861588216003277]\n",
      "48.461219631678425\n",
      "Validation MSE: 48.9184\n",
      "Epoch 236 LR: 0.000977\n",
      "Gradient norms: [4.320492702432101, 2.9027784875666893, 3.676033023766127]\n",
      "49.01644167004274\n",
      "Validation MSE: 48.8877\n",
      "Epoch 237 LR: 0.000977\n",
      "Gradient norms: [3.9605165524262578, 2.6565899041762964, 3.9204860235093104]\n",
      "48.21487202669942\n",
      "Validation MSE: 48.8546\n",
      "Epoch 238 LR: 0.000977\n",
      "Gradient norms: [4.49815088342457, 2.394519766776026, 3.8265402980497134]\n",
      "47.777711611899726\n",
      "Validation MSE: 48.8230\n",
      "Epoch 239 LR: 0.000977\n",
      "Gradient norms: [5.856821113795604, 2.6869968887522453, 3.652700278400788]\n",
      "49.08363577767553\n",
      "Validation MSE: 48.7669\n",
      "Epoch 240 LR: 0.000977\n",
      "Gradient norms: [4.983868810516043, 2.2755505618418854, 3.625012093432839]\n",
      "48.38902263109269\n",
      "Validation MSE: 48.7084\n",
      "Epoch 241 LR: 0.000976\n",
      "Gradient norms: [5.291051855046243, 2.277075345656008, 3.9239014489622512]\n",
      "48.019355237462385\n",
      "Validation MSE: 48.7060\n",
      "Epoch 242 LR: 0.000976\n",
      "Gradient norms: [5.885538085625651, 1.8773788201643054, 4.170102259510478]\n",
      "47.2833976607784\n",
      "Validation MSE: 48.6838\n",
      "Epoch 243 LR: 0.000976\n",
      "Gradient norms: [6.717725290427174, 2.2435382128829167, 4.120244624553663]\n",
      "47.32587396697374\n",
      "Validation MSE: 48.6950\n",
      "Epoch 244 LR: 0.000976\n",
      "Gradient norms: [4.747456961777528, 2.565861231197092, 3.8893639670507634]\n",
      "47.99956411602304\n",
      "Validation MSE: 48.6855\n",
      "Epoch 245 LR: 0.000976\n",
      "Gradient norms: [4.682956171300768, 2.8791085776432905, 3.621231306143347]\n",
      "48.461203237336726\n",
      "Validation MSE: 48.7179\n",
      "Epoch 246 LR: 0.000976\n",
      "Gradient norms: [5.764986868335929, 2.5230296051548886, 3.4501278729710574]\n",
      "48.40242846968161\n",
      "Validation MSE: 48.7144\n",
      "Epoch 247 LR: 0.000976\n",
      "Gradient norms: [4.960262021746598, 2.516856883783957, 4.215592617852739]\n",
      "47.431418272135836\n",
      "Validation MSE: 48.7414\n",
      "Epoch 248 LR: 0.000976\n",
      "Gradient norms: [4.893851473459914, 2.4223595766808277, 3.8930343674794137]\n",
      "48.16438919966717\n",
      "Validation MSE: 48.7345\n",
      "Epoch 249 LR: 0.000976\n",
      "Gradient norms: [5.437466605668724, 2.6802470400467073, 3.4025707947680064]\n",
      "48.7784675560328\n",
      "Validation MSE: 48.7544\n",
      "Epoch 250 LR: 0.000976\n",
      "Gradient norms: [4.550482180145324, 2.267435309375333, 4.2033893645313745]\n",
      "47.21667763266836\n",
      "Validation MSE: 48.7704\n",
      "Epoch 251 LR: 0.000976\n",
      "Gradient norms: [4.591928243750728, 2.6514705527123033, 3.5029533437845717]\n",
      "48.331121364274345\n",
      "Validation MSE: 48.8054\n",
      "Epoch 252 LR: 0.000975\n",
      "Gradient norms: [4.936707610288966, 2.4625105732185553, 3.6259295620775003]\n",
      "47.992262030634656\n",
      "Validation MSE: 48.7984\n",
      "Epoch 253 LR: 0.000975\n",
      "Gradient norms: [5.46281128867206, 2.425405302332279, 3.402077430865273]\n",
      "49.07324087235303\n",
      "Validation MSE: 48.7911\n",
      "Epoch 254 LR: 0.000975\n",
      "Gradient norms: [7.1195594390450365, 2.542266552803631, 3.4558939052030433]\n",
      "48.56425577196137\n",
      "Validation MSE: 48.7794\n",
      "Epoch 255 LR: 0.000975\n",
      "Gradient norms: [6.009101814764841, 2.8570633551535622, 3.829816978129002]\n",
      "47.72841903808163\n",
      "Validation MSE: 48.7325\n",
      "Epoch 256 LR: 0.000975\n",
      "Gradient norms: [4.433637684532644, 2.6441396926081344, 3.9437538782735313]\n",
      "47.493060642240145\n",
      "Validation MSE: 48.7179\n",
      "Epoch 257 LR: 0.000975\n",
      "Gradient norms: [8.675769136976955, 2.633826906236495, 3.7754461185277597]\n",
      "47.60138315418609\n",
      "Validation MSE: 48.6871\n",
      "Epoch 258 LR: 0.000975\n",
      "Gradient norms: [5.800315450715053, 2.649652862381313, 3.473064995503639]\n",
      "48.41756947865068\n",
      "Validation MSE: 48.6328\n",
      "Epoch 259 LR: 0.000975\n",
      "Gradient norms: [5.077979544531065, 2.7089166722420113, 3.693997883761382]\n",
      "47.82729771959589\n",
      "Validation MSE: 48.5825\n",
      "Epoch 260 LR: 0.000975\n",
      "Gradient norms: [4.658591282840394, 2.324367512373455, 4.033461884598154]\n",
      "47.17563098553443\n",
      "Validation MSE: 48.5011\n",
      "Epoch 261 LR: 0.000975\n",
      "Gradient norms: [8.046878864980007, 2.7510900137641245, 3.3766575526858515]\n",
      "48.106319982721935\n",
      "Validation MSE: 48.4509\n",
      "Epoch 262 LR: 0.000974\n",
      "Gradient norms: [5.142860162034622, 2.234703151094123, 3.3047410331629252]\n",
      "48.18563887419482\n",
      "Validation MSE: 48.4055\n",
      "Epoch 263 LR: 0.000974\n",
      "Gradient norms: [4.949425242589664, 2.5573125812790525, 3.9619466963768337]\n",
      "46.74576090292708\n",
      "Validation MSE: 48.3970\n",
      "Epoch 264 LR: 0.000974\n",
      "Gradient norms: [5.602663457015566, 2.8457182513951564, 3.8661541187687574]\n",
      "46.96157734207228\n",
      "Validation MSE: 48.3567\n",
      "Epoch 265 LR: 0.000974\n",
      "Gradient norms: [4.549468515133265, 2.323990129136239, 3.5012102043456523]\n",
      "47.97341432183145\n",
      "Validation MSE: 48.3382\n",
      "Epoch 266 LR: 0.000974\n",
      "Gradient norms: [5.8164804285973215, 2.702181120605647, 3.4426417785489787]\n",
      "48.25975224013198\n",
      "Validation MSE: 48.3035\n",
      "Epoch 267 LR: 0.000974\n",
      "Gradient norms: [6.714383004712445, 2.6177240024189214, 3.0803528990675755]\n",
      "48.86873973118113\n",
      "Validation MSE: 48.3394\n",
      "Epoch 268 LR: 0.000974\n",
      "Gradient norms: [5.574566775711971, 2.427275648350526, 3.4022556746496337]\n",
      "48.14543848776616\n",
      "Validation MSE: 48.3469\n",
      "Epoch 269 LR: 0.000974\n",
      "Gradient norms: [6.671180555900563, 3.0683123392952787, 3.6880557798030322]\n",
      "47.44110034515914\n",
      "Validation MSE: 48.3418\n",
      "Epoch 270 LR: 0.000974\n",
      "Gradient norms: [7.080349998717311, 2.8037935281478865, 3.649400753359673]\n",
      "47.78877884960802\n",
      "Validation MSE: 48.3479\n",
      "Epoch 271 LR: 0.000974\n",
      "Gradient norms: [5.8781834808772055, 2.7686656070648636, 2.8218353434404433]\n",
      "49.135601426814155\n",
      "Validation MSE: 48.3452\n",
      "Epoch 272 LR: 0.000974\n",
      "Gradient norms: [5.366379326443534, 2.453844650920087, 3.9364194857485475]\n",
      "46.61298142672605\n",
      "Validation MSE: 48.2958\n",
      "Epoch 273 LR: 0.000973\n",
      "Gradient norms: [6.210412292767678, 3.3561256929031535, 2.870924418345141]\n",
      "48.909913486357894\n",
      "Validation MSE: 48.2842\n",
      "Epoch 274 LR: 0.000973\n",
      "Gradient norms: [5.3904199205393075, 2.5559400874618485, 3.947270522101431]\n",
      "46.211074501601466\n",
      "Validation MSE: 48.2468\n",
      "Epoch 275 LR: 0.000973\n",
      "Gradient norms: [6.001278716841807, 3.296934618744186, 3.330500350294201]\n",
      "48.20440773060711\n",
      "Validation MSE: 48.1908\n",
      "Epoch 276 LR: 0.000973\n",
      "Gradient norms: [8.113319602869897, 2.3534553319913107, 3.4705462883666742]\n",
      "47.238839073426696\n",
      "Validation MSE: 48.1509\n",
      "Epoch 277 LR: 0.000973\n",
      "Gradient norms: [5.046671138536654, 3.018680254972599, 3.8104033808101225]\n",
      "46.73457931730468\n",
      "Validation MSE: 48.1341\n",
      "Epoch 278 LR: 0.000973\n",
      "Gradient norms: [4.950246351926819, 2.1038938681125954, 3.613606107244182]\n",
      "46.62639204522135\n",
      "Validation MSE: 48.1304\n",
      "Epoch 279 LR: 0.000973\n",
      "Gradient norms: [5.2924395766124395, 2.5168854961231295, 3.5576021734348564]\n",
      "47.20406772066446\n",
      "Validation MSE: 48.1055\n",
      "Epoch 280 LR: 0.000973\n",
      "Gradient norms: [4.915154392777393, 3.2055862283366032, 3.4274469489337087]\n",
      "47.652452648680175\n",
      "Validation MSE: 48.1241\n",
      "Epoch 281 LR: 0.000973\n",
      "Gradient norms: [7.296833881830916, 2.8985492798074843, 3.360169669766096]\n",
      "47.327987744113464\n",
      "Validation MSE: 48.1819\n",
      "Epoch 282 LR: 0.000973\n",
      "Gradient norms: [5.232841741597586, 2.538653105974905, 3.655512992815686]\n",
      "47.30082508857701\n",
      "Validation MSE: 48.2059\n",
      "Epoch 283 LR: 0.000972\n",
      "Gradient norms: [6.525676061431238, 2.6304861907779116, 3.4089760371069833]\n",
      "46.795934240001756\n",
      "Validation MSE: 48.2605\n",
      "Epoch 284 LR: 0.000972\n",
      "Gradient norms: [6.253318560228239, 2.333668496910654, 3.2359012624711436]\n",
      "48.027672145317425\n",
      "Validation MSE: 48.2677\n",
      "Epoch 285 LR: 0.000972\n",
      "Gradient norms: [6.076346841820903, 2.6972484064769353, 3.0234755214243227]\n",
      "47.9013780401334\n",
      "Validation MSE: 48.2662\n",
      "Epoch 286 LR: 0.000972\n",
      "Gradient norms: [6.798174215139025, 3.0578514529250334, 3.248142924991323]\n",
      "48.17340758826702\n",
      "Validation MSE: 48.2872\n",
      "Epoch 287 LR: 0.000972\n",
      "Gradient norms: [7.264943665013776, 2.9240159179477843, 2.6765063832354423]\n",
      "48.640322411280586\n",
      "Validation MSE: 48.2958\n",
      "Epoch 288 LR: 0.000972\n",
      "Gradient norms: [6.09432710844135, 2.8160281442914488, 3.2321918310221784]\n",
      "47.50832318575286\n",
      "Validation MSE: 48.2517\n",
      "Epoch 289 LR: 0.000972\n",
      "Gradient norms: [8.708184310136108, 2.719529164874376, 3.3384053135349485]\n",
      "46.97305019647316\n",
      "Validation MSE: 48.2608\n",
      "Epoch 290 LR: 0.000972\n",
      "Gradient norms: [5.674441776811262, 2.7444762352616094, 3.7079197830409893]\n",
      "46.34198741212804\n",
      "Validation MSE: 48.2720\n",
      "Epoch 291 LR: 0.000972\n",
      "Gradient norms: [7.53224207986771, 2.27685657607727, 3.3758247537694306]\n",
      "46.94441538254207\n",
      "Validation MSE: 48.2704\n",
      "Epoch 292 LR: 0.000972\n",
      "Gradient norms: [8.205011103923, 2.3391958083556026, 3.8529501940734034]\n",
      "46.32365851383097\n",
      "Validation MSE: 48.2508\n",
      "Epoch 293 LR: 0.000972\n",
      "Gradient norms: [6.936679285856481, 2.705357736167741, 3.695256247783644]\n",
      "46.25888013936465\n",
      "Validation MSE: 48.2203\n",
      "Epoch 294 LR: 0.000971\n",
      "Gradient norms: [10.027757494654283, 3.6736259432741467, 3.512439659363673]\n",
      "46.909353517010544\n",
      "Validation MSE: 48.1674\n",
      "Epoch 295 LR: 0.000971\n",
      "Gradient norms: [5.133045392431185, 2.311641434523806, 3.200842633221928]\n",
      "46.746265888497305\n",
      "Validation MSE: 48.1171\n",
      "Epoch 296 LR: 0.000971\n",
      "Gradient norms: [7.484728852615043, 3.6739308495513376, 3.163055601824322]\n",
      "47.35218601324688\n",
      "Validation MSE: 48.1014\n",
      "Epoch 297 LR: 0.000971\n",
      "Gradient norms: [7.138501823583407, 2.5686190730007, 3.612648516058314]\n",
      "46.61961057455107\n",
      "Validation MSE: 48.0726\n",
      "Epoch 298 LR: 0.000971\n",
      "Gradient norms: [5.066098984004166, 2.854895635886565, 3.46766573028876]\n",
      "46.70185158051768\n",
      "Validation MSE: 48.0575\n",
      "Epoch 299 LR: 0.000971\n",
      "Gradient norms: [7.538702468973215, 3.0450505131809225, 3.27437594789481]\n",
      "46.95251680028047\n",
      "Validation MSE: 48.0408\n",
      "Epoch 300 LR: 0.000971\n",
      "Gradient norms: [5.906655767369408, 2.674590262168342, 3.4090200538563975]\n",
      "46.74745621664931\n",
      "Validation MSE: 48.0415\n",
      "Epoch 301 LR: 0.000971\n",
      "Gradient norms: [5.450233901439745, 2.9838760985012858, 3.8470594784867065]\n",
      "46.2079760954479\n",
      "Validation MSE: 48.0327\n",
      "Epoch 302 LR: 0.000971\n",
      "Gradient norms: [7.591690541445853, 2.71774949972504, 3.080821443705576]\n",
      "47.792730206806105\n",
      "Validation MSE: 48.0183\n",
      "Epoch 303 LR: 0.000971\n",
      "Gradient norms: [5.3391478813609305, 3.3706363823602508, 3.164614356101684]\n",
      "46.71652917719989\n",
      "Validation MSE: 47.9628\n",
      "Epoch 304 LR: 0.000970\n",
      "Gradient norms: [7.0685986827732705, 2.8659072876172593, 3.7086137549058247]\n",
      "45.68040728913935\n",
      "Validation MSE: 47.9104\n",
      "Epoch 305 LR: 0.000970\n",
      "Gradient norms: [6.19738613751431, 2.425284230218159, 3.318390963590602]\n",
      "46.80049132160954\n",
      "Validation MSE: 47.8977\n",
      "Epoch 306 LR: 0.000970\n",
      "Gradient norms: [4.908314170996519, 3.2577938767014025, 3.317133833967396]\n",
      "46.549037560605\n",
      "Validation MSE: 47.8678\n",
      "Epoch 307 LR: 0.000970\n",
      "Gradient norms: [5.108484544987969, 2.4322214009908185, 3.027032645569409]\n",
      "47.13395361211735\n",
      "Validation MSE: 47.8238\n",
      "Epoch 308 LR: 0.000970\n",
      "Gradient norms: [5.901263189274922, 2.719836222202442, 3.0061770904501093]\n",
      "46.68351503254363\n",
      "Validation MSE: 47.8096\n",
      "Epoch 309 LR: 0.000970\n",
      "Gradient norms: [4.381499930944397, 2.542172116414775, 2.9716741238402644]\n",
      "47.4219477130927\n",
      "Validation MSE: 47.7944\n",
      "Epoch 310 LR: 0.000970\n",
      "Gradient norms: [7.161969896455237, 3.013239801070396, 2.96868552557183]\n",
      "47.43920928794189\n",
      "Validation MSE: 47.7969\n",
      "Epoch 311 LR: 0.000970\n",
      "Gradient norms: [5.580054745078709, 2.7094110343549396, 3.4452207853601995]\n",
      "45.869994585063665\n",
      "Validation MSE: 47.7632\n",
      "Epoch 312 LR: 0.000970\n",
      "Gradient norms: [6.301232756543102, 3.0276462990707245, 3.493714181165106]\n",
      "45.930346590942285\n",
      "Validation MSE: 47.7529\n",
      "Epoch 313 LR: 0.000970\n",
      "Gradient norms: [7.5460673549756585, 2.22015356352869, 3.188037053547812]\n",
      "46.9333405059501\n",
      "Validation MSE: 47.7284\n",
      "Epoch 314 LR: 0.000970\n",
      "Gradient norms: [7.874639988211381, 3.09869827114684, 3.4666884962895597]\n",
      "45.96342801388659\n",
      "Validation MSE: 47.7220\n",
      "Epoch 315 LR: 0.000969\n",
      "Gradient norms: [5.355523655867407, 2.8953379955697334, 3.8802234614938906]\n",
      "44.76732856350392\n",
      "Validation MSE: 47.7230\n",
      "Epoch 316 LR: 0.000969\n",
      "Gradient norms: [9.70339260125237, 2.7377360671468383, 3.4466612322377967]\n",
      "45.91566671687284\n",
      "Validation MSE: 47.7809\n",
      "Epoch 317 LR: 0.000969\n",
      "Gradient norms: [7.532703679901286, 2.9355019376260874, 3.3386220828557227]\n",
      "46.36318671911099\n",
      "Validation MSE: 47.7853\n",
      "Epoch 318 LR: 0.000969\n",
      "Gradient norms: [8.052722603265675, 3.154701096804177, 3.4360417893929984]\n",
      "45.841467084324606\n",
      "Validation MSE: 47.8099\n",
      "Epoch 319 LR: 0.000969\n",
      "Gradient norms: [8.939734650470324, 3.306412520737544, 2.9259614954197883]\n",
      "46.79633580262223\n",
      "Validation MSE: 47.8327\n",
      "Epoch 320 LR: 0.000969\n",
      "Gradient norms: [7.429608639214759, 2.6158044228482846, 3.665165162487951]\n",
      "45.0092457821758\n",
      "Validation MSE: 47.8652\n",
      "Epoch 321 LR: 0.000969\n",
      "Gradient norms: [6.435999249521904, 2.476033096907073, 3.028888530613146]\n",
      "46.68254558722627\n",
      "Validation MSE: 47.8914\n",
      "Epoch 322 LR: 0.000969\n",
      "Gradient norms: [7.578445876083744, 2.6338155750069925, 2.782875766698029]\n",
      "47.34323695717818\n",
      "Validation MSE: 47.9086\n",
      "Epoch 323 LR: 0.000969\n",
      "Gradient norms: [5.547208032549329, 2.4500302188010576, 3.222940296875052]\n",
      "46.32583966581801\n",
      "Validation MSE: 47.8951\n",
      "Epoch 324 LR: 0.000969\n",
      "Gradient norms: [5.969423087266136, 3.168473934854283, 3.1863877780271395]\n",
      "46.34023651260783\n",
      "Validation MSE: 47.9139\n",
      "Epoch 325 LR: 0.000969\n",
      "Gradient norms: [11.177823871590538, 2.722230733811329, 3.337217824059255]\n",
      "45.64549074198244\n",
      "Validation MSE: 47.9055\n",
      "Epoch 326 LR: 0.000968\n",
      "Gradient norms: [6.261635093590267, 2.6999495178690043, 3.2926928176604817]\n",
      "46.140043659437154\n",
      "Validation MSE: 47.8825\n",
      "Epoch 327 LR: 0.000968\n",
      "Gradient norms: [8.25630605055465, 2.624057838247172, 3.2029603753436966]\n",
      "45.62086756513507\n",
      "Validation MSE: 47.8694\n",
      "Epoch 328 LR: 0.000968\n",
      "Gradient norms: [5.563921249281635, 2.1443666351317128, 2.932893556123685]\n",
      "46.23962780806908\n",
      "Validation MSE: 47.8601\n",
      "Epoch 329 LR: 0.000968\n",
      "Gradient norms: [9.262086030886065, 2.814106469669426, 2.9906914618636216]\n",
      "46.32587941476904\n",
      "Validation MSE: 47.8473\n",
      "Epoch 330 LR: 0.000968\n",
      "Gradient norms: [7.451228370760845, 2.456949751429679, 3.3063099959673976]\n",
      "45.783125554547006\n",
      "Validation MSE: 47.7933\n",
      "Epoch 331 LR: 0.000968\n",
      "Gradient norms: [6.238835758824818, 2.692815334162814, 3.323876670287736]\n",
      "45.44894083988571\n",
      "Validation MSE: 47.7850\n",
      "Epoch 332 LR: 0.000968\n",
      "Gradient norms: [6.215093634854425, 2.927187857733536, 1.9278244359527692]\n",
      "48.8035564688183\n",
      "Validation MSE: 47.7550\n",
      "Epoch 333 LR: 0.000968\n",
      "Gradient norms: [10.927082234612875, 2.1886748962130995, 2.5246926019969282]\n",
      "47.08316921256641\n",
      "Validation MSE: 47.7440\n",
      "Epoch 334 LR: 0.000968\n",
      "Gradient norms: [5.3075138548262295, 3.0105082660833062, 3.383565648963691]\n",
      "45.077378473550866\n",
      "Validation MSE: 47.7247\n",
      "Epoch 335 LR: 0.000968\n",
      "Gradient norms: [6.84931189793115, 2.5984283690002927, 3.1820702726835353]\n",
      "46.074868853501314\n",
      "Validation MSE: 47.6803\n",
      "Epoch 336 LR: 0.000967\n",
      "Gradient norms: [6.861343540518881, 2.5543714531645465, 3.3332005779017764]\n",
      "45.84683226222129\n",
      "Validation MSE: 47.6561\n",
      "Epoch 337 LR: 0.000967\n",
      "Gradient norms: [8.225337166198598, 2.7032351467122773, 3.3575015221427136]\n",
      "45.50150520657419\n",
      "Validation MSE: 47.6274\n",
      "Epoch 338 LR: 0.000967\n",
      "Gradient norms: [6.42837965576683, 2.361139801320351, 3.147047754247101]\n",
      "46.642061040213186\n",
      "Validation MSE: 47.5982\n",
      "Epoch 339 LR: 0.000967\n",
      "Gradient norms: [6.976572059283491, 2.9349727546860263, 3.0566878760133784]\n",
      "46.22245517975802\n",
      "Validation MSE: 47.5818\n",
      "Epoch 340 LR: 0.000967\n",
      "Gradient norms: [8.074683688448989, 3.2741835653837703, 2.3318483257333846]\n",
      "48.364815103531214\n",
      "Validation MSE: 47.5563\n",
      "Epoch 341 LR: 0.000967\n",
      "Gradient norms: [6.7553114524922915, 3.0677981732659987, 2.7940793940760487]\n",
      "46.72428568063903\n",
      "Validation MSE: 47.5031\n",
      "Epoch 342 LR: 0.000967\n",
      "Gradient norms: [5.502908098924018, 2.5251221767828493, 2.6984018533883978]\n",
      "46.56971315092939\n",
      "Validation MSE: 47.5160\n",
      "Epoch 343 LR: 0.000967\n",
      "Gradient norms: [6.648189341920659, 2.9499087455939086, 2.7001594450608346]\n",
      "46.64640930577776\n",
      "Validation MSE: 47.5087\n",
      "Epoch 344 LR: 0.000967\n",
      "Gradient norms: [12.364984136668356, 3.291284908609584, 3.498489650194184]\n",
      "44.574580230289044\n",
      "Validation MSE: 47.5335\n",
      "Epoch 345 LR: 0.000967\n",
      "Gradient norms: [6.570499924367148, 2.5988534918506048, 3.4210390552111867]\n",
      "44.53751968866313\n",
      "Validation MSE: 47.5300\n",
      "Epoch 346 LR: 0.000967\n",
      "Gradient norms: [10.967844885428118, 3.0768209974766174, 2.867547663876323]\n",
      "45.91524277551183\n",
      "Validation MSE: 47.5402\n",
      "Epoch 347 LR: 0.000966\n",
      "Gradient norms: [7.674876444291485, 2.5322785236566974, 3.243702258304]\n",
      "44.855694002800966\n",
      "Validation MSE: 47.5323\n",
      "Epoch 348 LR: 0.000966\n",
      "Gradient norms: [7.957394451200324, 3.4364939752805035, 2.8115574849911007]\n",
      "46.52835364823069\n",
      "Validation MSE: 47.5424\n",
      "Epoch 349 LR: 0.000966\n",
      "Gradient norms: [6.6246674179703, 2.457079663687381, 3.5269723540210096]\n",
      "44.15444051213341\n",
      "Validation MSE: 47.5022\n",
      "Epoch 350 LR: 0.000966\n",
      "Gradient norms: [5.583420738265776, 2.25845181190286, 2.7081276274037624]\n",
      "46.26664140532053\n",
      "Validation MSE: 47.4852\n",
      "Epoch 351 LR: 0.000966\n",
      "Gradient norms: [13.168103744296262, 3.0918563117576223, 2.7860461474984595]\n",
      "45.85833297918071\n",
      "Validation MSE: 47.5168\n",
      "Epoch 352 LR: 0.000966\n",
      "Gradient norms: [6.824020238433631, 2.807151557785382, 2.5926367353983197]\n",
      "46.42463378783316\n",
      "Validation MSE: 47.5382\n",
      "Epoch 353 LR: 0.000966\n",
      "Gradient norms: [6.6921065450039325, 3.224779082797229, 2.7323363952066084]\n",
      "46.194128526348365\n",
      "Validation MSE: 47.5435\n",
      "Epoch 354 LR: 0.000966\n",
      "Gradient norms: [7.108538696417007, 3.042148262930013, 2.3335859834210075]\n",
      "46.734150581436445\n",
      "Validation MSE: 47.5213\n",
      "Epoch 355 LR: 0.000966\n",
      "Gradient norms: [8.989793924789186, 2.847093792637261, 2.514057392375856]\n",
      "46.176342707368455\n",
      "Validation MSE: 47.4593\n",
      "Epoch 356 LR: 0.000966\n",
      "Gradient norms: [7.4699665351581, 2.6466746908930245, 2.7665146747525777]\n",
      "45.311868421098374\n",
      "Validation MSE: 47.4209\n",
      "Epoch 357 LR: 0.000966\n",
      "Gradient norms: [7.104668593201558, 3.433934190059775, 3.815182924590231]\n",
      "43.12048332773163\n",
      "Validation MSE: 47.4090\n",
      "Epoch 358 LR: 0.000965\n",
      "Gradient norms: [6.933051341946049, 3.0119038923004853, 3.05866401031925]\n",
      "45.87078047358595\n",
      "Validation MSE: 47.4060\n",
      "Epoch 359 LR: 0.000965\n",
      "Gradient norms: [6.1658982830564995, 2.9965773729733325, 3.3130515175370427]\n",
      "44.06407651988589\n",
      "Validation MSE: 47.3836\n",
      "Epoch 360 LR: 0.000965\n",
      "Gradient norms: [5.342288195678524, 2.469457187804413, 2.6904513293310552]\n",
      "46.30188029357351\n",
      "Validation MSE: 47.3849\n",
      "Epoch 361 LR: 0.000965\n",
      "Gradient norms: [6.547820596649984, 2.678504731457076, 2.883726009020866]\n",
      "45.52569874139023\n",
      "Validation MSE: 47.3384\n",
      "Epoch 362 LR: 0.000965\n",
      "Gradient norms: [7.49555701922661, 2.6761752534612486, 2.7357319071599098]\n",
      "45.790346616549776\n",
      "Validation MSE: 47.3126\n",
      "Epoch 363 LR: 0.000965\n",
      "Gradient norms: [7.571870189403327, 3.1913765675539474, 2.5831940422027087]\n",
      "46.467083731570895\n",
      "Validation MSE: 47.2809\n",
      "Epoch 364 LR: 0.000965\n",
      "Gradient norms: [9.270969055477876, 3.2407618485411724, 4.197178550383227]\n",
      "42.21296942854676\n",
      "Validation MSE: 47.2947\n",
      "Epoch 365 LR: 0.000965\n",
      "Gradient norms: [7.731625112662373, 2.976719927844306, 2.4317063115649735]\n",
      "46.71647569467929\n",
      "Validation MSE: 47.3038\n",
      "Epoch 366 LR: 0.000965\n",
      "Gradient norms: [8.718719778735943, 2.826274941718543, 2.360255178806605]\n",
      "46.63665185717928\n",
      "Validation MSE: 47.3061\n",
      "Epoch 367 LR: 0.000965\n",
      "Gradient norms: [4.962036027813402, 3.248588895352153, 3.3451081023979308]\n",
      "45.21600919365504\n",
      "Validation MSE: 47.3140\n",
      "Epoch 368 LR: 0.000965\n",
      "Gradient norms: [7.753824467227245, 3.5604907225602784, 2.9360180396922533]\n",
      "45.303699582782905\n",
      "Validation MSE: 47.3391\n",
      "Epoch 369 LR: 0.000964\n",
      "Gradient norms: [8.199629172650235, 2.5412619800092315, 2.3446073191427987]\n",
      "46.8036913951412\n",
      "Validation MSE: 47.3527\n",
      "Epoch 370 LR: 0.000964\n",
      "Gradient norms: [4.891053138298943, 2.869992368708686, 2.7794778826823556]\n",
      "45.40023090237301\n",
      "Validation MSE: 47.3620\n",
      "Epoch 371 LR: 0.000964\n",
      "Gradient norms: [7.162730105852755, 2.917480149862768, 3.2150226523552483]\n",
      "43.871166462077504\n",
      "Validation MSE: 47.4202\n",
      "Epoch 372 LR: 0.000964\n",
      "Gradient norms: [7.785304176959748, 3.1149642833567834, 2.462165340581119]\n",
      "46.67772863018057\n",
      "Validation MSE: 47.4725\n",
      "Epoch 373 LR: 0.000964\n",
      "Gradient norms: [9.270510406003904, 2.469312846857707, 3.1604640896748855]\n",
      "44.76440567820065\n",
      "Validation MSE: 47.5056\n",
      "Epoch 374 LR: 0.000964\n",
      "Gradient norms: [5.990624563158128, 2.563648396802396, 2.60346299582848]\n",
      "45.92505003537655\n",
      "Validation MSE: 47.5416\n",
      "Epoch 375 LR: 0.000964\n",
      "Gradient norms: [6.341450712203535, 3.1832223012769063, 2.124130481088569]\n",
      "47.30897960055979\n",
      "Validation MSE: 47.5324\n",
      "Epoch 376 LR: 0.000964\n",
      "Gradient norms: [7.443768748598189, 2.5965252793766544, 3.066959560380411]\n",
      "44.476106761313595\n",
      "Validation MSE: 47.5450\n",
      "Epoch 377 LR: 0.000964\n",
      "Gradient norms: [5.509679853095381, 2.5884895329360056, 3.0938952314422408]\n",
      "44.235951339711015\n",
      "Validation MSE: 47.5690\n",
      "Epoch 378 LR: 0.000964\n",
      "Gradient norms: [9.604061839567517, 3.7479184058577917, 2.311385336555417]\n",
      "46.518821752641955\n",
      "Validation MSE: 47.5555\n",
      "Epoch 379 LR: 0.000963\n",
      "Gradient norms: [5.84230086183399, 2.6663696690547023, 2.4233997052290546]\n",
      "45.35477584884483\n",
      "Validation MSE: 47.5448\n",
      "Epoch 380 LR: 0.000963\n",
      "Gradient norms: [6.901489959860754, 2.8167501615885273, 2.6636786450880727]\n",
      "45.3152662593049\n",
      "Validation MSE: 47.5386\n",
      "Epoch 381 LR: 0.000963\n",
      "Gradient norms: [7.100922589312456, 3.73286617351093, 2.8372730759893554]\n",
      "45.262320695044174\n",
      "Validation MSE: 47.5541\n",
      "Epoch 382 LR: 0.000963\n",
      "Gradient norms: [6.770016722177678, 3.5027673851852015, 2.826185377818204]\n",
      "44.75403366614781\n",
      "Validation MSE: 47.5177\n",
      "Epoch 383 LR: 0.000963\n",
      "Gradient norms: [8.007979613080009, 3.8064866342440675, 2.2156794566209927]\n",
      "46.702170771270985\n",
      "Validation MSE: 47.4772\n",
      "Epoch 384 LR: 0.000963\n",
      "Gradient norms: [6.343543668227726, 3.2629639484569086, 2.585999956775388]\n",
      "45.60669511945199\n",
      "Validation MSE: 47.4124\n",
      "Epoch 385 LR: 0.000963\n",
      "Gradient norms: [8.176377703051052, 3.1303716836503996, 2.9944220878547654]\n",
      "44.289225099377745\n",
      "Validation MSE: 47.3593\n",
      "Epoch 386 LR: 0.000963\n",
      "Gradient norms: [9.099788358706816, 2.6945856391153367, 3.2812995565405463]\n",
      "44.019170334293754\n",
      "Validation MSE: 47.3322\n",
      "Epoch 387 LR: 0.000963\n",
      "Gradient norms: [5.942385494498928, 2.4327077896751934, 2.0214212757988186]\n",
      "46.52536987291075\n",
      "Validation MSE: 47.3024\n",
      "Epoch 388 LR: 0.000963\n",
      "Gradient norms: [8.103893007061165, 2.8325607039092233, 2.024637698173223]\n",
      "46.65019530239247\n",
      "Validation MSE: 47.2583\n",
      "Epoch 389 LR: 0.000963\n",
      "Gradient norms: [7.315860962980175, 3.174012891567751, 2.364588480172406]\n",
      "45.27788259673195\n",
      "Validation MSE: 47.2722\n",
      "Epoch 390 LR: 0.000962\n",
      "Gradient norms: [7.159469675582687, 3.0191455855899245, 2.38524870472911]\n",
      "45.35471007507649\n",
      "Validation MSE: 47.2715\n",
      "Epoch 391 LR: 0.000962\n",
      "Gradient norms: [9.344010119627399, 2.9005611216664797, 1.985504760673756]\n",
      "46.97905775539199\n",
      "Validation MSE: 47.2319\n",
      "Epoch 392 LR: 0.000962\n",
      "Gradient norms: [8.731807048063452, 3.1714543867127047, 2.520383762438695]\n",
      "45.09522914885958\n",
      "Validation MSE: 47.2188\n",
      "Epoch 393 LR: 0.000962\n",
      "Gradient norms: [8.525738514020736, 2.872370015798065, 2.7666111938894584]\n",
      "44.386326140147865\n",
      "Validation MSE: 47.2217\n",
      "Epoch 394 LR: 0.000962\n",
      "Gradient norms: [12.318958815620169, 2.7234856954002455, 2.2405242679567556]\n",
      "46.03906695250067\n",
      "Validation MSE: 47.1855\n",
      "Epoch 395 LR: 0.000962\n",
      "Gradient norms: [8.50730841351234, 2.781612393296748, 2.6402338532919147]\n",
      "45.54756595042386\n",
      "Validation MSE: 47.1592\n",
      "Epoch 396 LR: 0.000962\n",
      "Gradient norms: [7.728734959576489, 3.1888795421339458, 3.2502888816261266]\n",
      "43.57644412306263\n",
      "Validation MSE: 47.1503\n",
      "Epoch 397 LR: 0.000962\n",
      "Gradient norms: [9.32604628099756, 2.6046216703347977, 2.3965520747857347]\n",
      "45.04623206450726\n",
      "Validation MSE: 47.1427\n",
      "Epoch 398 LR: 0.000962\n",
      "Gradient norms: [8.300544059936128, 3.043251797257799, 2.399535430010512]\n",
      "45.07283265339802\n",
      "Validation MSE: 47.1341\n",
      "Epoch 399 LR: 0.000962\n",
      "Gradient norms: [8.984713884352544, 3.150507361053448, 2.9252790410755853]\n",
      "44.507447797995546\n",
      "Validation MSE: 47.1724\n",
      "Epoch 400 LR: 0.000962\n",
      "Gradient norms: [10.987667897978978, 2.9493209224552968, 2.072256099416774]\n",
      "45.81648937031223\n",
      "Validation MSE: 47.2063\n",
      "Epoch 401 LR: 0.000961\n",
      "Gradient norms: [8.594492995936367, 2.9788749470925726, 2.5106430022123196]\n",
      "45.741804364538645\n",
      "Validation MSE: 47.2433\n",
      "Epoch 402 LR: 0.000961\n",
      "Gradient norms: [8.912818805401747, 2.5721716355366273, 2.4324246750841176]\n",
      "44.46680172734389\n",
      "Validation MSE: 47.2783\n",
      "Epoch 403 LR: 0.000961\n",
      "Gradient norms: [10.48683571789224, 2.745239071972379, 2.709739373129722]\n",
      "44.5193064483798\n",
      "Validation MSE: 47.2949\n",
      "Epoch 404 LR: 0.000961\n",
      "Gradient norms: [8.496314843024896, 3.512417902427315, 2.189850340564483]\n",
      "45.95605673507874\n",
      "Validation MSE: 47.3470\n",
      "Epoch 405 LR: 0.000961\n",
      "Gradient norms: [6.632333332792914, 3.0698863884108705, 2.7375417594625815]\n",
      "44.909367540629184\n",
      "Validation MSE: 47.3670\n",
      "Epoch 406 LR: 0.000961\n",
      "Gradient norms: [8.872969905737211, 3.057747103692681, 2.266163200604292]\n",
      "45.46366700467312\n",
      "Validation MSE: 47.4214\n",
      "Epoch 407 LR: 0.000961\n",
      "Gradient norms: [8.25290713024418, 2.986830617323114, 2.02024263235773]\n",
      "46.54294099234414\n",
      "Validation MSE: 47.4015\n",
      "Epoch 408 LR: 0.000961\n",
      "Gradient norms: [8.93658286173187, 4.031582581096398, 2.6573296531924493]\n",
      "44.85496340929442\n",
      "Validation MSE: 47.3773\n",
      "Epoch 409 LR: 0.000961\n",
      "Gradient norms: [9.360737277129687, 3.567111154443279, 2.557694495507815]\n",
      "44.71921413373472\n",
      "Validation MSE: 47.4108\n",
      "Epoch 410 LR: 0.000961\n",
      "Gradient norms: [12.914527522353344, 2.5156614893619023, 2.5864538876597516]\n",
      "44.80545713612651\n",
      "Validation MSE: 47.3713\n",
      "Epoch 411 LR: 0.000961\n",
      "Gradient norms: [6.4408082765595935, 2.7564426566761004, 2.6089721264960226]\n",
      "44.72331388374239\n",
      "Validation MSE: 47.3635\n",
      "Epoch 412 LR: 0.000960\n",
      "Gradient norms: [9.519802276381977, 2.950995254877841, 2.1920776479050814]\n",
      "45.580108492920225\n",
      "Validation MSE: 47.3734\n",
      "Epoch 413 LR: 0.000960\n",
      "Gradient norms: [7.729646953396146, 3.025695046081875, 2.7802387292045463]\n",
      "44.601197098236526\n",
      "Validation MSE: 47.3779\n",
      "Epoch 414 LR: 0.000960\n",
      "Gradient norms: [9.794392355793322, 2.966498578846279, 2.3213572712687434]\n",
      "44.81458225257286\n",
      "Validation MSE: 47.3901\n",
      "Epoch 415 LR: 0.000960\n",
      "Gradient norms: [6.725561678052089, 3.0148990237188262, 2.6383630988816984]\n",
      "44.012245378418825\n",
      "Validation MSE: 47.3827\n",
      "Epoch 416 LR: 0.000960\n",
      "Gradient norms: [7.6064395181805295, 3.0796470011580137, 2.265548145582323]\n",
      "44.63436282892338\n",
      "Validation MSE: 47.3708\n",
      "Epoch 417 LR: 0.000960\n",
      "Gradient norms: [6.82815860363833, 2.8252842644402416, 2.9029866464085576]\n",
      "43.41732914820764\n",
      "Validation MSE: 47.3658\n",
      "Epoch 418 LR: 0.000960\n",
      "Gradient norms: [12.522047867710683, 2.8398780892978874, 2.3200231248232193]\n",
      "44.82882792793862\n",
      "Validation MSE: 47.3448\n",
      "Epoch 419 LR: 0.000960\n",
      "Gradient norms: [7.015491966374341, 2.481956259472733, 2.445897634241266]\n",
      "43.827698605653175\n",
      "Validation MSE: 47.3271\n",
      "Epoch 420 LR: 0.000960\n",
      "Gradient norms: [9.408985807865708, 3.476459827123499, 2.003797613996045]\n",
      "45.179281775055124\n",
      "Validation MSE: 47.2854\n",
      "Epoch 421 LR: 0.000960\n",
      "Gradient norms: [8.283976731581657, 3.084775156526015, 2.065721137494147]\n",
      "45.92327762301987\n",
      "Validation MSE: 47.2738\n",
      "Epoch 422 LR: 0.000960\n",
      "Gradient norms: [9.385063184331473, 3.1010046994252316, 2.709294136158803]\n",
      "43.62242477032551\n",
      "Validation MSE: 47.2862\n",
      "Epoch 423 LR: 0.000959\n",
      "Gradient norms: [8.13064199204456, 3.186277035478448, 2.786131294382359]\n",
      "43.839112282039984\n",
      "Validation MSE: 47.2886\n",
      "Epoch 424 LR: 0.000959\n",
      "Gradient norms: [7.542722033857953, 3.167011389057235, 2.6460210973111833]\n",
      "44.09945115280405\n",
      "Validation MSE: 47.3171\n",
      "Epoch 425 LR: 0.000959\n",
      "Gradient norms: [10.68629882797285, 3.6780555898905027, 2.12651093692783]\n",
      "45.52790371086084\n",
      "Validation MSE: 47.3521\n",
      "Epoch 426 LR: 0.000959\n",
      "Gradient norms: [10.049871410801881, 3.0357529620289108, 2.367650810183354]\n",
      "44.48966043738975\n",
      "Validation MSE: 47.3173\n",
      "Epoch 427 LR: 0.000959\n",
      "Gradient norms: [9.882225053267247, 3.459418774584635, 2.9576041942309685]\n",
      "43.216978073303956\n",
      "Validation MSE: 47.3001\n",
      "Epoch 428 LR: 0.000959\n",
      "Gradient norms: [7.701480300654516, 3.1453367603255127, 2.7169793333957872]\n",
      "43.023133718746855\n",
      "Validation MSE: 47.2793\n",
      "Epoch 429 LR: 0.000959\n",
      "Gradient norms: [8.909472814137338, 3.2581807509044833, 2.2587243599690123]\n",
      "44.06949690896226\n",
      "Validation MSE: 47.2631\n",
      "Epoch 430 LR: 0.000959\n",
      "Gradient norms: [9.29383364502626, 2.6579427346617557, 1.749376090397575]\n",
      "46.189454807020006\n",
      "Validation MSE: 47.2327\n",
      "Epoch 431 LR: 0.000959\n",
      "Gradient norms: [8.888050257266254, 3.586792699924428, 2.9412486426749074]\n",
      "42.660295208366\n",
      "Validation MSE: 47.2174\n",
      "Epoch 432 LR: 0.000959\n",
      "Gradient norms: [6.859239288621537, 3.3700726445290936, 1.9988188570756928]\n",
      "46.96983673021211\n",
      "Validation MSE: 47.2501\n",
      "Epoch 433 LR: 0.000958\n",
      "Gradient norms: [9.519336375159225, 2.7405674813485374, 2.4483119961767095]\n",
      "44.64695680550922\n",
      "Validation MSE: 47.2346\n",
      "Epoch 434 LR: 0.000958\n",
      "Gradient norms: [11.459080373918155, 3.5452617078262145, 3.033926579162493]\n",
      "42.72655861229287\n",
      "Validation MSE: 47.2583\n",
      "Epoch 435 LR: 0.000958\n",
      "Gradient norms: [10.407141222574968, 3.399507290334167, 2.06770706157131]\n",
      "44.70595791443939\n",
      "Validation MSE: 47.2584\n",
      "Epoch 436 LR: 0.000958\n",
      "Gradient norms: [14.683602355817758, 3.2569596037941495, 2.3156706697478153]\n",
      "44.15873136083796\n",
      "Validation MSE: 47.2488\n",
      "Epoch 437 LR: 0.000958\n",
      "Gradient norms: [9.23351269295756, 3.3916693629272068, 2.358993738014246]\n",
      "44.43315664739401\n",
      "Validation MSE: 47.2405\n",
      "Epoch 438 LR: 0.000958\n",
      "Gradient norms: [9.312928881780081, 3.4980536569658205, 1.5684041933722483]\n",
      "46.76466600015414\n",
      "Validation MSE: 47.2157\n",
      "Epoch 439 LR: 0.000958\n",
      "Gradient norms: [7.762818166803895, 3.2973589186090315, 2.81439714125992]\n",
      "44.09349776908171\n",
      "Validation MSE: 47.1961\n",
      "Epoch 440 LR: 0.000958\n",
      "Gradient norms: [12.719793506413149, 3.192951576509755, 2.349619582167821]\n",
      "44.053966947712375\n",
      "Validation MSE: 47.1733\n",
      "Epoch 441 LR: 0.000958\n",
      "Gradient norms: [6.253824914555105, 3.3612700090204433, 2.487988056782323]\n",
      "43.63969714770521\n",
      "Validation MSE: 47.1419\n",
      "Epoch 442 LR: 0.000958\n",
      "Gradient norms: [8.597120773854446, 3.4370485288201547, 2.985628172743601]\n",
      "43.20674999960359\n",
      "Validation MSE: 47.1110\n",
      "Epoch 443 LR: 0.000958\n",
      "Gradient norms: [7.852412748986341, 3.7065744009136505, 1.989852446506313]\n",
      "45.198178638029574\n",
      "Validation MSE: 47.1091\n",
      "Epoch 444 LR: 0.000957\n",
      "Gradient norms: [9.220961280072528, 2.815900863160732, 2.204585548585148]\n",
      "43.79467393730454\n",
      "Validation MSE: 47.0871\n",
      "Epoch 445 LR: 0.000957\n",
      "Gradient norms: [10.830117862816518, 3.1407841256408475, 1.8788929834595598]\n",
      "45.84474675775015\n",
      "Validation MSE: 47.0602\n",
      "Epoch 446 LR: 0.000957\n",
      "Gradient norms: [11.275358843908498, 3.2515729219425054, 1.8328658636678843]\n",
      "45.43388663817606\n",
      "Validation MSE: 47.0329\n",
      "Epoch 447 LR: 0.000957\n",
      "Gradient norms: [7.767220435872238, 2.7762682890767887, 2.278525622864726]\n",
      "43.759883638904675\n",
      "Validation MSE: 47.0182\n",
      "Epoch 448 LR: 0.000957\n",
      "Gradient norms: [10.550448979655204, 2.87540656612369, 2.269184875518902]\n",
      "43.99579474044719\n",
      "Validation MSE: 47.0060\n",
      "Epoch 449 LR: 0.000957\n",
      "Gradient norms: [8.606489892268787, 2.7468400218142612, 2.07257262595199]\n",
      "44.151958169278004\n",
      "Validation MSE: 46.9811\n",
      "Epoch 450 LR: 0.000957\n",
      "Gradient norms: [6.939644314273704, 3.0688407907678608, 2.4866627643391057]\n",
      "43.22314473502573\n",
      "Validation MSE: 46.9882\n",
      "Epoch 451 LR: 0.000957\n",
      "Gradient norms: [8.73113581346721, 3.8527020836839947, 2.186003442283811]\n",
      "44.29077581554311\n",
      "Validation MSE: 46.9942\n",
      "Epoch 452 LR: 0.000957\n",
      "Gradient norms: [10.572574276427027, 2.956000973788626, 2.7471182978980533]\n",
      "43.107727397731274\n",
      "Validation MSE: 46.9644\n",
      "Epoch 453 LR: 0.000957\n",
      "Gradient norms: [11.394612347159129, 3.4227397145934315, 2.072002643913895]\n",
      "44.707970907102485\n",
      "Validation MSE: 46.9987\n",
      "Epoch 454 LR: 0.000957\n",
      "Gradient norms: [10.675714140783708, 3.310217381926051, 2.5889396164084926]\n",
      "42.86147922133705\n",
      "Validation MSE: 46.9987\n",
      "Epoch 455 LR: 0.000956\n",
      "Gradient norms: [8.906627079445071, 3.13845948298844, 1.4671576575705418]\n",
      "46.253397839587294\n",
      "Validation MSE: 47.0238\n",
      "Epoch 456 LR: 0.000956\n",
      "Gradient norms: [7.097510472770206, 2.5757315382648507, 1.899870462278423]\n",
      "44.84185798047426\n",
      "Validation MSE: 47.0343\n",
      "Epoch 457 LR: 0.000956\n",
      "Gradient norms: [10.7346318252508, 3.07404578789699, 2.7378009829259016]\n",
      "42.98742816029563\n",
      "Validation MSE: 47.0721\n",
      "Epoch 458 LR: 0.000956\n",
      "Gradient norms: [8.665895136038289, 2.9097952623160523, 1.7992543913643577]\n",
      "45.28568611699658\n",
      "Validation MSE: 47.1012\n",
      "Epoch 459 LR: 0.000956\n",
      "Gradient norms: [9.085808505689965, 2.9116697515425476, 2.1237312909248374]\n",
      "43.61739420752192\n",
      "Validation MSE: 47.1362\n",
      "Epoch 460 LR: 0.000956\n",
      "Gradient norms: [9.720141355387607, 2.955582410431599, 2.158932219492079]\n",
      "44.25265364051016\n",
      "Validation MSE: 47.1735\n",
      "Epoch 461 LR: 0.000956\n",
      "Gradient norms: [12.210316872642037, 3.485762911023539, 2.1105596794661015]\n",
      "44.16529193506634\n",
      "Validation MSE: 47.2352\n",
      "Epoch 462 LR: 0.000956\n",
      "Gradient norms: [11.91761360357027, 3.0349333745616773, 2.132304786921534]\n",
      "43.82438083886677\n",
      "Validation MSE: 47.2380\n",
      "Epoch 463 LR: 0.000956\n",
      "Gradient norms: [13.278423518340148, 3.622001951776522, 2.1710084324320773]\n",
      "44.22462792108069\n",
      "Validation MSE: 47.2559\n",
      "Epoch 464 LR: 0.000956\n",
      "Gradient norms: [9.033132463711953, 3.0464356321936172, 2.551752666138454]\n",
      "42.98251064863566\n",
      "Validation MSE: 47.2886\n",
      "Epoch 465 LR: 0.000956\n",
      "Gradient norms: [10.473222705782053, 2.966657232699266, 2.1995168910377654]\n",
      "44.48194335614168\n",
      "Validation MSE: 47.2800\n",
      "Epoch 466 LR: 0.000955\n",
      "Gradient norms: [11.557914032150586, 3.0902816325826943, 1.5549432411053505]\n",
      "45.34049603959064\n",
      "Validation MSE: 47.2992\n",
      "Epoch 467 LR: 0.000955\n",
      "Gradient norms: [11.906627884114167, 3.282386969478433, 2.355077106910339]\n",
      "43.87534002874747\n",
      "Validation MSE: 47.2941\n",
      "Epoch 468 LR: 0.000955\n",
      "Gradient norms: [6.84339677127116, 3.045326142218682, 2.090986538220816]\n",
      "43.37225268763085\n",
      "Validation MSE: 47.2974\n",
      "Epoch 469 LR: 0.000955\n",
      "Gradient norms: [11.0053443713697, 2.9766604388751476, 2.068689934968622]\n",
      "44.30178533629134\n",
      "Validation MSE: 47.3341\n",
      "Epoch 470 LR: 0.000955\n",
      "Gradient norms: [9.992410482624635, 2.9353021960625436, 2.203361128613475]\n",
      "44.690357938886486\n",
      "Validation MSE: 47.3819\n",
      "Epoch 471 LR: 0.000955\n",
      "Gradient norms: [9.518189170091272, 2.7936230359398855, 2.1529399644548026]\n",
      "43.196280340112374\n",
      "Validation MSE: 47.4048\n",
      "Epoch 472 LR: 0.000955\n",
      "Gradient norms: [10.85353853735942, 3.035775805747422, 1.6524119195351124]\n",
      "44.8397880513826\n",
      "Validation MSE: 47.4147\n",
      "Epoch 473 LR: 0.000955\n",
      "Gradient norms: [9.99567932484226, 3.408931860511695, 2.539452548018272]\n",
      "43.08070304363739\n",
      "Validation MSE: 47.4110\n",
      "Epoch 474 LR: 0.000955\n",
      "Gradient norms: [11.0906285971402, 2.911433149864408, 3.2020203081283545]\n",
      "40.01602571422032\n",
      "Validation MSE: 47.4154\n",
      "Epoch 475 LR: 0.000955\n",
      "Gradient norms: [12.463499129049156, 3.614188657222627, 2.537848240045231]\n",
      "43.381539597361964\n",
      "Validation MSE: 47.4508\n",
      "Epoch 476 LR: 0.000955\n",
      "Gradient norms: [10.797075960381536, 3.618265639640085, 2.0979790680806967]\n",
      "43.70697585391865\n",
      "Validation MSE: 47.4705\n",
      "Epoch 477 LR: 0.000954\n",
      "Gradient norms: [8.150724625466987, 3.3675804390919395, 2.383101366812667]\n",
      "43.44429092419201\n",
      "Validation MSE: 47.4776\n",
      "Epoch 478 LR: 0.000954\n",
      "Gradient norms: [10.060485111712879, 3.8889342574469326, 1.6485091021277365]\n",
      "44.90753315920676\n",
      "Validation MSE: 47.4564\n",
      "Epoch 479 LR: 0.000954\n",
      "Gradient norms: [9.9768724250938, 2.867843136903156, 2.1425389929656204]\n",
      "43.53192193603297\n",
      "Validation MSE: 47.4386\n",
      "Epoch 480 LR: 0.000954\n",
      "Gradient norms: [11.989983671178374, 3.62083961712181, 2.166457905968111]\n",
      "43.35917664613644\n",
      "Validation MSE: 47.4275\n",
      "Epoch 481 LR: 0.000954\n",
      "Gradient norms: [8.54474988891241, 3.4004103483771986, 2.1555159151061436]\n",
      "43.01731085490942\n",
      "Validation MSE: 47.4008\n",
      "Epoch 482 LR: 0.000954\n",
      "Gradient norms: [8.047012469747845, 3.8530057181553334, 2.3869708930425357]\n",
      "43.339386862967544\n",
      "Validation MSE: 47.3898\n",
      "Epoch 483 LR: 0.000954\n",
      "Gradient norms: [10.879451086064613, 3.1289168706173762, 2.9893155810024887]\n",
      "41.32895435411413\n",
      "Validation MSE: 47.3530\n",
      "Epoch 484 LR: 0.000954\n",
      "Gradient norms: [10.197534141742661, 3.23795255893092, 2.102578291613672]\n",
      "43.389792957237404\n",
      "Validation MSE: 47.3231\n",
      "Epoch 485 LR: 0.000954\n",
      "Gradient norms: [9.588795195244701, 3.063333434262455, 2.021449301122417]\n",
      "43.62154917383706\n",
      "Validation MSE: 47.3216\n",
      "Epoch 486 LR: 0.000954\n",
      "Gradient norms: [8.17092893992155, 3.3205872702804644, 1.9904519790915856]\n",
      "44.1008962120435\n",
      "Validation MSE: 47.3236\n",
      "Epoch 487 LR: 0.000954\n",
      "Gradient norms: [11.277505177840723, 2.8310638603631344, 2.255596663848496]\n",
      "43.82609827798078\n",
      "Validation MSE: 47.2960\n",
      "Epoch 488 LR: 0.000953\n",
      "Gradient norms: [11.515906758306103, 3.1867210494773874, 1.739425996392066]\n",
      "44.982529966496\n",
      "Validation MSE: 47.2621\n",
      "Epoch 489 LR: 0.000953\n",
      "Gradient norms: [13.74336559917947, 2.7976182359740345, 1.7350179507704515]\n",
      "43.69228125295455\n",
      "Validation MSE: 47.2082\n",
      "Epoch 490 LR: 0.000953\n",
      "Gradient norms: [8.507547270646276, 3.5605607637998435, 1.9562593041557053]\n",
      "43.66577116505414\n",
      "Validation MSE: 47.1762\n",
      "Epoch 491 LR: 0.000953\n",
      "Gradient norms: [10.819489697005158, 3.3862366611852495, 2.286393433101371]\n",
      "43.961851247600784\n",
      "Validation MSE: 47.1415\n",
      "Epoch 492 LR: 0.000953\n",
      "Gradient norms: [9.551574246726508, 3.623922243787061, 2.3306719280651977]\n",
      "43.290025161604675\n",
      "Validation MSE: 47.1289\n",
      "Epoch 493 LR: 0.000953\n",
      "Gradient norms: [7.8437300975589785, 2.946415122363693, 2.1890814935811003]\n",
      "42.95200513302498\n",
      "Validation MSE: 47.1172\n",
      "Epoch 494 LR: 0.000953\n",
      "Gradient norms: [12.202348471210641, 3.551331683675275, 2.211989598414402]\n",
      "43.120269802888274\n",
      "Validation MSE: 47.0971\n",
      "Epoch 495 LR: 0.000953\n",
      "Gradient norms: [12.328249116207825, 3.6231805224831075, 1.5387083173311902]\n",
      "44.99067485919974\n",
      "Validation MSE: 47.0834\n",
      "Epoch 496 LR: 0.000953\n",
      "Gradient norms: [13.913920761877419, 3.467482454607316, 1.7767217952360295]\n",
      "44.32063785446194\n",
      "Validation MSE: 47.0765\n",
      "Epoch 497 LR: 0.000953\n",
      "Gradient norms: [12.660644839566954, 2.8986850158556643, 1.7587466722608396]\n",
      "45.03807540187585\n",
      "Validation MSE: 47.0510\n",
      "Epoch 498 LR: 0.000953\n",
      "Gradient norms: [11.805656311872724, 3.994532279053334, 1.2400028183979626]\n",
      "45.91294618230607\n",
      "Validation MSE: 46.9906\n",
      "Epoch 499 LR: 0.000952\n",
      "Gradient norms: [8.280332689189313, 2.931634243824894, 1.966392597434684]\n",
      "43.29357700477905\n",
      "Validation MSE: 46.9684\n",
      "Epoch 500 LR: 0.000952\n",
      "Gradient norms: [8.590691438625385, 3.375350525350552, 1.6595458498376081]\n",
      "45.17064707034243\n",
      "Validation MSE: 46.9178\n",
      "Epoch 501 LR: 0.000952\n",
      "Gradient norms: [10.583215526155579, 3.641754175146968, 2.6343968152655437]\n",
      "41.5280063701437\n",
      "Validation MSE: 46.9074\n",
      "Epoch 502 LR: 0.000952\n",
      "Gradient norms: [9.786633194736549, 3.186553441443181, 1.61454482108495]\n",
      "44.66766839030954\n",
      "Validation MSE: 46.8949\n",
      "Epoch 503 LR: 0.000952\n",
      "Gradient norms: [10.632545574435058, 2.924602502412727, 1.9999598531449359]\n",
      "43.115067347493095\n",
      "Validation MSE: 46.9126\n",
      "Epoch 504 LR: 0.000952\n",
      "Gradient norms: [9.829221715050924, 3.7787458004147756, 1.9522632764637937]\n",
      "44.450379111839766\n",
      "Validation MSE: 46.9091\n",
      "Epoch 505 LR: 0.000952\n",
      "Gradient norms: [10.562671006363143, 3.195189072946787, 1.7070155844083408]\n",
      "44.16519906817036\n",
      "Validation MSE: 46.9189\n",
      "Epoch 506 LR: 0.000952\n",
      "Gradient norms: [8.0639390193644, 3.034728350761597, 1.8613808496834248]\n",
      "43.06767576528785\n",
      "Validation MSE: 46.9454\n",
      "Epoch 507 LR: 0.000952\n",
      "Gradient norms: [9.81487848211121, 3.2100482535400165, 1.8409523660902298]\n",
      "44.101458978425136\n",
      "Validation MSE: 46.9225\n",
      "Epoch 508 LR: 0.000952\n",
      "Gradient norms: [10.124317960995224, 2.739828305841026, 2.1392500105808994]\n",
      "42.81847844572529\n",
      "Validation MSE: 46.9061\n",
      "Epoch 509 LR: 0.000952\n",
      "Gradient norms: [9.153908027403135, 3.500466544728619, 2.028151268446605]\n",
      "43.37650894696044\n",
      "Validation MSE: 46.8701\n",
      "Epoch 510 LR: 0.000951\n",
      "Gradient norms: [8.895458425077523, 2.737511034086351, 1.648638746672261]\n",
      "43.66609001967345\n",
      "Validation MSE: 46.8441\n",
      "Epoch 511 LR: 0.000951\n",
      "Gradient norms: [11.009881884511643, 3.331635225728416, 1.9441688091472862]\n",
      "43.383046598173635\n",
      "Validation MSE: 46.7920\n",
      "Epoch 512 LR: 0.000951\n",
      "Gradient norms: [8.849410072223327, 3.006926771192278, 1.7066397303782959]\n",
      "43.49469829943463\n",
      "Validation MSE: 46.7791\n",
      "Epoch 513 LR: 0.000951\n",
      "Gradient norms: [8.807806703423326, 3.8422164859952175, 2.447670417679397]\n",
      "41.912621497371084\n",
      "Validation MSE: 46.7807\n",
      "Epoch 514 LR: 0.000951\n",
      "Gradient norms: [12.33813513324863, 3.5541157177755727, 2.2519151107708257]\n",
      "42.43207508341746\n",
      "Validation MSE: 46.7750\n",
      "Epoch 515 LR: 0.000951\n",
      "Gradient norms: [9.189335561143421, 3.291446760858628, 2.130319254231062]\n",
      "42.419517145805216\n",
      "Validation MSE: 46.7934\n",
      "Epoch 516 LR: 0.000951\n",
      "Gradient norms: [10.517597908616365, 3.32015450327164, 1.318797965719352]\n",
      "44.41460069816538\n",
      "Validation MSE: 46.7898\n",
      "Epoch 517 LR: 0.000951\n",
      "Gradient norms: [12.125902611173062, 3.3093569454376945, 2.3889730874183166]\n",
      "41.694269026216425\n",
      "Validation MSE: 46.8033\n",
      "Epoch 518 LR: 0.000951\n",
      "Gradient norms: [11.19852215384613, 3.473958594857657, 2.082009090307605]\n",
      "44.391098579789535\n",
      "Validation MSE: 46.8310\n",
      "Epoch 519 LR: 0.000951\n",
      "Gradient norms: [14.497748103456864, 3.568633922297238, 2.139843080192001]\n",
      "42.209837186304\n",
      "Validation MSE: 46.8534\n",
      "Epoch 520 LR: 0.000951\n",
      "Gradient norms: [9.2635253999177, 3.2598849777612795, 2.2861652737488383]\n",
      "42.09765534739422\n",
      "Validation MSE: 46.8908\n",
      "Epoch 521 LR: 0.000950\n",
      "Gradient norms: [9.643051337203108, 2.8634291596942605, 1.9265924645147938]\n",
      "43.29379304588398\n",
      "Validation MSE: 46.8731\n",
      "Epoch 522 LR: 0.000950\n",
      "Gradient norms: [14.73768073417933, 3.028361134405223, 1.8282778590250688]\n",
      "44.01281597042737\n",
      "Validation MSE: 46.8499\n",
      "Epoch 523 LR: 0.000950\n",
      "Gradient norms: [8.879749417979392, 3.257086425950543, 1.7777058109261625]\n",
      "43.42505918691174\n",
      "Validation MSE: 46.8592\n",
      "Epoch 524 LR: 0.000950\n",
      "Gradient norms: [13.399822321889545, 3.0620814925059023, 1.5104710169276887]\n",
      "43.77257947993274\n",
      "Validation MSE: 46.8593\n",
      "Epoch 525 LR: 0.000950\n",
      "Gradient norms: [12.06619498368758, 2.985476693174953, 2.420007998456568]\n",
      "41.63982951400905\n",
      "Validation MSE: 46.8257\n",
      "Epoch 526 LR: 0.000950\n",
      "Gradient norms: [10.510271141945827, 3.522515998875963, 1.7370787126458989]\n",
      "43.65222357697592\n",
      "Validation MSE: 46.8029\n",
      "Epoch 527 LR: 0.000950\n",
      "Gradient norms: [14.069844770334184, 3.5866764759343996, 1.8505823509727695]\n",
      "43.19118511223505\n",
      "Validation MSE: 46.7684\n",
      "Epoch 528 LR: 0.000950\n",
      "Gradient norms: [11.719347723575675, 2.5328323104085664, 1.727696369051278]\n",
      "43.092253013884225\n",
      "Validation MSE: 46.7389\n",
      "Epoch 529 LR: 0.000950\n",
      "Gradient norms: [13.69712337111548, 3.548349844677672, 1.4345917814259799]\n",
      "44.1583274655176\n",
      "Validation MSE: 46.6875\n",
      "Epoch 530 LR: 0.000950\n",
      "Gradient norms: [13.51365736571976, 4.45496633836678, 1.8349507465154196]\n",
      "44.350941410603276\n",
      "Validation MSE: 46.6596\n",
      "Epoch 531 LR: 0.000950\n",
      "Gradient norms: [10.546156060896092, 3.323609055137939, 2.327857509272797]\n",
      "41.95184658309283\n",
      "Validation MSE: 46.6243\n",
      "Epoch 532 LR: 0.000949\n",
      "Gradient norms: [9.105305379160574, 3.6581623588075094, 2.0158022300630924]\n",
      "42.85600034301961\n",
      "Validation MSE: 46.5656\n",
      "Epoch 533 LR: 0.000949\n",
      "Gradient norms: [11.427242244560256, 3.234319426052679, 2.044384562351695]\n",
      "42.06278130997724\n",
      "Validation MSE: 46.5425\n",
      "Epoch 534 LR: 0.000949\n",
      "Gradient norms: [9.440963900634419, 3.263621952315217, 1.551869035811724]\n",
      "43.27089156611352\n",
      "Validation MSE: 46.5234\n",
      "Epoch 535 LR: 0.000949\n",
      "Gradient norms: [14.514649363880936, 3.1888925669379664, 1.9476219646357171]\n",
      "43.509680474333706\n",
      "Validation MSE: 46.5540\n",
      "Epoch 536 LR: 0.000949\n",
      "Gradient norms: [15.412754217923638, 3.1975671853655765, 2.6128333528302172]\n",
      "41.9310863653119\n",
      "Validation MSE: 46.6105\n",
      "Epoch 537 LR: 0.000949\n",
      "Gradient norms: [12.93526836669748, 3.3352636042282393, 1.6195078006558539]\n",
      "44.122908544408844\n",
      "Validation MSE: 46.6731\n",
      "Epoch 538 LR: 0.000949\n",
      "Gradient norms: [15.449955536489897, 2.643161650553835, 1.911425439545772]\n",
      "42.4256780843126\n",
      "Validation MSE: 46.7029\n",
      "Epoch 539 LR: 0.000949\n",
      "Gradient norms: [10.290718215775591, 2.995607909189311, 2.2356412490394555]\n",
      "41.9540862948993\n",
      "Validation MSE: 46.7409\n",
      "Epoch 540 LR: 0.000949\n",
      "Gradient norms: [9.665546876428953, 3.3102280117363403, 2.1841750781876326]\n",
      "42.29274735195689\n",
      "Validation MSE: 46.7565\n",
      "Epoch 541 LR: 0.000949\n",
      "Gradient norms: [13.870805650013695, 3.7278556659881983, 2.0460764185350513]\n",
      "42.647750242429446\n",
      "Validation MSE: 46.7863\n",
      "Epoch 542 LR: 0.000949\n",
      "Gradient norms: [12.262129882445135, 3.4110722516524534, 1.9938137495705672]\n",
      "42.999741976524064\n",
      "Validation MSE: 46.8060\n",
      "Epoch 543 LR: 0.000948\n",
      "Gradient norms: [11.017655024810564, 2.9802040277971957, 1.9006361009017658]\n",
      "42.70017833746506\n",
      "Validation MSE: 46.8268\n",
      "Epoch 544 LR: 0.000948\n",
      "Gradient norms: [15.532582998372778, 3.3723734479503764, 1.5288454291785776]\n",
      "44.56753107986627\n",
      "Validation MSE: 46.8236\n",
      "Epoch 545 LR: 0.000948\n",
      "Gradient norms: [14.899296720378281, 3.440114273399875, 2.011593862219721]\n",
      "42.79837228346894\n",
      "Validation MSE: 46.7870\n",
      "Epoch 546 LR: 0.000948\n",
      "Gradient norms: [8.80980806905563, 3.270934404706733, 1.6320007860595598]\n",
      "43.33862426388646\n",
      "Validation MSE: 46.7824\n",
      "Epoch 547 LR: 0.000948\n",
      "Gradient norms: [10.232087746181355, 3.130383675668935, 1.7758690917236488]\n",
      "42.82577857111284\n",
      "Validation MSE: 46.7905\n",
      "Epoch 548 LR: 0.000948\n",
      "Gradient norms: [9.713368039791376, 3.1543161853142756, 1.6910674099421115]\n",
      "42.571552607121376\n",
      "Validation MSE: 46.8223\n",
      "Epoch 549 LR: 0.000948\n",
      "Gradient norms: [11.126791856568225, 3.788718999217281, 1.9751670054914952]\n",
      "43.13010547128108\n",
      "Validation MSE: 46.8407\n",
      "Epoch 550 LR: 0.000948\n",
      "Gradient norms: [12.93831007551586, 3.4945257889747503, 1.7997634550380355]\n",
      "42.96559142256242\n",
      "Validation MSE: 46.8280\n",
      "Epoch 551 LR: 0.000948\n",
      "Gradient norms: [11.137942410500116, 3.1528101612978885, 1.8180286150020362]\n",
      "42.43566688040446\n",
      "Validation MSE: 46.8350\n",
      "Epoch 552 LR: 0.000948\n",
      "Gradient norms: [12.24679487579935, 3.6886423766225884, 1.8362323455428182]\n",
      "42.125794272727425\n",
      "Validation MSE: 46.8510\n",
      "Epoch 553 LR: 0.000948\n",
      "Gradient norms: [12.310223983339597, 3.0809695776638524, 2.368051761430225]\n",
      "41.731110228065894\n",
      "Validation MSE: 46.8463\n",
      "Epoch 554 LR: 0.000948\n",
      "Gradient norms: [16.169835855692853, 3.696241036566737, 1.0601704408387855]\n",
      "45.357279354049375\n",
      "Validation MSE: 46.7963\n",
      "Epoch 555 LR: 0.000947\n",
      "Gradient norms: [9.789670717679128, 2.872341490982555, 1.8830544926490405]\n",
      "42.04939513165867\n",
      "Validation MSE: 46.7570\n",
      "Epoch 556 LR: 0.000947\n",
      "Gradient norms: [8.9182432944322, 3.5990664790449323, 1.6035358296652154]\n",
      "43.42148238703571\n",
      "Validation MSE: 46.7284\n",
      "Epoch 557 LR: 0.000947\n",
      "Gradient norms: [11.180677813387078, 3.9961219995726305, 1.8503976470666796]\n",
      "43.30535651067573\n",
      "Validation MSE: 46.6912\n",
      "Epoch 558 LR: 0.000947\n",
      "Gradient norms: [10.834587221619225, 3.350357328842752, 1.5920853905121644]\n",
      "43.221654174194356\n",
      "Validation MSE: 46.6286\n",
      "Epoch 559 LR: 0.000947\n",
      "Gradient norms: [13.008524390063164, 3.40100030334326, 1.2615707762499506]\n",
      "44.53178468738893\n",
      "Validation MSE: 46.6072\n",
      "Epoch 560 LR: 0.000947\n",
      "Gradient norms: [11.067527554226364, 3.4646061280595375, 1.629835320080678]\n",
      "42.66632824276961\n",
      "Validation MSE: 46.5863\n",
      "Epoch 561 LR: 0.000947\n",
      "Gradient norms: [12.249905629399494, 3.585653442373887, 2.171983458882228]\n",
      "41.60842169386554\n",
      "Validation MSE: 46.5530\n",
      "Epoch 562 LR: 0.000947\n",
      "Gradient norms: [12.759518138788962, 3.877046603455113, 1.6078089610402277]\n",
      "43.97272608997549\n",
      "Validation MSE: 46.5518\n",
      "Epoch 563 LR: 0.000947\n",
      "Gradient norms: [13.310914390738942, 3.641400264534732, 1.4634793196373153]\n",
      "43.697582109110144\n",
      "Validation MSE: 46.5717\n",
      "Epoch 564 LR: 0.000947\n",
      "Gradient norms: [14.778701469389913, 3.8287429841372718, 1.2150720176274892]\n",
      "45.67942587161626\n",
      "Validation MSE: 46.5555\n",
      "Epoch 565 LR: 0.000947\n",
      "Gradient norms: [14.296243870778124, 3.1331436211109054, 2.0618181749799334]\n",
      "42.655258094531945\n",
      "Validation MSE: 46.5762\n",
      "Epoch 566 LR: 0.000946\n",
      "Gradient norms: [12.207115470777968, 3.690731963597946, 1.5849177640254277]\n",
      "43.19199937216775\n",
      "Validation MSE: 46.5780\n",
      "Epoch 567 LR: 0.000946\n",
      "Gradient norms: [12.757778997135325, 3.9532647901629425, 1.4246669361516662]\n",
      "43.55745830852406\n",
      "Validation MSE: 46.6108\n",
      "Epoch 568 LR: 0.000946\n",
      "Gradient norms: [11.303336185925053, 3.3631629535595837, 1.959965497012186]\n",
      "41.83462631970784\n",
      "Validation MSE: 46.6700\n",
      "Epoch 569 LR: 0.000946\n",
      "Gradient norms: [10.637317615232599, 3.198017278151021, 1.8484954868042571]\n",
      "42.9864281590139\n",
      "Validation MSE: 46.7462\n",
      "Epoch 570 LR: 0.000946\n",
      "Gradient norms: [9.947917916714877, 3.466896826303605, 1.5906011638917217]\n",
      "43.35995252525784\n",
      "Validation MSE: 46.8124\n",
      "Epoch 571 LR: 0.000946\n",
      "Gradient norms: [10.496495485915041, 3.4076002586307736, 1.9915178109893097]\n",
      "41.420982819602585\n",
      "Validation MSE: 46.8852\n",
      "Epoch 572 LR: 0.000946\n",
      "Gradient norms: [6.973407732132454, 3.5289669585952095, 1.802245816679395]\n",
      "42.263097041942984\n",
      "Validation MSE: 46.9681\n",
      "Epoch 573 LR: 0.000946\n",
      "Gradient norms: [14.505861567028589, 3.506410969691052, 1.8427459890882094]\n",
      "41.95021104038078\n",
      "Validation MSE: 46.9868\n",
      "Epoch 574 LR: 0.000946\n",
      "Gradient norms: [9.831719549581624, 3.6751196013744725, 1.6388993962808773]\n",
      "41.86988681708652\n",
      "Validation MSE: 47.0164\n",
      "Epoch 575 LR: 0.000946\n",
      "Gradient norms: [10.492067779560955, 3.1964684957720477, 1.641687029756762]\n",
      "43.145936990905135\n",
      "Validation MSE: 47.0982\n",
      "Epoch 576 LR: 0.000946\n",
      "Gradient norms: [13.873969735158685, 3.2490759059666967, 1.9453942679845724]\n",
      "41.90860703614381\n",
      "Validation MSE: 47.1649\n",
      "Epoch 577 LR: 0.000945\n",
      "Gradient norms: [15.354696411619766, 3.6357074089669026, 1.310766810267701]\n",
      "43.55939827866426\n",
      "Validation MSE: 47.1492\n",
      "Epoch 578 LR: 0.000945\n",
      "Gradient norms: [8.734725962924998, 4.318025846647163, 2.644023881105921]\n",
      "40.36533951255874\n",
      "Validation MSE: 47.1431\n",
      "Epoch 579 LR: 0.000945\n",
      "Gradient norms: [9.890930391112635, 3.5089328772831534, 1.4610309400825354]\n",
      "42.852810424721234\n",
      "Validation MSE: 47.1284\n",
      "Epoch 580 LR: 0.000945\n",
      "Gradient norms: [12.944370125797754, 3.781164311772134, 1.828358974985988]\n",
      "43.4805408553133\n",
      "Validation MSE: 47.1421\n",
      "Epoch 581 LR: 0.000945\n",
      "Gradient norms: [10.605371429682648, 4.134713603863567, 2.130026333002286]\n",
      "41.614638375324176\n",
      "Validation MSE: 47.1097\n",
      "Epoch 582 LR: 0.000945\n",
      "Gradient norms: [11.463977981898067, 4.478313132767882, 1.573520156152478]\n",
      "42.828320201035226\n",
      "Validation MSE: 47.0907\n",
      "Epoch 583 LR: 0.000945\n",
      "Gradient norms: [13.471910798811988, 3.3015308117423983, 1.4819111546607049]\n",
      "43.789274548601036\n",
      "Validation MSE: 47.0651\n",
      "Epoch 584 LR: 0.000945\n",
      "Gradient norms: [11.624692709198932, 4.054391936423326, 1.293608194027339]\n",
      "44.04120404854872\n",
      "Validation MSE: 47.0173\n",
      "Epoch 585 LR: 0.000945\n",
      "Gradient norms: [10.351892972712331, 3.043617313616518, 1.661003404409254]\n",
      "43.02340682536314\n",
      "Validation MSE: 46.9961\n",
      "Epoch 586 LR: 0.000945\n",
      "Gradient norms: [12.199682537157326, 3.4960143425565446, 1.5123070874038846]\n",
      "44.76834414061539\n",
      "Validation MSE: 46.9698\n",
      "Epoch 587 LR: 0.000945\n",
      "Gradient norms: [11.901432161346237, 4.1232631194881515, 0.8768988150786131]\n",
      "44.45102256792939\n",
      "Validation MSE: 46.9282\n",
      "Epoch 588 LR: 0.000944\n",
      "Gradient norms: [14.805878064311786, 3.8072445207523558, 1.4048989874831344]\n",
      "44.823825157536625\n",
      "Validation MSE: 46.8571\n",
      "Epoch 589 LR: 0.000944\n",
      "Gradient norms: [13.863813919451745, 3.554863691721954, 1.3886821654863641]\n",
      "43.34605291728428\n",
      "Validation MSE: 46.7963\n",
      "Epoch 590 LR: 0.000944\n",
      "Gradient norms: [19.142036992468228, 3.7262042775623128, 2.069643132409035]\n",
      "40.77977343708347\n",
      "Validation MSE: 46.8124\n",
      "Epoch 591 LR: 0.000944\n",
      "Gradient norms: [11.322011908337947, 3.3635715810059077, 1.9334451555004533]\n",
      "42.330737934146235\n",
      "Validation MSE: 46.8258\n",
      "Epoch 592 LR: 0.000944\n",
      "Gradient norms: [11.795445220795983, 3.918963497890118, 1.462368338633127]\n",
      "45.01010851021222\n",
      "Validation MSE: 46.8589\n",
      "Epoch 593 LR: 0.000944\n",
      "Gradient norms: [15.287540670473433, 3.3397797286933244, 1.537528866684043]\n",
      "43.33333107135815\n",
      "Validation MSE: 46.9445\n",
      "Epoch 594 LR: 0.000944\n",
      "Gradient norms: [13.004441329759622, 4.018590702359276, 1.855667871609263]\n",
      "41.47804524426363\n",
      "Validation MSE: 46.9605\n",
      "Epoch 595 LR: 0.000944\n",
      "Gradient norms: [11.336027795938818, 4.722930580229712, 1.8229321547399078]\n",
      "41.71309125245899\n",
      "Validation MSE: 46.9778\n",
      "Epoch 596 LR: 0.000944\n",
      "Gradient norms: [14.113673935357735, 3.2920324562177767, 2.1772194206836315]\n",
      "40.678761350761064\n",
      "Validation MSE: 46.9672\n",
      "Epoch 597 LR: 0.000944\n",
      "Gradient norms: [9.469140524564118, 3.6903298164296072, 1.4836683906400592]\n",
      "43.02093864403664\n",
      "Validation MSE: 46.9818\n",
      "Epoch 598 LR: 0.000944\n",
      "Gradient norms: [9.324938144768053, 3.2645403457401305, 1.7376486522327177]\n",
      "42.22810219036399\n",
      "Validation MSE: 46.9709\n",
      "Epoch 599 LR: 0.000943\n",
      "Gradient norms: [14.664478228645677, 3.8993324335839326, 1.8021030354235872]\n",
      "41.69230530695823\n",
      "Validation MSE: 46.9548\n",
      "Epoch 600 LR: 0.000943\n",
      "Gradient norms: [11.516374152043541, 3.7542686293188763, 1.3691850684406537]\n",
      "43.147039077505426\n",
      "Validation MSE: 46.9686\n",
      "Epoch 601 LR: 0.000943\n",
      "Gradient norms: [11.544102991709188, 3.239286710654155, 1.4066541093123883]\n",
      "42.81712842556933\n",
      "Validation MSE: 46.9967\n",
      "Epoch 602 LR: 0.000943\n",
      "Gradient norms: [13.720164200267678, 3.9168391793108346, 1.138085188180899]\n",
      "43.36673713827263\n",
      "Validation MSE: 47.0247\n",
      "Epoch 603 LR: 0.000943\n",
      "Gradient norms: [12.089169916374852, 4.0140227155436135, 0.9792251161302308]\n",
      "44.79569327414817\n",
      "Validation MSE: 47.0299\n",
      "Epoch 604 LR: 0.000943\n",
      "Gradient norms: [13.777426722903192, 3.1991618253705116, 1.372336385120068]\n",
      "42.994580984504864\n",
      "Validation MSE: 47.0238\n",
      "Epoch 605 LR: 0.000943\n",
      "Gradient norms: [14.897520134416428, 3.6377392723473028, 2.043249748745403]\n",
      "41.465255955767745\n",
      "Validation MSE: 46.9893\n",
      "Epoch 606 LR: 0.000943\n",
      "Gradient norms: [11.347387435945393, 3.0802528157046245, 1.710892809326413]\n",
      "43.06227030048553\n",
      "Validation MSE: 46.9433\n",
      "Epoch 607 LR: 0.000943\n",
      "Gradient norms: [12.12699413604938, 3.7593086595588048, 1.158292669510919]\n",
      "44.45310156055663\n",
      "Validation MSE: 46.9062\n",
      "Epoch 608 LR: 0.000943\n",
      "Gradient norms: [12.598207238600235, 3.732980459030081, 1.7418161011436002]\n",
      "42.466860442357095\n",
      "Validation MSE: 46.8904\n",
      "Epoch 609 LR: 0.000943\n",
      "Gradient norms: [14.409644307651545, 3.5107592110258974, 1.0916978643730821]\n",
      "44.52793887544962\n",
      "Validation MSE: 46.8727\n",
      "Epoch 610 LR: 0.000943\n",
      "Gradient norms: [11.027747873608726, 3.502278737046151, 1.1037044172473918]\n",
      "43.55123130095333\n",
      "Validation MSE: 46.8711\n",
      "Epoch 611 LR: 0.000942\n",
      "Gradient norms: [17.08686533805778, 4.1709447274554154, 1.5452057310366383]\n",
      "43.624805665100915\n",
      "Validation MSE: 46.8295\n",
      "Epoch 612 LR: 0.000942\n",
      "Gradient norms: [12.323834832787288, 3.4342152494074916, 1.5815363421949642]\n",
      "43.03324582744156\n",
      "Validation MSE: 46.8210\n",
      "Epoch 613 LR: 0.000942\n",
      "Gradient norms: [10.842666016071334, 3.9987343897282086, 1.2995468830767711]\n",
      "44.03594148292162\n",
      "Validation MSE: 46.7945\n",
      "Epoch 614 LR: 0.000942\n",
      "Gradient norms: [14.147159376556269, 3.8376399479254912, 1.5590244545608947]\n",
      "42.64933732881523\n",
      "Validation MSE: 46.7893\n",
      "Epoch 615 LR: 0.000942\n",
      "Gradient norms: [12.992280186558187, 4.41857855030718, 1.606319783413279]\n",
      "41.74956148759258\n",
      "Validation MSE: 46.7622\n",
      "Epoch 616 LR: 0.000942\n",
      "Gradient norms: [14.985201750350651, 2.640457336073442, 1.5577682245095497]\n",
      "42.96706733159228\n",
      "Validation MSE: 46.7143\n",
      "Epoch 617 LR: 0.000942\n",
      "Gradient norms: [11.914987382669766, 3.8044097975040887, 1.3155365753862263]\n",
      "43.824317842768856\n",
      "Validation MSE: 46.7031\n",
      "Epoch 618 LR: 0.000942\n",
      "Gradient norms: [13.605884254352453, 3.2712858835937775, 1.7856872862069584]\n",
      "41.55202493557295\n",
      "Validation MSE: 46.6855\n",
      "Epoch 619 LR: 0.000942\n",
      "Gradient norms: [14.000301084929276, 4.088842640794533, 1.553683340896222]\n",
      "42.95007966599258\n",
      "Validation MSE: 46.6723\n",
      "Epoch 620 LR: 0.000942\n",
      "Gradient norms: [12.257264179664933, 3.939869559185833, 1.4533054494523547]\n",
      "42.091991180066586\n",
      "Validation MSE: 46.6484\n",
      "Epoch 621 LR: 0.000942\n",
      "Gradient norms: [12.879684283625389, 3.6710672293245317, 2.2308542259690958]\n",
      "40.648318457605335\n",
      "Validation MSE: 46.6144\n",
      "Epoch 622 LR: 0.000941\n",
      "Gradient norms: [10.62210276314824, 3.215496426020214, 1.3596513359173255]\n",
      "42.324253200298486\n",
      "Validation MSE: 46.5713\n",
      "Epoch 623 LR: 0.000941\n",
      "Gradient norms: [12.200592091227804, 3.1216991191806405, 1.1472414924967387]\n",
      "43.50821748453265\n",
      "Validation MSE: 46.5227\n",
      "Epoch 624 LR: 0.000941\n",
      "Gradient norms: [11.924326288240266, 3.4365465350153643, 1.2332368437095813]\n",
      "43.716257656683744\n",
      "Validation MSE: 46.4662\n",
      "Epoch 625 LR: 0.000941\n",
      "Gradient norms: [13.744215507253001, 3.1828197148389683, 1.324256343548841]\n",
      "42.38175034095834\n",
      "Validation MSE: 46.4673\n",
      "Epoch 626 LR: 0.000941\n",
      "Gradient norms: [15.53497759537731, 3.360894379049855, 1.4030188146944003]\n",
      "43.013909139043605\n",
      "Validation MSE: 46.4907\n",
      "Epoch 627 LR: 0.000941\n",
      "Gradient norms: [9.93045036818255, 2.822739699728473, 1.1840343966175035]\n",
      "43.372033448494705\n",
      "Validation MSE: 46.5073\n",
      "Epoch 628 LR: 0.000941\n",
      "Gradient norms: [11.883762803454587, 3.153106696809733, 1.5718110922652346]\n",
      "41.238028239197334\n",
      "Validation MSE: 46.5253\n",
      "Epoch 629 LR: 0.000941\n",
      "Gradient norms: [16.142931712072063, 2.8647179608108395, 1.1074417849013665]\n",
      "43.96887262978018\n",
      "Validation MSE: 46.5544\n",
      "Epoch 630 LR: 0.000941\n",
      "Gradient norms: [16.496388015535913, 3.4514464992522496, 1.717668856823095]\n",
      "41.34914406954396\n",
      "Validation MSE: 46.5941\n",
      "Epoch 631 LR: 0.000941\n",
      "Gradient norms: [13.364057504311308, 3.6766450820963943, 1.8425609967992074]\n",
      "41.59035567945621\n",
      "Validation MSE: 46.6460\n",
      "Epoch 632 LR: 0.000941\n",
      "Gradient norms: [14.28261801385632, 3.3492233430748843, 1.2583408103725295]\n",
      "42.999581529261484\n",
      "Validation MSE: 46.6553\n",
      "Epoch 633 LR: 0.000940\n",
      "Gradient norms: [11.77014363663808, 3.1436994159374687, 1.752308272775075]\n",
      "41.6676051764596\n",
      "Validation MSE: 46.6489\n",
      "Epoch 634 LR: 0.000940\n",
      "Gradient norms: [11.494318802025253, 4.264561132132125, 1.3793178739354683]\n",
      "42.662777231401456\n",
      "Validation MSE: 46.6218\n",
      "Epoch 635 LR: 0.000940\n",
      "Gradient norms: [11.899915947428088, 3.423384141229146, 0.8593395285234102]\n",
      "44.48034435210916\n",
      "Validation MSE: 46.5877\n",
      "Epoch 636 LR: 0.000940\n",
      "Gradient norms: [15.63417595010241, 3.816093109658503, 1.1679983211038698]\n",
      "44.39655155747497\n",
      "Validation MSE: 46.5574\n",
      "Epoch 637 LR: 0.000940\n",
      "Gradient norms: [11.793108533990887, 4.1268178825798385, 1.7806793542614403]\n",
      "42.39268881224519\n",
      "Validation MSE: 46.4905\n",
      "Epoch 638 LR: 0.000940\n",
      "Gradient norms: [9.559331838522631, 3.6255221583045194, 1.4654362445922589]\n",
      "42.20368485288\n",
      "Validation MSE: 46.4410\n",
      "Epoch 639 LR: 0.000940\n",
      "Gradient norms: [17.081638122360253, 4.085284898292302, 1.5515283605952892]\n",
      "41.77748086544493\n",
      "Validation MSE: 46.3975\n",
      "Epoch 640 LR: 0.000940\n",
      "Gradient norms: [11.204999165750898, 3.752032547729511, 1.2779942981051453]\n",
      "42.38908219740142\n",
      "Validation MSE: 46.3466\n",
      "Epoch 641 LR: 0.000940\n",
      "Gradient norms: [15.15389708995387, 3.5437817297124576, 1.7874283900724368]\n",
      "41.29779996406513\n",
      "Validation MSE: 46.3389\n",
      "Epoch 642 LR: 0.000940\n",
      "Gradient norms: [15.938136259092298, 3.7463160838869327, 1.303125743932871]\n",
      "44.034793548386205\n",
      "Validation MSE: 46.3416\n",
      "Epoch 643 LR: 0.000940\n",
      "Gradient norms: [12.445624450450568, 3.441894971734229, 2.339417254454915]\n",
      "39.37381271820952\n",
      "Validation MSE: 46.3627\n",
      "Epoch 644 LR: 0.000939\n",
      "Gradient norms: [10.6209835143866, 3.1893881823407146, 1.384369705368752]\n",
      "42.535051049044924\n",
      "Validation MSE: 46.3817\n",
      "Epoch 645 LR: 0.000939\n",
      "Gradient norms: [13.42198376831342, 4.090820638286182, 1.58847977636141]\n",
      "41.62325894973989\n",
      "Validation MSE: 46.3853\n",
      "Epoch 646 LR: 0.000939\n",
      "Gradient norms: [14.598116234401397, 4.281052733598501, 1.6760387898787124]\n",
      "41.42516907621848\n",
      "Validation MSE: 46.4301\n",
      "Epoch 647 LR: 0.000939\n",
      "Gradient norms: [11.090776838323654, 3.3417686221975322, 1.5740147370750144]\n",
      "41.370013081073054\n",
      "Validation MSE: 46.4734\n",
      "Epoch 648 LR: 0.000939\n",
      "Gradient norms: [12.677988953039476, 3.528510445859056, 1.6870130831760617]\n",
      "42.126421882747096\n",
      "Validation MSE: 46.4790\n",
      "Epoch 649 LR: 0.000939\n",
      "Gradient norms: [10.778655937730232, 3.439924193523748, 1.2778352591607323]\n",
      "43.189082575464994\n",
      "Validation MSE: 46.5117\n",
      "Epoch 650 LR: 0.000939\n",
      "Gradient norms: [12.410629396994462, 3.2474129967362106, 1.1889547450464928]\n",
      "42.698090381756735\n",
      "Validation MSE: 46.5430\n",
      "Epoch 651 LR: 0.000939\n",
      "Gradient norms: [11.904363442811627, 3.3510140485844366, 1.6846026641762113]\n",
      "41.66893379048491\n",
      "Validation MSE: 46.5666\n",
      "Epoch 652 LR: 0.000939\n",
      "Gradient norms: [14.566582338352745, 3.5626517102055995, 1.279436327879946]\n",
      "43.01405863396897\n",
      "Validation MSE: 46.6038\n",
      "Epoch 653 LR: 0.000939\n",
      "Gradient norms: [11.342053090136494, 3.19205247611546, 1.0822336792760114]\n",
      "43.13041426285633\n",
      "Validation MSE: 46.6326\n",
      "Epoch 654 LR: 0.000939\n",
      "Gradient norms: [16.964639321823828, 3.502891774147522, 1.4682872532280005]\n",
      "41.10918065606065\n",
      "Validation MSE: 46.6366\n",
      "Epoch 655 LR: 0.000939\n",
      "Gradient norms: [11.87094072324571, 4.003556434600182, 1.2831138263730166]\n",
      "41.98178694549116\n",
      "Validation MSE: 46.6112\n",
      "Epoch 656 LR: 0.000938\n",
      "Gradient norms: [16.089732935672618, 3.7995532052031753, 1.3140243380800227]\n",
      "42.26951050740842\n",
      "Validation MSE: 46.6131\n",
      "Epoch 657 LR: 0.000938\n",
      "Gradient norms: [12.797229116247353, 3.227260668917422, 1.4863337435192474]\n",
      "43.342161182282005\n",
      "Validation MSE: 46.5911\n",
      "Epoch 658 LR: 0.000938\n",
      "Gradient norms: [12.027197723828783, 3.470279728009419, 1.6957080911537776]\n",
      "41.52038859979053\n",
      "Validation MSE: 46.5769\n",
      "Epoch 659 LR: 0.000938\n",
      "Gradient norms: [11.074585467700716, 2.9929166665563347, 1.5397095544687671]\n",
      "41.69735919963467\n",
      "Validation MSE: 46.5589\n",
      "Epoch 660 LR: 0.000938\n",
      "Gradient norms: [16.253530544362697, 3.4893486169639463, 2.0883003457989737]\n",
      "40.82001383743503\n",
      "Validation MSE: 46.5753\n",
      "Epoch 661 LR: 0.000938\n",
      "Gradient norms: [15.510438872932234, 4.030306770754402, 1.344824306457471]\n",
      "43.9458791034691\n",
      "Validation MSE: 46.5625\n",
      "Epoch 662 LR: 0.000938\n",
      "Gradient norms: [13.850988909232193, 4.164373497499776, 1.8708536740518875]\n",
      "41.185823192672906\n",
      "Validation MSE: 46.5703\n",
      "Epoch 663 LR: 0.000938\n",
      "Gradient norms: [10.896396192677905, 3.6896672270121598, 1.5369719835449371]\n",
      "42.814543351909464\n",
      "Validation MSE: 46.5659\n",
      "Epoch 664 LR: 0.000938\n",
      "Gradient norms: [14.413476645415585, 4.365441621170581, 1.6074096749109674]\n",
      "42.18608222888556\n",
      "Validation MSE: 46.5626\n",
      "Epoch 665 LR: 0.000938\n",
      "Gradient norms: [11.895884858105731, 4.121186291064335, 1.303998266774208]\n",
      "43.27494876930599\n",
      "Validation MSE: 46.5598\n",
      "Epoch 666 LR: 0.000938\n",
      "Gradient norms: [10.458272518792764, 4.202199847441729, 1.6388930438658496]\n",
      "42.45723723285889\n",
      "Validation MSE: 46.5439\n",
      "Epoch 667 LR: 0.000937\n",
      "Gradient norms: [12.461231051489468, 3.8079978239958487, 1.6242223519943115]\n",
      "40.967815477721636\n",
      "Validation MSE: 46.5244\n",
      "Epoch 668 LR: 0.000937\n",
      "Gradient norms: [13.506729861035831, 4.769346209634941, 1.3548727878809428]\n",
      "43.69530948800567\n",
      "Validation MSE: 46.5470\n",
      "Epoch 669 LR: 0.000937\n",
      "Gradient norms: [13.20518581029431, 3.356395087340035, 1.1186080694463523]\n",
      "42.574224234340555\n",
      "Validation MSE: 46.5519\n",
      "Epoch 670 LR: 0.000937\n",
      "Gradient norms: [15.511885917847728, 4.077576422024923, 2.0820461215250656]\n",
      "39.939212717623455\n",
      "Validation MSE: 46.5062\n",
      "Epoch 671 LR: 0.000937\n",
      "Gradient norms: [11.518778612762512, 3.5755916345739354, 1.793166106385676]\n",
      "40.89769640782104\n",
      "Validation MSE: 46.4741\n",
      "Epoch 672 LR: 0.000937\n",
      "Gradient norms: [15.099058621058278, 4.30786065790533, 1.8574833506122854]\n",
      "41.26934335928035\n",
      "Validation MSE: 46.4291\n",
      "Epoch 673 LR: 0.000937\n",
      "Gradient norms: [12.799259527070431, 4.001383898497022, 1.6789141860927934]\n",
      "41.05088973980371\n",
      "Validation MSE: 46.3755\n",
      "Epoch 674 LR: 0.000937\n",
      "Gradient norms: [9.844378222734143, 3.0677295809631135, 1.0383114537833584]\n",
      "43.7228188988126\n",
      "Validation MSE: 46.3348\n",
      "Epoch 675 LR: 0.000937\n",
      "Gradient norms: [10.808346352634901, 3.869157700911447, 1.2336240977124224]\n",
      "43.987162734933\n",
      "Validation MSE: 46.3181\n",
      "Epoch 676 LR: 0.000937\n",
      "Gradient norms: [15.131394678930041, 3.0621966864146293, 1.3682106755704861]\n",
      "41.85824058157546\n",
      "Validation MSE: 46.3004\n",
      "Epoch 677 LR: 0.000937\n",
      "Gradient norms: [14.823682098834581, 3.6592919705903997, 2.1058042716064387]\n",
      "39.92810479160815\n",
      "Validation MSE: 46.3088\n",
      "Epoch 678 LR: 0.000937\n",
      "Gradient norms: [14.146186273716978, 3.6727582735528412, 1.3482765794784928]\n",
      "41.7926044972907\n",
      "Validation MSE: 46.3156\n",
      "Epoch 679 LR: 0.000936\n",
      "Gradient norms: [14.012818997080364, 4.1537038444446654, 1.5824175217566374]\n",
      "41.66161902451036\n",
      "Validation MSE: 46.3271\n",
      "Epoch 680 LR: 0.000936\n",
      "Gradient norms: [12.273182441439653, 3.483587452890411, 1.1846629748566282]\n",
      "43.64789576270571\n",
      "Validation MSE: 46.3594\n",
      "Epoch 681 LR: 0.000936\n",
      "Gradient norms: [12.33842209831361, 3.77461560179493, 1.1702247818986031]\n",
      "42.19274210191249\n",
      "Validation MSE: 46.3793\n",
      "Epoch 682 LR: 0.000936\n",
      "Gradient norms: [12.018266388740624, 4.717781117964874, 0.9413747125908916]\n",
      "44.74199227890498\n",
      "Validation MSE: 46.3914\n",
      "Epoch 683 LR: 0.000936\n",
      "Gradient norms: [13.30375263615578, 3.805544387619057, 1.3107132863864284]\n",
      "41.83305888086501\n",
      "Validation MSE: 46.4456\n",
      "Epoch 684 LR: 0.000936\n",
      "Gradient norms: [11.292016686068179, 3.4905693924309844, 1.7419545383490935]\n",
      "40.70213083964828\n",
      "Validation MSE: 46.4946\n",
      "Epoch 685 LR: 0.000936\n",
      "Gradient norms: [13.163443599207717, 3.7038191596144414, 1.1777274496256538]\n",
      "42.913227069306636\n",
      "Validation MSE: 46.5320\n",
      "Epoch 686 LR: 0.000936\n",
      "Gradient norms: [10.768707077023205, 3.4270025756808224, 1.3833875588993003]\n",
      "42.262514061536955\n",
      "Validation MSE: 46.5681\n",
      "Epoch 687 LR: 0.000936\n",
      "Gradient norms: [15.415800634374786, 4.127982151077328, 1.653614997955168]\n",
      "41.17962263269462\n",
      "Validation MSE: 46.6005\n",
      "Epoch 688 LR: 0.000936\n",
      "Gradient norms: [15.33043405313142, 3.924377028487137, 1.3600355341860142]\n",
      "42.28035661149178\n",
      "Validation MSE: 46.5901\n",
      "Epoch 689 LR: 0.000936\n",
      "Gradient norms: [13.780507474614538, 4.281780393994543, 1.6029846170982578]\n",
      "41.21981934740282\n",
      "Validation MSE: 46.5738\n",
      "Epoch 690 LR: 0.000935\n",
      "Gradient norms: [9.154587950456186, 3.397127220730106, 1.489720341963673]\n",
      "40.98745149677218\n",
      "Validation MSE: 46.5670\n",
      "Epoch 691 LR: 0.000935\n",
      "Gradient norms: [12.598186642080227, 3.1673117752504125, 1.0950728313356768]\n",
      "42.27701319204342\n",
      "Validation MSE: 46.5550\n",
      "Epoch 692 LR: 0.000935\n",
      "Gradient norms: [13.328975104266314, 3.495270322843961, 1.2837927528687698]\n",
      "42.61133259513051\n",
      "Validation MSE: 46.5363\n",
      "Epoch 693 LR: 0.000935\n",
      "Gradient norms: [12.565459761816292, 4.147901146821277, 1.5220365539220533]\n",
      "41.38076583030949\n",
      "Validation MSE: 46.5382\n",
      "Epoch 694 LR: 0.000935\n",
      "Gradient norms: [10.40378697394276, 3.2847607334513453, 1.1559996896042533]\n",
      "42.8705365946797\n",
      "Validation MSE: 46.5115\n",
      "Epoch 695 LR: 0.000935\n",
      "Gradient norms: [11.151299151253074, 4.2818025045897095, 1.5116008563883137]\n",
      "41.395771831179516\n",
      "Validation MSE: 46.4902\n",
      "Epoch 696 LR: 0.000935\n",
      "Gradient norms: [15.824563301114106, 4.106769618268259, 1.025445553077506]\n",
      "43.93378787943337\n",
      "Validation MSE: 46.4346\n",
      "Epoch 697 LR: 0.000935\n",
      "Gradient norms: [13.421181236799855, 3.774213220921009, 1.2690517639376269]\n",
      "41.746404115320516\n",
      "Validation MSE: 46.4209\n",
      "Epoch 698 LR: 0.000935\n",
      "Gradient norms: [14.223285278136231, 3.88717096869575, 1.6032916854432655]\n",
      "40.0612254431657\n",
      "Validation MSE: 46.4409\n",
      "Epoch 699 LR: 0.000935\n",
      "Gradient norms: [12.116390212950769, 4.784680889769022, 0.7764598067978352]\n",
      "45.00771450263622\n",
      "Validation MSE: 46.4230\n",
      "Epoch 700 LR: 0.000935\n",
      "Gradient norms: [11.290874407831039, 3.5413750490787383, 1.3694565017383964]\n",
      "41.14990746814542\n",
      "Validation MSE: 46.4230\n",
      "Epoch 701 LR: 0.000934\n",
      "Gradient norms: [16.09735162382085, 4.163174109382411, 1.0686484502902351]\n",
      "44.00382109829718\n",
      "Validation MSE: 46.4217\n",
      "Epoch 702 LR: 0.000934\n",
      "Gradient norms: [13.476301451442898, 4.155693722384257, 1.7436614156225991]\n",
      "40.284820514658364\n",
      "Validation MSE: 46.4319\n",
      "Epoch 703 LR: 0.000934\n",
      "Gradient norms: [13.032734150094305, 3.9091020280946966, 1.2828633212808591]\n",
      "43.202436015393374\n",
      "Validation MSE: 46.4604\n",
      "Epoch 704 LR: 0.000934\n",
      "Gradient norms: [18.482258535890384, 3.6907158523230055, 1.254637337012026]\n",
      "41.452295656505626\n",
      "Validation MSE: 46.4803\n",
      "Epoch 705 LR: 0.000934\n",
      "Gradient norms: [13.449798574446776, 3.36475467307724, 1.221261393283715]\n",
      "41.99852381229901\n",
      "Validation MSE: 46.4832\n",
      "Epoch 706 LR: 0.000934\n",
      "Gradient norms: [14.453586674303367, 3.58781470922967, 1.1020303141722942]\n",
      "42.51662873017732\n",
      "Validation MSE: 46.5145\n",
      "Epoch 707 LR: 0.000934\n",
      "Gradient norms: [14.20349679655458, 3.654155959230659, 1.5928767192803168]\n",
      "40.92112901230284\n",
      "Validation MSE: 46.5421\n",
      "Epoch 708 LR: 0.000934\n",
      "Gradient norms: [14.7097452865309, 4.488097281219983, 1.350345274084319]\n",
      "42.45982213066286\n",
      "Validation MSE: 46.5505\n",
      "Epoch 709 LR: 0.000934\n",
      "Gradient norms: [14.037950473977652, 3.5581639842077846, 1.0446712266698035]\n",
      "43.71882033706626\n",
      "Validation MSE: 46.5285\n",
      "Epoch 710 LR: 0.000934\n",
      "Gradient norms: [14.094625721192504, 3.873365716709452, 1.312547437932845]\n",
      "42.834845566488156\n",
      "Validation MSE: 46.5025\n",
      "Epoch 711 LR: 0.000934\n",
      "Gradient norms: [13.51590667465092, 3.9353136848437815, 1.47289691080287]\n",
      "42.16945548330171\n",
      "Validation MSE: 46.4561\n",
      "Epoch 712 LR: 0.000934\n",
      "Gradient norms: [16.32080910176492, 3.619960521271859, 0.913805272479373]\n",
      "43.34954273103877\n",
      "Validation MSE: 46.3867\n",
      "Epoch 713 LR: 0.000933\n",
      "Gradient norms: [13.104816435375055, 3.146824947582855, 1.5452960033308356]\n",
      "41.16279944065015\n",
      "Validation MSE: 46.3185\n",
      "Epoch 714 LR: 0.000933\n",
      "Gradient norms: [13.116503051182065, 3.821200707499306, 1.2860082252742082]\n",
      "43.213269551755076\n",
      "Validation MSE: 46.2354\n",
      "Epoch 715 LR: 0.000933\n",
      "Gradient norms: [14.142011770456593, 3.6833898606388913, 1.3556669486472772]\n",
      "40.936869805246445\n",
      "Validation MSE: 46.1856\n",
      "Epoch 716 LR: 0.000933\n",
      "Gradient norms: [17.148211453017908, 3.6506905771335436, 1.7017299615988843]\n",
      "40.94612994100944\n",
      "Validation MSE: 46.1551\n",
      "Epoch 717 LR: 0.000933\n",
      "Gradient norms: [16.974679297207896, 3.8947324661651552, 1.362528747815661]\n",
      "42.43589562391205\n",
      "Validation MSE: 46.1610\n",
      "Epoch 718 LR: 0.000933\n",
      "Gradient norms: [13.871468194055012, 3.8944686049270354, 1.2275277025525835]\n",
      "43.297323363430905\n",
      "Validation MSE: 46.1439\n",
      "Epoch 719 LR: 0.000933\n",
      "Gradient norms: [12.224091595492197, 3.9953034463245896, 1.8198909367082878]\n",
      "39.35993586037069\n",
      "Validation MSE: 46.1272\n",
      "Epoch 720 LR: 0.000933\n",
      "Gradient norms: [12.949008640568886, 3.593680095908104, 0.8249674452650579]\n",
      "44.028268920895215\n",
      "Validation MSE: 46.1350\n",
      "Epoch 721 LR: 0.000933\n",
      "Gradient norms: [14.372703742803846, 3.7348591964204667, 1.1327772214078988]\n",
      "42.65643670840077\n",
      "Validation MSE: 46.1159\n",
      "Epoch 722 LR: 0.000933\n",
      "Gradient norms: [10.54288866123386, 3.5188310713871167, 1.7461908579120078]\n",
      "39.61218776905139\n",
      "Validation MSE: 46.1296\n",
      "Epoch 723 LR: 0.000933\n",
      "Gradient norms: [14.317822011979546, 3.798265780084076, 1.4346605819711453]\n",
      "41.820225823851764\n",
      "Validation MSE: 46.1583\n",
      "Epoch 724 LR: 0.000932\n",
      "Gradient norms: [13.101374980405678, 3.568060386284869, 1.154802935486398]\n",
      "42.0437043939316\n",
      "Validation MSE: 46.1939\n",
      "Epoch 725 LR: 0.000932\n",
      "Gradient norms: [15.20281786001416, 4.116091270015913, 1.3656410914334973]\n",
      "40.86609033772434\n",
      "Validation MSE: 46.2476\n",
      "Epoch 726 LR: 0.000932\n",
      "Gradient norms: [14.050656479452162, 4.384957857279173, 1.7059904518882798]\n",
      "41.25300410518625\n",
      "Validation MSE: 46.2661\n",
      "Epoch 727 LR: 0.000932\n",
      "Gradient norms: [12.307302279831653, 3.5651557404458045, 1.1447445416616757]\n",
      "42.33379182796137\n",
      "Validation MSE: 46.3054\n",
      "Epoch 728 LR: 0.000932\n",
      "Gradient norms: [17.161310068560674, 3.4149777921658955, 1.2485345886938366]\n",
      "40.927859884228795\n",
      "Validation MSE: 46.3073\n",
      "Epoch 729 LR: 0.000932\n",
      "Gradient norms: [17.523497614714902, 3.9355862556236447, 1.3794434863067264]\n",
      "41.4612591160574\n",
      "Validation MSE: 46.3544\n",
      "Epoch 730 LR: 0.000932\n",
      "Gradient norms: [14.20962840318753, 3.3313576044681077, 1.3575102514667319]\n",
      "41.43438355398847\n",
      "Validation MSE: 46.3885\n",
      "Epoch 731 LR: 0.000932\n",
      "Gradient norms: [14.088217047892556, 3.769016567544175, 1.0989088329494916]\n",
      "42.09656262958369\n",
      "Validation MSE: 46.3819\n",
      "Epoch 732 LR: 0.000932\n",
      "Gradient norms: [14.365955322215898, 3.904146842408164, 1.616316473777338]\n",
      "40.69894971516146\n",
      "Validation MSE: 46.3922\n",
      "Epoch 733 LR: 0.000932\n",
      "Gradient norms: [13.997066469199801, 3.9283530277848393, 1.1608541492181015]\n",
      "43.03703614173767\n",
      "Validation MSE: 46.3961\n",
      "Epoch 734 LR: 0.000932\n",
      "Gradient norms: [12.449660068368187, 2.924241823970443, 1.0701996110135084]\n",
      "41.869048762834566\n",
      "Validation MSE: 46.4035\n",
      "Epoch 735 LR: 0.000932\n",
      "Gradient norms: [15.417216525207447, 3.443121929124292, 1.8469243217356117]\n",
      "40.042002684108226\n",
      "Validation MSE: 46.4052\n",
      "Epoch 736 LR: 0.000931\n",
      "Gradient norms: [13.603948574823349, 3.804809741047841, 1.0621122679230746]\n",
      "42.45916098909557\n",
      "Validation MSE: 46.4268\n",
      "Epoch 737 LR: 0.000931\n",
      "Gradient norms: [15.202810577419084, 4.262151224452494, 1.3519782504967934]\n",
      "42.2289328012574\n",
      "Validation MSE: 46.4263\n",
      "Epoch 738 LR: 0.000931\n",
      "Gradient norms: [12.989644749035376, 3.5631226126068363, 0.8945639635861536]\n",
      "43.087929601293084\n",
      "Validation MSE: 46.4390\n",
      "Epoch 739 LR: 0.000931\n",
      "Gradient norms: [12.680321684307414, 3.339056570965911, 1.0559886597999981]\n",
      "42.41020088658702\n",
      "Validation MSE: 46.4612\n",
      "Epoch 740 LR: 0.000931\n",
      "Gradient norms: [14.49087843215997, 4.257757721889671, 1.1076359950900312]\n",
      "41.41558528851125\n",
      "Validation MSE: 46.4783\n",
      "Epoch 741 LR: 0.000931\n",
      "Gradient norms: [15.251663567600382, 3.4892971911013095, 1.476505468888981]\n",
      "40.74977892194996\n",
      "Validation MSE: 46.4962\n",
      "Epoch 742 LR: 0.000931\n",
      "Gradient norms: [12.77798516848811, 3.666645088290123, 0.9898404152379316]\n",
      "43.18380875498391\n",
      "Validation MSE: 46.5001\n",
      "Epoch 743 LR: 0.000931\n",
      "Gradient norms: [15.760873231119698, 4.147101565793715, 1.7667106890846191]\n",
      "39.87431540645444\n",
      "Validation MSE: 46.5164\n",
      "Epoch 744 LR: 0.000931\n",
      "Gradient norms: [14.652462909304699, 3.815551891945094, 1.4661684805350939]\n",
      "40.502593585037985\n",
      "Validation MSE: 46.5264\n",
      "Epoch 745 LR: 0.000931\n",
      "Gradient norms: [10.495827115269863, 3.8271411313488692, 1.1886728482735873]\n",
      "42.71905348233272\n",
      "Validation MSE: 46.5281\n",
      "Epoch 746 LR: 0.000931\n",
      "Gradient norms: [16.640420932191994, 3.6070283337353173, 1.5277042707401378]\n",
      "40.32580027416096\n",
      "Validation MSE: 46.5731\n",
      "Epoch 747 LR: 0.000930\n",
      "Gradient norms: [12.973484909210319, 3.566126037000225, 1.1308833379415102]\n",
      "41.69739724382044\n",
      "Validation MSE: 46.6344\n",
      "Epoch 748 LR: 0.000930\n",
      "Gradient norms: [20.828229844031334, 3.8631415726470006, 1.0669162715150804]\n",
      "42.17613505199518\n",
      "Validation MSE: 46.6551\n",
      "Epoch 749 LR: 0.000930\n",
      "Gradient norms: [13.240543502083895, 4.107194821176308, 1.2971232012804375]\n",
      "40.99041752846486\n",
      "Validation MSE: 46.7050\n",
      "Epoch 750 LR: 0.000930\n",
      "Gradient norms: [15.633335556228722, 3.773003036519415, 0.8634125642271199]\n",
      "43.227734748823394\n",
      "Validation MSE: 46.7274\n",
      "Epoch 751 LR: 0.000930\n",
      "Gradient norms: [14.827916317428926, 4.1271981068133075, 1.2801098459661064]\n",
      "40.58982151306192\n",
      "Validation MSE: 46.7301\n",
      "Epoch 752 LR: 0.000930\n",
      "Gradient norms: [15.656975539900092, 3.1226239784906906, 0.7803555322896985]\n",
      "42.699707616494145\n",
      "Validation MSE: 46.7590\n",
      "Epoch 753 LR: 0.000930\n",
      "Gradient norms: [13.353956238171019, 3.500733396706993, 1.639511936775388]\n",
      "39.76964160879611\n",
      "Validation MSE: 46.7780\n",
      "Epoch 754 LR: 0.000930\n",
      "Gradient norms: [14.333475280791442, 4.566921571260345, 1.6209530621433947]\n",
      "39.93381344171333\n",
      "Validation MSE: 46.7945\n",
      "Epoch 755 LR: 0.000930\n",
      "Gradient norms: [13.611725758831897, 3.7083034414220206, 0.9828036013979634]\n",
      "42.82286868353049\n",
      "Validation MSE: 46.8481\n",
      "Epoch 756 LR: 0.000930\n",
      "Gradient norms: [16.211795678268295, 4.018192264816566, 0.8619982930909454]\n",
      "43.835017437088325\n",
      "Validation MSE: 46.8763\n",
      "Epoch 757 LR: 0.000930\n",
      "Gradient norms: [15.682287222337438, 3.621533814524865, 0.8975009644193506]\n",
      "42.168074040688175\n",
      "Validation MSE: 46.8917\n",
      "Epoch 758 LR: 0.000930\n",
      "Gradient norms: [18.821983130041936, 3.9395094996585547, 1.3804015584215412]\n",
      "41.16557296500174\n",
      "Validation MSE: 46.9063\n",
      "Epoch 759 LR: 0.000929\n",
      "Gradient norms: [15.793924419269619, 4.061872467880907, 1.2253210611127907]\n",
      "42.27952007373894\n",
      "Validation MSE: 46.8686\n",
      "Epoch 760 LR: 0.000929\n",
      "Gradient norms: [16.752829419604264, 3.993359217836102, 1.0868073269514802]\n",
      "42.59987315977171\n",
      "Validation MSE: 46.8411\n",
      "Epoch 761 LR: 0.000929\n",
      "Gradient norms: [14.787274779398752, 3.417892544694481, 0.8763489585018384]\n",
      "42.45216064514063\n",
      "Validation MSE: 46.8107\n",
      "Epoch 762 LR: 0.000929\n",
      "Gradient norms: [13.81957801574623, 4.007472235025159, 0.9351706543166317]\n",
      "42.582238902781064\n",
      "Validation MSE: 46.7897\n",
      "Epoch 763 LR: 0.000929\n",
      "Gradient norms: [12.401316974993913, 3.8545697006109543, 1.0285439003058299]\n",
      "41.49339357052077\n",
      "Validation MSE: 46.7546\n",
      "Epoch 764 LR: 0.000929\n",
      "Gradient norms: [14.625237982957966, 4.824842935575411, 1.2042354383978666]\n",
      "42.40957857143329\n",
      "Validation MSE: 46.7533\n",
      "Epoch 765 LR: 0.000929\n",
      "Gradient norms: [18.085885375313104, 4.011208398646832, 1.749978724596849]\n",
      "38.76869600103395\n",
      "Validation MSE: 46.7623\n",
      "Epoch 766 LR: 0.000929\n",
      "Gradient norms: [12.424883486153703, 3.58627587674507, 1.1158944950406096]\n",
      "41.01501461380155\n",
      "Validation MSE: 46.7557\n",
      "Epoch 767 LR: 0.000929\n",
      "Gradient norms: [13.467623336801244, 3.855951588660441, 0.8952900730619505]\n",
      "42.59504819928113\n",
      "Validation MSE: 46.7550\n",
      "Epoch 768 LR: 0.000929\n",
      "Gradient norms: [17.305865766360704, 3.7386741238250085, 1.1297523677776855]\n",
      "42.315284946440606\n",
      "Validation MSE: 46.7687\n",
      "Epoch 769 LR: 0.000929\n",
      "Gradient norms: [12.270715055073461, 3.558141045277901, 1.581256638184325]\n",
      "40.967220452593374\n",
      "Validation MSE: 46.7573\n",
      "Epoch 770 LR: 0.000929\n",
      "Gradient norms: [13.11421635464851, 3.5334508161031524, 0.9650639389321967]\n",
      "41.68722064543404\n",
      "Validation MSE: 46.7463\n",
      "Epoch 771 LR: 0.000928\n",
      "Gradient norms: [12.71304928436306, 4.321195423354002, 0.9931226073969962]\n",
      "41.87840509959387\n",
      "Validation MSE: 46.7125\n",
      "Epoch 772 LR: 0.000928\n",
      "Gradient norms: [16.435455441828296, 3.8322699696405533, 1.6905516511837868]\n",
      "39.14411283787732\n",
      "Validation MSE: 46.6467\n",
      "Epoch 773 LR: 0.000928\n",
      "Gradient norms: [15.278917012954924, 4.1047640415718485, 1.2174418823032884]\n",
      "41.73140006646624\n",
      "Validation MSE: 46.5951\n",
      "Epoch 774 LR: 0.000928\n",
      "Gradient norms: [13.663453235488989, 3.989132459600103, 1.4070419641605996]\n",
      "39.812562191945034\n",
      "Validation MSE: 46.5440\n",
      "Epoch 775 LR: 0.000928\n",
      "Gradient norms: [12.553510636758425, 3.86396184314093, 0.8150268049307063]\n",
      "44.25283209979869\n",
      "Validation MSE: 46.5082\n",
      "Epoch 776 LR: 0.000928\n",
      "Gradient norms: [11.530148960869992, 2.9504748398449436, 1.0581590147187603]\n",
      "42.68215283305406\n",
      "Validation MSE: 46.4691\n",
      "Epoch 777 LR: 0.000928\n",
      "Gradient norms: [13.870005356858812, 4.1696434852046105, 1.4972958332368858]\n",
      "40.91101260026947\n",
      "Validation MSE: 46.4299\n",
      "Epoch 778 LR: 0.000928\n",
      "Gradient norms: [11.901218560927184, 3.9228100857968764, 1.6499841413488057]\n",
      "41.22494380884462\n",
      "Validation MSE: 46.4173\n",
      "Epoch 779 LR: 0.000928\n",
      "Gradient norms: [16.735898408495363, 3.2343965881267858, 1.2301923258369407]\n",
      "42.249236552817436\n",
      "Validation MSE: 46.3628\n",
      "Epoch 780 LR: 0.000928\n",
      "Gradient norms: [12.760230027624175, 4.465000247842568, 1.0586706324614996]\n",
      "41.43705987743674\n",
      "Validation MSE: 46.3350\n",
      "Epoch 781 LR: 0.000928\n",
      "Gradient norms: [16.929431054819073, 3.690509695709718, 1.0090850476938686]\n",
      "42.702746488580004\n",
      "Validation MSE: 46.3399\n",
      "Epoch 782 LR: 0.000927\n",
      "Gradient norms: [12.516657584099205, 3.8025521419998936, 0.9600353319318051]\n",
      "43.966297754141095\n",
      "Validation MSE: 46.3537\n",
      "Epoch 783 LR: 0.000927\n",
      "Gradient norms: [16.282972310511965, 4.07931596755057, 1.2703033597014317]\n",
      "42.13238346616814\n",
      "Validation MSE: 46.3993\n",
      "Epoch 784 LR: 0.000927\n",
      "Gradient norms: [13.427663328154207, 3.7979207917450224, 1.4028845881005667]\n",
      "40.195565912608636\n",
      "Validation MSE: 46.4317\n",
      "Epoch 785 LR: 0.000927\n",
      "Gradient norms: [14.109436830291374, 3.9979230112571553, 1.7544841222044998]\n",
      "38.329007001754846\n",
      "Validation MSE: 46.4976\n",
      "Epoch 786 LR: 0.000927\n",
      "Gradient norms: [16.12544188146477, 4.687154700763378, 1.1729813964049918]\n",
      "42.53903121601325\n",
      "Validation MSE: 46.5477\n",
      "Epoch 787 LR: 0.000927\n",
      "Gradient norms: [15.930014377128723, 4.086551758416483, 1.490558625651894]\n",
      "40.248006614417314\n",
      "Validation MSE: 46.5966\n",
      "Epoch 788 LR: 0.000927\n",
      "Gradient norms: [14.45171627800547, 4.086198520771797, 1.2482321962362943]\n",
      "40.86232902398346\n",
      "Validation MSE: 46.6668\n",
      "Epoch 789 LR: 0.000927\n",
      "Gradient norms: [15.985432194213136, 3.5433207119039016, 0.8292021647938488]\n",
      "41.2093077396433\n",
      "Validation MSE: 46.7025\n",
      "Epoch 790 LR: 0.000927\n",
      "Gradient norms: [16.642808616856808, 4.383751742222012, 1.1489939867948311]\n",
      "42.20290848529883\n",
      "Validation MSE: 46.7121\n",
      "Epoch 791 LR: 0.000927\n",
      "Gradient norms: [14.08829487505235, 3.9619117916933946, 1.5996533771866588]\n",
      "40.182782469829654\n",
      "Validation MSE: 46.7505\n",
      "Epoch 792 LR: 0.000927\n",
      "Gradient norms: [16.151619721665472, 3.848227227013195, 0.8813510518941776]\n",
      "42.32166207217031\n",
      "Validation MSE: 46.7742\n",
      "Epoch 793 LR: 0.000927\n",
      "Gradient norms: [13.525347462965701, 4.086083098941801, 1.5555213319195436]\n",
      "39.63984111740282\n",
      "Validation MSE: 46.8230\n",
      "Epoch 794 LR: 0.000926\n",
      "Gradient norms: [12.773002874895003, 3.4602338630742104, 1.5709796205767625]\n",
      "39.3382324210971\n",
      "Validation MSE: 46.8720\n",
      "Epoch 795 LR: 0.000926\n",
      "Gradient norms: [12.530644386194966, 3.8193444197791995, 1.5607536429380193]\n",
      "39.67826201963452\n",
      "Validation MSE: 46.9351\n",
      "Epoch 796 LR: 0.000926\n",
      "Gradient norms: [15.645738406216534, 3.928975152736006, 1.1764530216901448]\n",
      "40.476461586088554\n",
      "Validation MSE: 47.0281\n",
      "Epoch 797 LR: 0.000926\n",
      "Gradient norms: [18.10783960695224, 3.8494324582993333, 1.4930354914981485]\n",
      "41.09658737421605\n",
      "Validation MSE: 47.0736\n",
      "Epoch 798 LR: 0.000926\n",
      "Gradient norms: [19.73371394023988, 4.063337356243938, 1.089816819496825]\n",
      "42.564464336228426\n",
      "Validation MSE: 47.0464\n",
      "Epoch 799 LR: 0.000926\n",
      "Gradient norms: [12.237244795627527, 3.2643204991536128, 1.0349506007430698]\n",
      "41.142959837837815\n",
      "Validation MSE: 47.0495\n",
      "Epoch 800 LR: 0.000926\n",
      "Gradient norms: [12.496595089947512, 4.025859304732026, 1.590269593207069]\n",
      "38.984628117105515\n",
      "Validation MSE: 47.0604\n",
      "Epoch 801 LR: 0.000926\n",
      "Gradient norms: [16.05862853355539, 4.2386919647115, 1.2928425457146744]\n",
      "40.92448850968681\n",
      "Validation MSE: 47.0641\n",
      "Epoch 802 LR: 0.000926\n",
      "Gradient norms: [16.20884712756905, 3.983177796222146, 1.3585458579978562]\n",
      "40.862243874909446\n",
      "Validation MSE: 47.0365\n",
      "Epoch 803 LR: 0.000926\n",
      "Gradient norms: [16.391085040582908, 4.092955308270787, 1.0930681834451408]\n",
      "42.210590272352604\n",
      "Validation MSE: 47.0028\n",
      "Epoch 804 LR: 0.000926\n",
      "Gradient norms: [12.094643798293973, 4.5362268964351085, 1.082717219123244]\n",
      "42.77692920165537\n",
      "Validation MSE: 46.9594\n",
      "Epoch 805 LR: 0.000925\n",
      "Gradient norms: [13.999965552390645, 4.0927148923257, 0.9995672335022038]\n",
      "42.06013165083295\n",
      "Validation MSE: 46.9245\n",
      "Epoch 806 LR: 0.000925\n",
      "Gradient norms: [13.21474997920843, 3.693138605707582, 0.8047954883452866]\n",
      "41.3131829563363\n",
      "Validation MSE: 46.8828\n",
      "Epoch 807 LR: 0.000925\n",
      "Gradient norms: [18.83758079675562, 4.082997345147524, 1.2890406686553149]\n",
      "40.48730139230208\n",
      "Validation MSE: 46.8481\n",
      "Epoch 808 LR: 0.000925\n",
      "Gradient norms: [15.689334366204768, 4.451653027291709, 0.7057353298316527]\n",
      "43.107402327857535\n",
      "Validation MSE: 46.8108\n",
      "Epoch 809 LR: 0.000925\n",
      "Gradient norms: [12.684202460970965, 4.343964927012634, 1.182239681417039]\n",
      "40.27564723512426\n",
      "Validation MSE: 46.7963\n",
      "Epoch 810 LR: 0.000925\n",
      "Gradient norms: [17.17500894891155, 3.601569827989587, 0.9970955241366031]\n",
      "40.68151703060535\n",
      "Validation MSE: 46.7847\n",
      "Epoch 811 LR: 0.000925\n",
      "Gradient norms: [13.500326475040175, 4.035742600674987, 0.5674228149386612]\n",
      "43.27322178466792\n",
      "Validation MSE: 46.7407\n",
      "Epoch 812 LR: 0.000925\n",
      "Gradient norms: [20.053148079804846, 4.562902575955577, 1.2488202745305024]\n",
      "40.74350615982065\n",
      "Validation MSE: 46.7212\n",
      "Epoch 813 LR: 0.000925\n",
      "Gradient norms: [13.127882572936471, 3.411309268907785, 0.8895091961807329]\n",
      "41.19745712409805\n",
      "Validation MSE: 46.6796\n",
      "Epoch 814 LR: 0.000925\n",
      "Gradient norms: [10.807558923966235, 3.837167130705668, 1.0448288717220942]\n",
      "41.67089907897593\n",
      "Validation MSE: 46.6704\n",
      "Epoch 815 LR: 0.000925\n",
      "Gradient norms: [15.360751953149933, 3.657943060795982, 1.1432100340684694]\n",
      "40.587086168889776\n",
      "Validation MSE: 46.6641\n",
      "Epoch 816 LR: 0.000925\n",
      "Gradient norms: [13.6205391142322, 3.720517995987398, 1.3458415192228708]\n",
      "40.087769199319055\n",
      "Validation MSE: 46.6689\n",
      "Epoch 817 LR: 0.000924\n",
      "Gradient norms: [18.76394907416156, 4.025936049212809, 1.01550516712244]\n",
      "41.91837817048061\n",
      "Validation MSE: 46.7107\n",
      "Epoch 818 LR: 0.000924\n",
      "Gradient norms: [14.539074628070095, 3.7926044502458893, 1.3022650222327072]\n",
      "41.48504885426129\n",
      "Validation MSE: 46.7399\n",
      "Epoch 819 LR: 0.000924\n",
      "Gradient norms: [13.747270187171628, 4.27557299638163, 1.2942345859876399]\n",
      "40.24601929975964\n",
      "Validation MSE: 46.7963\n",
      "Epoch 820 LR: 0.000924\n",
      "Gradient norms: [12.336792369436083, 3.621341422742243, 1.1518012323067472]\n",
      "40.536690620301925\n",
      "Validation MSE: 46.8206\n",
      "Epoch 821 LR: 0.000924\n",
      "Gradient norms: [17.04963620995922, 3.83124659665389, 1.264207893333998]\n",
      "41.71432181454325\n",
      "Validation MSE: 46.8428\n",
      "Epoch 822 LR: 0.000924\n",
      "Gradient norms: [16.43401591508018, 3.887389842619428, 1.000047669525068]\n",
      "41.61056438926269\n",
      "Validation MSE: 46.8886\n",
      "Epoch 823 LR: 0.000924\n",
      "Gradient norms: [15.676123998350203, 4.454227957766237, 1.2125614610782554]\n",
      "40.87049920674786\n",
      "Validation MSE: 46.9512\n",
      "Epoch 824 LR: 0.000924\n",
      "Gradient norms: [18.436781779262752, 3.331963343335937, 0.9950850921394431]\n",
      "41.0757298970225\n",
      "Validation MSE: 47.0147\n",
      "Epoch 825 LR: 0.000924\n",
      "Gradient norms: [16.17144672161695, 4.061240480493512, 1.4964679650229933]\n",
      "39.719320484575945\n",
      "Validation MSE: 47.0365\n",
      "Epoch 826 LR: 0.000924\n",
      "Gradient norms: [12.883513049746394, 3.696773997376407, 0.9394644741719012]\n",
      "42.92629038493431\n",
      "Validation MSE: 47.0334\n",
      "Epoch 827 LR: 0.000924\n",
      "Gradient norms: [18.72301716017105, 3.3070167495100224, 0.9948262481094184]\n",
      "40.96092183528102\n",
      "Validation MSE: 47.0478\n",
      "Epoch 828 LR: 0.000924\n",
      "Gradient norms: [16.835378251859623, 3.765492722207496, 0.8480072582264973]\n",
      "42.5747575991141\n",
      "Validation MSE: 47.0607\n",
      "Epoch 829 LR: 0.000923\n",
      "Gradient norms: [15.095623290168401, 3.9600185847207943, 1.0930290325817376]\n",
      "42.66527368373556\n",
      "Validation MSE: 47.0317\n",
      "Epoch 830 LR: 0.000923\n",
      "Gradient norms: [15.592899238618108, 3.283103656162374, 1.4484562698653078]\n",
      "40.74053027660625\n",
      "Validation MSE: 46.9924\n",
      "Epoch 831 LR: 0.000923\n",
      "Gradient norms: [18.40119619474639, 3.6061904953742534, 0.7488483467196552]\n",
      "42.24544594651026\n",
      "Validation MSE: 46.9592\n",
      "Epoch 832 LR: 0.000923\n",
      "Gradient norms: [17.212048917122022, 4.099957106044813, 0.7586475769348164]\n",
      "42.50792381607587\n",
      "Validation MSE: 46.9120\n",
      "Epoch 833 LR: 0.000923\n",
      "Gradient norms: [15.440678579760638, 3.8576639393503327, 1.3593228682881746]\n",
      "39.42022955907787\n",
      "Validation MSE: 46.8447\n",
      "Epoch 834 LR: 0.000923\n",
      "Gradient norms: [17.942623011974423, 4.592432385132525, 1.2084499953037577]\n",
      "43.147473327694584\n",
      "Validation MSE: 46.7511\n",
      "Epoch 835 LR: 0.000923\n",
      "Gradient norms: [14.132069766380829, 3.7310952260997614, 0.9907943769976142]\n",
      "41.747582100892046\n",
      "Validation MSE: 46.6890\n",
      "Epoch 836 LR: 0.000923\n",
      "Gradient norms: [13.889455451735365, 4.0474065000551045, 0.8155050608594969]\n",
      "43.139593160708685\n",
      "Validation MSE: 46.6466\n",
      "Epoch 837 LR: 0.000923\n",
      "Gradient norms: [15.243368136554789, 3.691989343667045, 1.1414489622799648]\n",
      "41.90273516433778\n",
      "Validation MSE: 46.6173\n",
      "Epoch 838 LR: 0.000923\n",
      "Gradient norms: [15.991428488261185, 4.105884765519553, 1.0202714065423704]\n",
      "41.46076333552246\n",
      "Validation MSE: 46.5860\n",
      "Epoch 839 LR: 0.000923\n",
      "Gradient norms: [18.04629408183159, 4.012132502474568, 1.34453495834425]\n",
      "40.52084945443962\n",
      "Validation MSE: 46.5528\n",
      "Epoch 840 LR: 0.000923\n",
      "Gradient norms: [17.958101598807776, 4.347784352573323, 1.231488317604868]\n",
      "41.144096901413455\n",
      "Validation MSE: 46.5405\n",
      "Epoch 841 LR: 0.000922\n",
      "Gradient norms: [14.274753798466731, 4.211103470884024, 0.818643121352837]\n",
      "41.958307506146866\n",
      "Validation MSE: 46.5386\n",
      "Epoch 842 LR: 0.000922\n",
      "Gradient norms: [14.86251381136506, 4.0321657625658744, 0.7162535131024941]\n",
      "41.80899692715478\n",
      "Validation MSE: 46.5349\n",
      "Epoch 843 LR: 0.000922\n",
      "Gradient norms: [16.71635169669668, 4.314854024550227, 0.9944908111191636]\n",
      "40.99276125722349\n",
      "Validation MSE: 46.5202\n",
      "Epoch 844 LR: 0.000922\n",
      "Gradient norms: [14.599036110623338, 3.806120820774685, 0.9703023934179834]\n",
      "41.58208922281971\n",
      "Validation MSE: 46.5174\n",
      "Epoch 845 LR: 0.000922\n",
      "Gradient norms: [14.963362956918834, 4.02222693339042, 0.9594709857810059]\n",
      "42.75245402038493\n",
      "Validation MSE: 46.5069\n",
      "Epoch 846 LR: 0.000922\n",
      "Gradient norms: [10.798242385287741, 3.4500560136121545, 0.94291683875637]\n",
      "41.89665204456636\n",
      "Validation MSE: 46.5097\n",
      "Epoch 847 LR: 0.000922\n",
      "Gradient norms: [13.904737753279289, 4.080407607129008, 1.195380347267876]\n",
      "41.05667223292917\n",
      "Validation MSE: 46.5026\n",
      "Epoch 848 LR: 0.000922\n",
      "Gradient norms: [15.465599467000253, 4.649804898237985, 1.0361908612990283]\n",
      "42.017013013418286\n",
      "Validation MSE: 46.4747\n",
      "Epoch 849 LR: 0.000922\n",
      "Gradient norms: [14.538511917522271, 4.100182906721818, 1.382839929007942]\n",
      "39.074477697364664\n",
      "Validation MSE: 46.4521\n",
      "Epoch 850 LR: 0.000922\n",
      "Gradient norms: [20.9548889943898, 4.443893751120958, 0.9741485803466111]\n",
      "40.943606632945816\n",
      "Validation MSE: 46.4169\n",
      "Epoch 851 LR: 0.000922\n",
      "Gradient norms: [14.674240847992028, 4.23264049209814, 0.8945729882346091]\n",
      "42.54588662091855\n",
      "Validation MSE: 46.4143\n",
      "Epoch 852 LR: 0.000921\n",
      "Gradient norms: [15.971898172955829, 3.968103765386737, 1.1559571609272419]\n",
      "40.59974425535059\n",
      "Validation MSE: 46.3701\n",
      "Epoch 853 LR: 0.000921\n",
      "Gradient norms: [16.457458846004556, 4.465460376552754, 0.9012983065836324]\n",
      "43.0800676739267\n",
      "Validation MSE: 46.3392\n",
      "Epoch 854 LR: 0.000921\n",
      "Gradient norms: [17.515169818377665, 4.050945510204747, 1.1401051485959448]\n",
      "41.71057964333301\n",
      "Validation MSE: 46.3150\n",
      "Epoch 855 LR: 0.000921\n",
      "Gradient norms: [18.417578596440116, 4.712878691627613, 1.2424393342919833]\n",
      "41.22030871934365\n",
      "Validation MSE: 46.2818\n",
      "Epoch 856 LR: 0.000921\n",
      "Gradient norms: [15.49498061792346, 4.183373280999299, 1.0259287282327239]\n",
      "42.073449323234804\n",
      "Validation MSE: 46.2477\n",
      "Epoch 857 LR: 0.000921\n",
      "Gradient norms: [17.79110290698197, 3.760261405138136, 1.190800887598101]\n",
      "39.50779992355082\n",
      "Validation MSE: 46.1937\n",
      "Epoch 858 LR: 0.000921\n",
      "Gradient norms: [18.105803954345195, 4.079203966440337, 0.8675585685110075]\n",
      "41.599956282369085\n",
      "Validation MSE: 46.1396\n",
      "Epoch 859 LR: 0.000921\n",
      "Gradient norms: [18.400604009966354, 3.6309933455805643, 1.027877129446783]\n",
      "41.24366012833607\n",
      "Validation MSE: 46.0820\n",
      "Epoch 860 LR: 0.000921\n",
      "Gradient norms: [15.916409970736915, 4.11461586711252, 0.8331030102525556]\n",
      "42.25754280239004\n",
      "Validation MSE: 46.0715\n",
      "Epoch 861 LR: 0.000921\n",
      "Gradient norms: [12.483022952400002, 3.8060873727670828, 1.0328279298634984]\n",
      "41.4245859069092\n",
      "Validation MSE: 46.0468\n",
      "Epoch 862 LR: 0.000921\n",
      "Gradient norms: [15.820985231204755, 4.034788882665288, 1.417671851355478]\n",
      "41.74562612628525\n",
      "Validation MSE: 46.0218\n",
      "Epoch 863 LR: 0.000921\n",
      "Gradient norms: [16.23328881984542, 3.7928118513283016, 1.3314694794235291]\n",
      "40.64545058235142\n",
      "Validation MSE: 46.0290\n",
      "Epoch 864 LR: 0.000920\n",
      "Gradient norms: [17.073879473637724, 3.9344511172457795, 0.8849136637111715]\n",
      "41.27362021741196\n",
      "Validation MSE: 46.0577\n",
      "Epoch 865 LR: 0.000920\n",
      "Gradient norms: [16.42012484320787, 3.524798126451961, 1.4323176591463602]\n",
      "40.16982634062552\n",
      "Validation MSE: 46.0991\n",
      "Epoch 866 LR: 0.000920\n",
      "Gradient norms: [15.877412142432071, 3.4043957813372536, 0.8604458592514747]\n",
      "41.96292719341656\n",
      "Validation MSE: 46.1681\n",
      "Epoch 867 LR: 0.000920\n",
      "Gradient norms: [16.160902117742292, 3.7811170187817384, 1.5170368745356744]\n",
      "39.15056166894132\n",
      "Validation MSE: 46.2210\n",
      "Epoch 868 LR: 0.000920\n",
      "Gradient norms: [18.193824591213485, 4.743385108028698, 0.8425323923143767]\n",
      "43.53002453997882\n",
      "Validation MSE: 46.2932\n",
      "Epoch 869 LR: 0.000920\n",
      "Gradient norms: [17.580161007390046, 4.367040584128951, 1.5058753674492664]\n",
      "39.34165793880199\n",
      "Validation MSE: 46.3416\n",
      "Epoch 870 LR: 0.000920\n",
      "Gradient norms: [16.578142085933877, 3.8802136773176805, 0.979373615292726]\n",
      "41.55332900124104\n",
      "Validation MSE: 46.3966\n",
      "Epoch 871 LR: 0.000920\n",
      "Gradient norms: [17.88992252923499, 3.7211956576532788, 1.3122086816209197]\n",
      "39.15444949263651\n",
      "Validation MSE: 46.4690\n",
      "Epoch 872 LR: 0.000920\n",
      "Gradient norms: [14.357268604366872, 3.9478480630933253, 1.2615784616224037]\n",
      "41.31059827053753\n",
      "Validation MSE: 46.5076\n",
      "Epoch 873 LR: 0.000920\n",
      "Gradient norms: [19.242181154837166, 4.1970590749255425, 1.0772497261584797]\n",
      "40.2393343599761\n",
      "Validation MSE: 46.5729\n",
      "Epoch 874 LR: 0.000920\n",
      "Gradient norms: [15.430880779321908, 4.272502679091969, 1.55079544763699]\n",
      "40.26267607266742\n",
      "Validation MSE: 46.5808\n",
      "Epoch 875 LR: 0.000920\n",
      "Gradient norms: [13.895483022712597, 3.6264331655150914, 1.3747700589721168]\n",
      "40.48207070877981\n",
      "Validation MSE: 46.6303\n",
      "Epoch 876 LR: 0.000919\n",
      "Gradient norms: [16.09257189636827, 3.804989401031332, 1.1136522756359466]\n",
      "41.0596991958652\n",
      "Validation MSE: 46.6482\n",
      "Epoch 877 LR: 0.000919\n",
      "Gradient norms: [13.759512415143663, 4.163146768706896, 0.9121550894016395]\n",
      "42.092518180512414\n",
      "Validation MSE: 46.6889\n",
      "Epoch 878 LR: 0.000919\n",
      "Gradient norms: [15.976931597177238, 3.360927341597945, 1.1677901187361737]\n",
      "39.14116694036999\n",
      "Validation MSE: 46.7268\n",
      "Epoch 879 LR: 0.000919\n",
      "Gradient norms: [15.108225722439057, 4.140896660407619, 0.9683866817319206]\n",
      "40.41821343981424\n",
      "Validation MSE: 46.7618\n",
      "Epoch 880 LR: 0.000919\n",
      "Gradient norms: [15.013199681367528, 4.19325123494748, 1.3044274586183935]\n",
      "40.83760494039449\n",
      "Validation MSE: 46.7979\n",
      "Epoch 881 LR: 0.000919\n",
      "Gradient norms: [14.630626395644448, 3.63865498621183, 0.8881212120677551]\n",
      "43.117665960096566\n",
      "Validation MSE: 46.7939\n",
      "Epoch 882 LR: 0.000919\n",
      "Gradient norms: [17.329369051146774, 4.890258810937229, 1.1626287738140109]\n",
      "39.75855592782677\n",
      "Validation MSE: 46.8157\n",
      "Epoch 883 LR: 0.000919\n",
      "Gradient norms: [14.84632847744026, 4.412428942375215, 0.9750259463028732]\n",
      "43.023801875112426\n",
      "Validation MSE: 46.8174\n",
      "Epoch 884 LR: 0.000919\n",
      "Gradient norms: [16.04921945444753, 4.124086130996494, 0.9287031498524515]\n",
      "41.58462622482581\n",
      "Validation MSE: 46.8549\n",
      "Epoch 885 LR: 0.000919\n",
      "Gradient norms: [14.817405857018276, 3.4772642403599514, 0.7936140297395478]\n",
      "41.759065832170464\n",
      "Validation MSE: 46.8729\n",
      "Epoch 886 LR: 0.000919\n",
      "Gradient norms: [16.23784342224981, 3.2873254322098515, 0.8663867785682614]\n",
      "42.70221884770846\n",
      "Validation MSE: 46.8323\n",
      "Epoch 887 LR: 0.000919\n",
      "Gradient norms: [19.626941870357435, 4.442143421965784, 0.964823762943928]\n",
      "40.362208423054426\n",
      "Validation MSE: 46.8321\n",
      "Epoch 888 LR: 0.000918\n",
      "Gradient norms: [16.008459302654625, 3.886507502105549, 1.1131832104963482]\n",
      "41.499180398347114\n",
      "Validation MSE: 46.8097\n",
      "Epoch 889 LR: 0.000918\n",
      "Gradient norms: [14.40702619922599, 3.7283852747500283, 1.3989471168781589]\n",
      "41.543705050293276\n",
      "Validation MSE: 46.8162\n",
      "Epoch 890 LR: 0.000918\n",
      "Gradient norms: [16.311141437339852, 3.9119805328663135, 0.7012851225458582]\n",
      "42.43336343891226\n",
      "Validation MSE: 46.7993\n",
      "Epoch 891 LR: 0.000918\n",
      "Gradient norms: [18.587652062620766, 4.724582719818302, 0.7184381409988128]\n",
      "43.5511863893504\n",
      "Validation MSE: 46.7820\n",
      "Epoch 892 LR: 0.000918\n",
      "Gradient norms: [15.757136199516907, 4.768542766556487, 1.882479944055474]\n",
      "39.03622924092079\n",
      "Validation MSE: 46.7416\n",
      "Epoch 893 LR: 0.000918\n",
      "Gradient norms: [14.42115062370642, 4.282170443353165, 0.8956935684374153]\n",
      "42.55475912836417\n",
      "Validation MSE: 46.7315\n",
      "Epoch 894 LR: 0.000918\n",
      "Gradient norms: [13.875373592872062, 4.393280323928308, 1.1193821094623242]\n",
      "39.90833902274973\n",
      "Validation MSE: 46.7307\n",
      "Epoch 895 LR: 0.000918\n",
      "Gradient norms: [18.692367850136918, 5.370696650554113, 0.9353098530425471]\n",
      "41.422983578109736\n",
      "Validation MSE: 46.6857\n",
      "Epoch 896 LR: 0.000918\n",
      "Gradient norms: [16.157112593151382, 3.793783120750818, 0.8983144002206829]\n",
      "40.90915857406995\n",
      "Validation MSE: 46.6678\n",
      "Epoch 897 LR: 0.000918\n",
      "Gradient norms: [15.37836892436392, 3.594791631791681, 1.0556061913519845]\n",
      "40.76143109730741\n",
      "Validation MSE: 46.6369\n",
      "Epoch 898 LR: 0.000918\n",
      "Gradient norms: [14.724280344303336, 4.057430017476055, 0.8199128006838031]\n",
      "42.302617109461494\n",
      "Validation MSE: 46.6020\n",
      "Epoch 899 LR: 0.000918\n",
      "Gradient norms: [13.973432948305637, 4.117367708058224, 1.1012838767526638]\n",
      "41.852164212603036\n",
      "Validation MSE: 46.5606\n",
      "Epoch 900 LR: 0.000917\n",
      "Gradient norms: [13.249678411410994, 3.7356495122259643, 0.816169283818713]\n",
      "42.45397335199903\n",
      "Validation MSE: 46.5359\n",
      "Epoch 901 LR: 0.000917\n",
      "Gradient norms: [14.616524331826056, 4.137858077358127, 1.2160488459144412]\n",
      "40.138178862490804\n",
      "Validation MSE: 46.5184\n",
      "Epoch 902 LR: 0.000917\n",
      "Gradient norms: [16.32603384790704, 4.59091064493426, 1.557714183473501]\n",
      "39.29517545256294\n",
      "Validation MSE: 46.5175\n",
      "Epoch 903 LR: 0.000917\n",
      "Gradient norms: [14.854692655326216, 4.4643546431782495, 1.0215947145949447]\n",
      "42.902897198671525\n",
      "Validation MSE: 46.5496\n",
      "Epoch 904 LR: 0.000917\n",
      "Gradient norms: [15.79088763138365, 4.338702976817582, 1.059335005739615]\n",
      "40.79141965119109\n",
      "Validation MSE: 46.6160\n",
      "Epoch 905 LR: 0.000917\n",
      "Gradient norms: [17.17277608652696, 4.339912879451756, 1.887834281248282]\n",
      "39.2254192161988\n",
      "Validation MSE: 46.6557\n",
      "Epoch 906 LR: 0.000917\n",
      "Gradient norms: [20.14949629686391, 4.146193486258236, 0.9513656207675051]\n",
      "42.70063877049538\n",
      "Validation MSE: 46.6749\n",
      "Epoch 907 LR: 0.000917\n",
      "Gradient norms: [15.010722041752189, 4.55896691772304, 0.8071044445402893]\n",
      "41.10518684450189\n",
      "Validation MSE: 46.6795\n",
      "Epoch 908 LR: 0.000917\n",
      "Gradient norms: [15.688905233698666, 4.479532670093141, 0.7157995960024348]\n",
      "42.28278307448542\n",
      "Validation MSE: 46.6774\n",
      "Epoch 909 LR: 0.000917\n",
      "Gradient norms: [18.99854001191892, 3.893944741872065, 1.1877244972337944]\n",
      "40.69785151587608\n",
      "Validation MSE: 46.6705\n",
      "Epoch 910 LR: 0.000917\n",
      "Gradient norms: [14.343789520804823, 3.8328438270944596, 1.0571744048890728]\n",
      "40.78580434890971\n",
      "Validation MSE: 46.6670\n",
      "Epoch 911 LR: 0.000917\n",
      "Gradient norms: [18.115166233948976, 4.052976215921276, 1.0689821135186224]\n",
      "42.105024421302275\n",
      "Validation MSE: 46.6576\n",
      "Epoch 912 LR: 0.000916\n",
      "Gradient norms: [16.10553804567676, 4.15220255576872, 1.080744465791693]\n",
      "40.89149116088859\n",
      "Validation MSE: 46.6420\n",
      "Epoch 913 LR: 0.000916\n",
      "Gradient norms: [17.40706105794094, 4.021047736615037, 1.3629241156454053]\n",
      "39.27440758400056\n",
      "Validation MSE: 46.6365\n",
      "Epoch 914 LR: 0.000916\n",
      "Gradient norms: [14.838979725552063, 5.265164881975714, 0.9837929742554511]\n",
      "42.03291949641311\n",
      "Validation MSE: 46.6395\n",
      "Epoch 915 LR: 0.000916\n",
      "Gradient norms: [18.362979702640324, 4.3086415263858475, 0.8848778448096655]\n",
      "41.05360157584331\n",
      "Validation MSE: 46.6675\n",
      "Epoch 916 LR: 0.000916\n",
      "Gradient norms: [15.642675970821989, 4.054366725829384, 0.9622866105305046]\n",
      "41.063858946233346\n",
      "Validation MSE: 46.7007\n",
      "Epoch 917 LR: 0.000916\n",
      "Gradient norms: [18.05541216565634, 4.299044845452759, 1.0312128321254692]\n",
      "40.36145371558261\n",
      "Validation MSE: 46.7309\n",
      "Epoch 918 LR: 0.000916\n",
      "Gradient norms: [16.803709311990662, 4.0337562825425355, 0.8154349222025836]\n",
      "42.054321988023794\n",
      "Validation MSE: 46.7309\n",
      "Epoch 919 LR: 0.000916\n",
      "Gradient norms: [15.596670343895365, 4.322192717461375, 0.8277494432971305]\n",
      "41.32936368754448\n",
      "Validation MSE: 46.7538\n",
      "Epoch 920 LR: 0.000916\n",
      "Gradient norms: [17.40777422466243, 4.424545127032193, 1.075699470073166]\n",
      "41.949766748564535\n",
      "Validation MSE: 46.7890\n",
      "Epoch 921 LR: 0.000916\n",
      "Gradient norms: [15.883442649868593, 4.413772902228483, 1.2954988187213883]\n",
      "39.93758352645544\n",
      "Validation MSE: 46.8642\n",
      "Epoch 922 LR: 0.000916\n",
      "Gradient norms: [15.694131394849926, 4.252901357903572, 1.6853825840304095]\n",
      "37.87898969233201\n",
      "Validation MSE: 46.9434\n",
      "Epoch 923 LR: 0.000915\n",
      "Gradient norms: [12.70994387061139, 3.712341862128265, 1.5968164354262497]\n",
      "39.57158025658365\n",
      "Validation MSE: 46.9898\n",
      "Epoch 924 LR: 0.000915\n",
      "Gradient norms: [11.988988771568236, 3.6436356522635704, 1.3219206924494469]\n",
      "39.3626878547908\n",
      "Validation MSE: 47.0312\n",
      "Epoch 925 LR: 0.000915\n",
      "Gradient norms: [15.829174786161445, 3.595642866075331, 1.0867188909504526]\n",
      "39.294787527567784\n",
      "Validation MSE: 47.0322\n",
      "Epoch 926 LR: 0.000915\n",
      "Gradient norms: [17.81143847986899, 4.612664763044551, 1.021302092668197]\n",
      "41.24644034246531\n",
      "Validation MSE: 47.0789\n",
      "Epoch 927 LR: 0.000915\n",
      "Gradient norms: [16.99024642138202, 4.560228743176677, 1.7501544850098596]\n",
      "40.26046440517627\n",
      "Validation MSE: 47.1231\n",
      "Epoch 928 LR: 0.000915\n",
      "Gradient norms: [16.99384683673391, 5.04500670754132, 0.6030664867390447]\n",
      "41.926803652820745\n",
      "Validation MSE: 47.1311\n",
      "Epoch 929 LR: 0.000915\n",
      "Gradient norms: [17.420945790280523, 4.441050141974716, 1.2562603092057112]\n",
      "39.25090549770868\n",
      "Validation MSE: 47.1336\n",
      "Epoch 930 LR: 0.000915\n",
      "Gradient norms: [17.50804531498938, 5.062375796546296, 1.275454527720012]\n",
      "40.081788604021696\n",
      "Validation MSE: 47.1457\n",
      "Epoch 931 LR: 0.000915\n",
      "Gradient norms: [16.81286742168621, 4.365260982770304, 1.0967757692982785]\n",
      "41.204696897391585\n",
      "Validation MSE: 47.1139\n",
      "Epoch 932 LR: 0.000915\n",
      "Gradient norms: [16.612531819757958, 3.9540515551809974, 0.7908792584575698]\n",
      "41.23502359259708\n",
      "Validation MSE: 47.0956\n",
      "Epoch 933 LR: 0.000915\n",
      "Gradient norms: [17.311691329007562, 4.717090303646641, 0.9911391408407969]\n",
      "41.0297280948698\n",
      "Validation MSE: 47.0672\n",
      "Epoch 934 LR: 0.000915\n",
      "Gradient norms: [21.053953406295115, 4.426522321984274, 1.4054194353692224]\n",
      "38.96914049751354\n",
      "Validation MSE: 47.0189\n",
      "Epoch 935 LR: 0.000914\n",
      "Gradient norms: [16.045450116404762, 4.907685655634835, 0.8505313298894353]\n",
      "41.78525981239563\n",
      "Validation MSE: 46.9659\n",
      "Epoch 936 LR: 0.000914\n",
      "Gradient norms: [17.811593807793916, 3.2223186904453254, 0.8859830633858817]\n",
      "40.325560934597625\n",
      "Validation MSE: 46.8993\n",
      "Epoch 937 LR: 0.000914\n",
      "Gradient norms: [14.037546927516727, 3.298255851876631, 0.8155510046121281]\n",
      "41.61824178826484\n",
      "Validation MSE: 46.8604\n",
      "Epoch 938 LR: 0.000914\n",
      "Gradient norms: [19.071192884189795, 4.504478806031815, 0.9623942828242615]\n",
      "41.43519564982832\n",
      "Validation MSE: 46.8332\n",
      "Epoch 939 LR: 0.000914\n",
      "Gradient norms: [16.7206796368335, 3.829507068866316, 1.039398035421324]\n",
      "40.36289308137735\n",
      "Validation MSE: 46.8454\n",
      "Epoch 940 LR: 0.000914\n",
      "Gradient norms: [17.194825219871856, 4.361324312460469, 1.6979579475600326]\n",
      "36.847882290484854\n",
      "Validation MSE: 46.8601\n",
      "Epoch 941 LR: 0.000914\n",
      "Gradient norms: [18.65451662602246, 4.273361764140892, 0.7948935066817798]\n",
      "42.16413199566061\n",
      "Validation MSE: 46.8935\n",
      "Epoch 942 LR: 0.000914\n",
      "Gradient norms: [17.903092379810023, 3.9552882347260945, 1.1146981573932286]\n",
      "40.637819548708954\n",
      "Validation MSE: 46.8976\n",
      "Epoch 943 LR: 0.000914\n",
      "Gradient norms: [17.825517903330987, 4.5608311332371505, 0.8582250210206384]\n",
      "43.273704991693194\n",
      "Validation MSE: 46.9062\n",
      "Epoch 944 LR: 0.000914\n",
      "Gradient norms: [15.493308433625666, 3.584496373536105, 1.449952282938268]\n",
      "38.41518068979171\n",
      "Validation MSE: 46.8727\n",
      "Epoch 945 LR: 0.000914\n",
      "Gradient norms: [14.639233981291552, 4.069413984435544, 0.924621870946849]\n",
      "40.18546659899654\n",
      "Validation MSE: 46.8645\n",
      "Epoch 946 LR: 0.000914\n",
      "Gradient norms: [14.224109332896406, 4.046063232522866, 1.2964087840536216]\n",
      "40.9697278745783\n",
      "Validation MSE: 46.8734\n",
      "Epoch 947 LR: 0.000913\n",
      "Gradient norms: [19.707406771107966, 3.964233569337343, 0.6930169093785432]\n",
      "41.65144757946684\n",
      "Validation MSE: 46.8848\n",
      "Epoch 948 LR: 0.000913\n",
      "Gradient norms: [18.023704414073666, 4.190782966767658, 1.2324241210089055]\n",
      "39.148186623964406\n",
      "Validation MSE: 46.8593\n",
      "Epoch 949 LR: 0.000913\n",
      "Gradient norms: [14.173725029193088, 3.7828724095480752, 0.8896443977059377]\n",
      "41.31137618206355\n",
      "Validation MSE: 46.8483\n",
      "Epoch 950 LR: 0.000913\n",
      "Gradient norms: [19.28861866934625, 4.210598307363046, 0.7560964613752824]\n",
      "43.062331234209104\n",
      "Validation MSE: 46.8363\n",
      "Epoch 951 LR: 0.000913\n",
      "Gradient norms: [16.419172456539417, 4.003465020019813, 0.8725761984442654]\n",
      "41.625959877419454\n",
      "Validation MSE: 46.8643\n",
      "Epoch 952 LR: 0.000913\n",
      "Gradient norms: [21.854852315038155, 4.514634162185211, 0.7944861722827322]\n",
      "43.791325589793\n",
      "Validation MSE: 46.8870\n",
      "Epoch 953 LR: 0.000913\n",
      "Gradient norms: [20.483551112037006, 3.9771459735447174, 1.1915501915279154]\n",
      "39.13328433620835\n",
      "Validation MSE: 46.9354\n",
      "Epoch 954 LR: 0.000913\n",
      "Gradient norms: [17.59954570459183, 4.708431131877273, 1.472436392883072]\n",
      "38.83654471639043\n",
      "Validation MSE: 46.9865\n",
      "Epoch 955 LR: 0.000913\n",
      "Gradient norms: [13.835510440154298, 3.9314712029036722, 1.0076423157716798]\n",
      "40.16304035136073\n",
      "Validation MSE: 47.0209\n",
      "Epoch 956 LR: 0.000913\n",
      "Gradient norms: [15.49025739338657, 3.5726044350745747, 0.9392649810847437]\n",
      "40.68310062758362\n",
      "Validation MSE: 47.0522\n",
      "Epoch 957 LR: 0.000913\n",
      "Gradient norms: [16.204259286411766, 4.197866285585061, 0.7318909503593733]\n",
      "42.264858003876704\n",
      "Validation MSE: 47.0817\n",
      "Epoch 958 LR: 0.000913\n",
      "Gradient norms: [16.363895775685222, 4.551110035613277, 1.1106183074973242]\n",
      "40.6977589376603\n",
      "Validation MSE: 47.0970\n",
      "Epoch 959 LR: 0.000912\n",
      "Gradient norms: [20.57503138396853, 5.047089948483831, 1.0365196254897993]\n",
      "41.624755543319\n",
      "Validation MSE: 47.1072\n",
      "Epoch 960 LR: 0.000912\n",
      "Gradient norms: [17.749406888881023, 4.655420667265217, 1.1115722166491704]\n",
      "39.916614008545714\n",
      "Validation MSE: 47.1488\n",
      "Epoch 961 LR: 0.000912\n",
      "Gradient norms: [22.20600746013165, 6.037616766229414, 0.7349966439447648]\n",
      "43.02893298882426\n",
      "Validation MSE: 47.1463\n",
      "Epoch 962 LR: 0.000912\n",
      "Gradient norms: [16.836595569133486, 4.383256732241718, 0.7441481153644695]\n",
      "41.542540094332914\n",
      "Validation MSE: 47.1343\n",
      "Epoch 963 LR: 0.000912\n",
      "Gradient norms: [17.34012206343895, 4.41517864487571, 0.7964610915942589]\n",
      "41.17398930910915\n",
      "Validation MSE: 47.1427\n",
      "Epoch 964 LR: 0.000912\n",
      "Gradient norms: [16.33440068539782, 4.545614385031511, 0.8748871725111368]\n",
      "40.21199469427905\n",
      "Validation MSE: 47.1870\n",
      "Epoch 965 LR: 0.000912\n",
      "Gradient norms: [19.554906279019214, 3.8033563603618954, 1.007009631403797]\n",
      "39.998282578382295\n",
      "Validation MSE: 47.2323\n",
      "Epoch 966 LR: 0.000912\n",
      "Gradient norms: [17.82601230299872, 4.308343893603086, 0.8216355282087181]\n",
      "43.58203717921451\n",
      "Validation MSE: 47.2891\n",
      "Epoch 967 LR: 0.000912\n",
      "Gradient norms: [18.367981406118794, 4.344657633779082, 1.1248054560826897]\n",
      "40.65886582466182\n",
      "Validation MSE: 47.3304\n",
      "Epoch 968 LR: 0.000912\n",
      "Gradient norms: [20.2513423417695, 4.3655346412223235, 1.1306934134628521]\n",
      "39.144714509456065\n",
      "Validation MSE: 47.3508\n",
      "Epoch 969 LR: 0.000912\n",
      "Gradient norms: [17.975056475145944, 4.069021009503716, 0.7811388529105838]\n",
      "41.08395506708225\n",
      "Validation MSE: 47.3768\n",
      "Epoch 970 LR: 0.000912\n",
      "Gradient norms: [19.740099898280842, 4.60121125651423, 1.022028824396934]\n",
      "42.975812377429364\n",
      "Validation MSE: 47.3617\n",
      "Epoch 971 LR: 0.000911\n",
      "Gradient norms: [19.849945624034458, 4.2331789163614495, 0.6676090557052252]\n",
      "42.473673437774394\n",
      "Validation MSE: 47.3661\n",
      "Epoch 972 LR: 0.000911\n",
      "Gradient norms: [13.889038069946276, 3.701939079393703, 1.0265042829449442]\n",
      "39.76161725965787\n",
      "Validation MSE: 47.3258\n",
      "Epoch 973 LR: 0.000911\n",
      "Gradient norms: [16.8378387588305, 4.9249400643241765, 1.044334205023065]\n",
      "41.18304407240451\n",
      "Validation MSE: 47.2190\n",
      "Epoch 974 LR: 0.000911\n",
      "Gradient norms: [18.99032962073924, 4.749260186352243, 0.7331789750833184]\n",
      "42.370720980365014\n",
      "Validation MSE: 47.1299\n",
      "Epoch 975 LR: 0.000911\n",
      "Gradient norms: [21.72088531313769, 4.4653455311705175, 0.836630747744402]\n",
      "41.722154033899386\n",
      "Validation MSE: 47.0551\n",
      "Epoch 976 LR: 0.000911\n",
      "Gradient norms: [14.392038884051887, 3.7002907402676697, 0.8085809750320859]\n",
      "40.84182208826417\n",
      "Validation MSE: 46.9277\n",
      "Epoch 977 LR: 0.000911\n",
      "Gradient norms: [19.102028553242125, 4.946200397068536, 1.1561131362377093]\n",
      "39.82226207565705\n",
      "Validation MSE: 46.8717\n",
      "Epoch 978 LR: 0.000911\n",
      "Gradient norms: [18.001494204552234, 4.669686982833047, 1.1786499978007705]\n",
      "40.10303785466064\n",
      "Validation MSE: 46.8268\n",
      "Epoch 979 LR: 0.000911\n",
      "Gradient norms: [19.656449824322703, 4.783600772422332, 1.0474371924496426]\n",
      "41.064439080684075\n",
      "Validation MSE: 46.8146\n",
      "Epoch 980 LR: 0.000911\n",
      "Gradient norms: [15.822394665303662, 4.218123706606261, 1.0562972687969654]\n",
      "39.62184556905483\n",
      "Validation MSE: 46.8406\n",
      "Epoch 981 LR: 0.000911\n",
      "Gradient norms: [16.988803763246395, 4.115435197483722, 1.1157721760120332]\n",
      "39.818462877546374\n",
      "Validation MSE: 46.8078\n",
      "Epoch 982 LR: 0.000911\n",
      "Gradient norms: [14.090909912664813, 4.812937757295101, 0.8357554162865016]\n",
      "41.04143760526785\n",
      "Validation MSE: 46.7914\n",
      "Epoch 983 LR: 0.000910\n",
      "Gradient norms: [20.225255095130105, 4.214567224132368, 1.0679173280389727]\n",
      "41.71755444570767\n",
      "Validation MSE: 46.7340\n",
      "Epoch 984 LR: 0.000910\n",
      "Gradient norms: [19.45043840062801, 4.941790598368666, 0.8109031493271349]\n",
      "42.86398647352476\n",
      "Validation MSE: 46.6719\n",
      "Epoch 985 LR: 0.000910\n",
      "Gradient norms: [16.19586443122278, 4.354362261632497, 0.7714178039535836]\n",
      "42.33447995227349\n",
      "Validation MSE: 46.6561\n",
      "Epoch 986 LR: 0.000910\n",
      "Gradient norms: [17.660426005294003, 4.332063557293228, 0.7459420973378619]\n",
      "42.51158787927736\n",
      "Validation MSE: 46.6092\n",
      "Epoch 987 LR: 0.000910\n",
      "Gradient norms: [17.8059068865641, 4.23429777698185, 0.9878767393939887]\n",
      "40.95743745725378\n",
      "Validation MSE: 46.5688\n",
      "Epoch 988 LR: 0.000910\n",
      "Gradient norms: [15.674921346098763, 4.785593023402182, 1.419543660884894]\n",
      "38.57894861619498\n",
      "Validation MSE: 46.5844\n",
      "Epoch 989 LR: 0.000910\n",
      "Gradient norms: [19.43452854682746, 4.390804891341433, 1.0342873342956922]\n",
      "37.833344113069764\n",
      "Validation MSE: 46.5847\n",
      "Epoch 990 LR: 0.000910\n",
      "Gradient norms: [20.456159888549053, 4.2475503501300444, 0.8255841022037165]\n",
      "41.20715363251202\n",
      "Validation MSE: 46.5515\n",
      "Epoch 991 LR: 0.000910\n",
      "Gradient norms: [16.085510549190868, 4.7143363922421, 1.0795247743095773]\n",
      "40.313012851264816\n",
      "Validation MSE: 46.5766\n",
      "Epoch 992 LR: 0.000910\n",
      "Gradient norms: [16.760550151409287, 4.483849309836757, 0.8212012086124143]\n",
      "43.22207666329584\n",
      "Validation MSE: 46.5788\n",
      "Epoch 993 LR: 0.000910\n",
      "Gradient norms: [19.15447457618634, 4.286693055932065, 1.12330106510475]\n",
      "38.120345703231564\n",
      "Validation MSE: 46.5873\n",
      "Epoch 994 LR: 0.000910\n",
      "Gradient norms: [16.35411304044073, 4.000210110608369, 0.8334491722052657]\n",
      "39.77911577166521\n",
      "Validation MSE: 46.6042\n",
      "Epoch 995 LR: 0.000910\n",
      "Gradient norms: [19.816526308496744, 3.831838550720485, 0.4240954568255663]\n",
      "41.85453975118935\n",
      "Validation MSE: 46.6140\n",
      "Epoch 996 LR: 0.000909\n",
      "Gradient norms: [17.701774928011993, 5.364949460811771, 1.0357487358601323]\n",
      "41.49775971300969\n",
      "Validation MSE: 46.6315\n",
      "Epoch 997 LR: 0.000909\n",
      "Gradient norms: [19.11804754950572, 4.416350156445477, 1.185412715937432]\n",
      "41.79405559178269\n",
      "Validation MSE: 46.6582\n",
      "Epoch 998 LR: 0.000909\n",
      "Gradient norms: [16.42894047377297, 4.229798892063594, 1.2193496637033305]\n",
      "40.31839821003735\n",
      "Validation MSE: 46.7061\n",
      "Epoch 999 LR: 0.000909\n",
      "Gradient norms: [18.147362664338186, 3.9818502606997863, 0.8211878618734275]\n",
      "41.20750959307339\n",
      "Validation MSE: 46.7146\n"
     ]
    }
   ],
   "source": [
    "optimizer = Optimizer_Adam(learning_rate=learning_rate, decay=1e-4, epsilon=1e-7)\n",
    "loss_function = MSE()\n",
    "train_losses = []\n",
    "validation_losses = []\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    batch_losses = []\n",
    "    for X_batch, y_batch in create_batches(X_train, y_train, batch_size):\n",
    "        # Convert to numpy if using pandas\n",
    "        if isinstance(X_batch, pd.DataFrame):\n",
    "            X_batch = X_batch.values\n",
    "        if isinstance(y_batch, (pd.Series, pd.DataFrame)):\n",
    "            y_batch = y_batch.values\n",
    "        \n",
    "        # Forward pass\n",
    "        model.forward(X_batch, training=True)\n",
    "        loss = loss_function.forward(model.output, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss_function.backward(model.output, y_batch)\n",
    "        dvalues = loss_function.dinputs\n",
    "        \n",
    "        # Gradient propagation\n",
    "        for layer in reversed(model.layers):\n",
    "            if hasattr(layer, 'backward'):\n",
    "                layer.backward(dvalues)\n",
    "                dvalues = layer.dinputs\n",
    "                if isinstance(dvalues, pd.DataFrame):\n",
    "                    dvalues = dvalues.values\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.pre_update_params()\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, Layer_Dense):\n",
    "                optimizer.update_params(layer)\n",
    "        optimizer.post_update_params()\n",
    "        current_lr = optimizer.current_learning_rate\n",
    "        print(f\"Epoch {epoch} LR: {current_lr:.6f}\")\n",
    "        grad_magnitudes = []\n",
    "        for layer in model.layers:\n",
    "            if isinstance(layer, Layer_Dense):\n",
    "                grad_norm = np.linalg.norm(layer.dweights)\n",
    "                grad_magnitudes.append(grad_norm)\n",
    "        print(f\"Gradient norms: {grad_magnitudes}\")\n",
    "        batch_losses.append(loss)\n",
    "    epoch_loss = np.mean(batch_losses)\n",
    "    print(epoch_loss)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.forward(X_validation, training=False)\n",
    "    val_mee = loss_function.forward(model.output, y_validation)\n",
    "    print(f\"Validation MSE: {val_mee:.4f}\")\n",
    "    validation_losses.append(val_mee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses(train_losses, validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 12)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = []\n",
    "print(X_test.shape)\n",
    "# Forward pass\n",
    "\n",
    "# loss = loss_activation.forward(dense2.output, y_test)\n",
    "print(loss_function.output.shape)\n",
    "# Calculate accuracy for this batch\n",
    "predictions = loss_function.output\n",
    "# if len(y_test.shape) == 2:\n",
    "#     y_true = np.argmax(y_batch, axis=1)\n",
    "# else:\n",
    "#     y_true = y_test\n",
    "# accuracy = np.mean(predictions == y_true)\n",
    "# test_accuracy.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.3117179846313"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = None\n",
    "# best_mse = float('inf')\n",
    "\n",
    "\n",
    "# for params in param_combinations:\n",
    "#     print(f\"\\nTesting parameters: {params}\")\n",
    "#     fold_val_mses = []\n",
    "    \n",
    "#     # K-Fold Cross Validation\n",
    "#     n_splits = 5\n",
    "#     indices = np.random.RandomState(seed=42).permutation(len(X))\n",
    "#     fold_sizes = (len(X) // n_splits) * np.ones(n_splits, dtype=int)\n",
    "#     fold_sizes[:len(X) % n_splits] += 1\n",
    "    \n",
    "#     current = 0\n",
    "#     for fold in range(n_splits):\n",
    "#         # Create fold indices\n",
    "#         start = current\n",
    "#         end = start + fold_sizes[fold]\n",
    "#         val_idx = indices[start:end]\n",
    "#         train_idx = np.concatenate([indices[:start], indices[end:]])\n",
    "#         current = end\n",
    "\n",
    "#         # Split data\n",
    "#         X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "#         y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "#         # Initialize model with current parameters\n",
    "#         model = NN(\n",
    "#             input_size=X.shape[1],\n",
    "#             hidden_sizes=params['hidden_sizes'],\n",
    "#             output_size=3,\n",
    "#             hidden_activations=[params['activations']()],\n",
    "#             dropout_rates=[params['dropout_rate']] * len(params['hidden_sizes']),\n",
    "#             l1=params['l1'],\n",
    "#             l2=params['l2']\n",
    "#         )\n",
    "#         optimizer = Optimizer_Adam(learning_rate=params['learning_rate'])\n",
    "#         loss_function = MSE()\n",
    "\n",
    "#         # Training loop\n",
    "#         for epoch in range(params['n_epochs']):\n",
    "#             # Mini-batch training\n",
    "#             batch_losses = []\n",
    "#             for X_batch, y_batch in create_batches(X_train_fold, y_train_fold, params['batch_size']):\n",
    "#                 # Convert to numpy if using pandas\n",
    "#                 if isinstance(X_batch, pd.DataFrame):\n",
    "#                     X_batch = X_batch.values\n",
    "#                 if isinstance(y_batch, (pd.Series, pd.DataFrame)):\n",
    "#                     y_batch = y_batch.values\n",
    "                \n",
    "#                 # Forward pass\n",
    "#                 model.forward(X_batch, training=True)\n",
    "#                 loss = loss_function.forward(model.output, y_batch)\n",
    "                \n",
    "#                 # Backward pass\n",
    "#                 loss_function.backward(model.output, y_batch)\n",
    "#                 dvalues = loss_function.dinputs\n",
    "                \n",
    "#                 # Gradient propagation\n",
    "#                 for layer in reversed(model.layers):\n",
    "#                     if hasattr(layer, 'backward'):\n",
    "#                         layer.backward(dvalues)\n",
    "#                         dvalues = layer.dinputs\n",
    "#                         # Convert to numpy if needed\n",
    "#                         if isinstance(dvalues, pd.DataFrame):\n",
    "#                             dvalues = dvalues.values\n",
    "                \n",
    "#                 # Update parameters\n",
    "#                 optimizer.pre_update_params()\n",
    "#                 for layer in model.layers:\n",
    "#                     if isinstance(layer, Layer_Dense):\n",
    "#                         optimizer.update_params(layer)\n",
    "#                 optimizer.post_update_params()\n",
    "                \n",
    "#                 batch_losses.append(loss)\n",
    "\n",
    "#             # Validation\n",
    "#             model.forward(X_val_fold, training=False)\n",
    "#             val_loss = loss_function.forward(model.output, y_val_fold)\n",
    "        \n",
    "#         fold_val_mses.append(val_loss)\n",
    "#         print(f\"Fold {fold+1} Val MSE: {val_loss:.4f}\")\n",
    "\n",
    "#     # Calculate average performance\n",
    "#     avg_val_mse = np.mean(fold_val_mses)\n",
    "#     print(f\"Average Validation MSE: {avg_val_mse:.4f}\")\n",
    "    \n",
    "#     # Update best parameters\n",
    "#     if avg_val_mse < best_mse:\n",
    "#         best_mse = avg_val_mse\n",
    "#         best_params = params\n",
    "\n",
    "# print(\"\\n=== Best Parameters ===\")\n",
    "# print(best_params)\n",
    "# print(f\"Best Validation MSE: {best_mse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
